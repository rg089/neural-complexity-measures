{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'marketdata'\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train_cs_0_0_2.pickle', 'test_cs_0_0_2.pickle'),\n",
       " ('train_cs_1_0_2.pickle', 'test_cs_1_0_2.pickle'),\n",
       " ('train_cs_2_0_2.pickle', 'test_cs_2_0_2.pickle'),\n",
       " ('train_cs_3_0_2.pickle', 'test_cs_3_0_2.pickle'),\n",
       " ('train_cs_4_0_2.pickle', 'test_cs_4_0_2.pickle'),\n",
       " ('train_cs_0_1_2.pickle', 'test_cs_0_1_2.pickle'),\n",
       " ('train_cs_1_1_2.pickle', 'test_cs_1_1_2.pickle'),\n",
       " ('train_cs_2_1_2.pickle', 'test_cs_2_1_2.pickle'),\n",
       " ('train_cs_3_1_2.pickle', 'test_cs_3_1_2.pickle'),\n",
       " ('train_cs_4_1_2.pickle', 'test_cs_4_1_2.pickle'),\n",
       " ('train_cs_0_2_2.pickle', 'test_cs_0_2_2.pickle'),\n",
       " ('train_cs_1_2_2.pickle', 'test_cs_1_2_2.pickle'),\n",
       " ('train_cs_2_2_2.pickle', 'test_cs_2_2_2.pickle'),\n",
       " ('train_cs_3_2_2.pickle', 'test_cs_3_2_2.pickle'),\n",
       " ('train_cs_4_2_2.pickle', 'test_cs_4_2_2.pickle'),\n",
       " ('train_cs_0_3_2.pickle', 'test_cs_0_3_2.pickle'),\n",
       " ('train_cs_1_3_2.pickle', 'test_cs_1_3_2.pickle'),\n",
       " ('train_cs_2_3_2.pickle', 'test_cs_2_3_2.pickle'),\n",
       " ('train_cs_3_3_2.pickle', 'test_cs_3_3_2.pickle'),\n",
       " ('train_cs_4_3_2.pickle', 'test_cs_4_3_2.pickle'),\n",
       " ('train_cs_0_4_2.pickle', 'test_cs_0_4_2.pickle'),\n",
       " ('train_cs_1_4_2.pickle', 'test_cs_1_4_2.pickle'),\n",
       " ('train_cs_2_4_2.pickle', 'test_cs_2_4_2.pickle'),\n",
       " ('train_cs_3_4_2.pickle', 'test_cs_3_4_2.pickle'),\n",
       " ('train_cs_4_4_2.pickle', 'test_cs_4_4_2.pickle'),\n",
       " ('train_cs_0_5_2.pickle', 'test_cs_0_5_2.pickle'),\n",
       " ('train_cs_1_5_2.pickle', 'test_cs_1_5_2.pickle'),\n",
       " ('train_cs_2_5_2.pickle', 'test_cs_2_5_2.pickle'),\n",
       " ('train_cs_3_5_2.pickle', 'test_cs_3_5_2.pickle'),\n",
       " ('train_cs_4_5_2.pickle', 'test_cs_4_5_2.pickle'),\n",
       " ('train_cs_0_6_2.pickle', 'test_cs_0_6_2.pickle'),\n",
       " ('train_cs_1_6_2.pickle', 'test_cs_1_6_2.pickle'),\n",
       " ('train_cs_2_6_2.pickle', 'test_cs_2_6_2.pickle'),\n",
       " ('train_cs_3_6_2.pickle', 'test_cs_3_6_2.pickle'),\n",
       " ('train_cs_4_6_2.pickle', 'test_cs_4_6_2.pickle'),\n",
       " ('train_cs_0_7_2.pickle', 'test_cs_0_7_2.pickle'),\n",
       " ('train_cs_1_7_2.pickle', 'test_cs_1_7_2.pickle'),\n",
       " ('train_cs_2_7_2.pickle', 'test_cs_2_7_2.pickle'),\n",
       " ('train_cs_3_7_2.pickle', 'test_cs_3_7_2.pickle'),\n",
       " ('train_cs_4_7_2.pickle', 'test_cs_4_7_2.pickle'),\n",
       " ('train_cs_0_8_2.pickle', 'test_cs_0_8_2.pickle'),\n",
       " ('train_cs_1_8_2.pickle', 'test_cs_1_8_2.pickle'),\n",
       " ('train_cs_2_8_2.pickle', 'test_cs_2_8_2.pickle'),\n",
       " ('train_cs_3_8_2.pickle', 'test_cs_3_8_2.pickle'),\n",
       " ('train_cs_4_8_2.pickle', 'test_cs_4_8_2.pickle'),\n",
       " ('train_cs_0_9_2.pickle', 'test_cs_0_9_2.pickle'),\n",
       " ('train_cs_1_9_2.pickle', 'test_cs_1_9_2.pickle'),\n",
       " ('train_cs_2_9_2.pickle', 'test_cs_2_9_2.pickle'),\n",
       " ('train_cs_3_9_2.pickle', 'test_cs_3_9_2.pickle'),\n",
       " ('train_cs_4_9_2.pickle', 'test_cs_4_9_2.pickle'),\n",
       " ('train_cs_0_10_2.pickle', 'test_cs_0_10_2.pickle'),\n",
       " ('train_cs_1_10_2.pickle', 'test_cs_1_10_2.pickle'),\n",
       " ('train_cs_2_10_2.pickle', 'test_cs_2_10_2.pickle'),\n",
       " ('train_cs_3_10_2.pickle', 'test_cs_3_10_2.pickle'),\n",
       " ('train_cs_4_10_2.pickle', 'test_cs_4_10_2.pickle'),\n",
       " ('train_cs_0_11_2.pickle', 'test_cs_0_11_2.pickle'),\n",
       " ('train_cs_1_11_2.pickle', 'test_cs_1_11_2.pickle'),\n",
       " ('train_cs_2_11_2.pickle', 'test_cs_2_11_2.pickle'),\n",
       " ('train_cs_3_11_2.pickle', 'test_cs_3_11_2.pickle'),\n",
       " ('train_cs_4_11_2.pickle', 'test_cs_4_11_2.pickle'),\n",
       " ('train_cs_0_12_2.pickle', 'test_cs_0_12_2.pickle'),\n",
       " ('train_cs_1_12_2.pickle', 'test_cs_1_12_2.pickle'),\n",
       " ('train_cs_2_12_2.pickle', 'test_cs_2_12_2.pickle'),\n",
       " ('train_cs_3_12_2.pickle', 'test_cs_3_12_2.pickle'),\n",
       " ('train_cs_4_12_2.pickle', 'test_cs_4_12_2.pickle'),\n",
       " ('train_cs_0_13_2.pickle', 'test_cs_0_13_2.pickle'),\n",
       " ('train_cs_1_13_2.pickle', 'test_cs_1_13_2.pickle'),\n",
       " ('train_cs_2_13_2.pickle', 'test_cs_2_13_2.pickle'),\n",
       " ('train_cs_3_13_2.pickle', 'test_cs_3_13_2.pickle'),\n",
       " ('train_cs_4_13_2.pickle', 'test_cs_4_13_2.pickle'),\n",
       " ('train_cs_0_14_2.pickle', 'test_cs_0_14_2.pickle'),\n",
       " ('train_cs_1_14_2.pickle', 'test_cs_1_14_2.pickle'),\n",
       " ('train_cs_2_14_2.pickle', 'test_cs_2_14_2.pickle'),\n",
       " ('train_cs_3_14_2.pickle', 'test_cs_3_14_2.pickle'),\n",
       " ('train_cs_4_14_2.pickle', 'test_cs_4_14_2.pickle'),\n",
       " ('train_cs_0_15_2.pickle', 'test_cs_0_15_2.pickle'),\n",
       " ('train_cs_1_15_2.pickle', 'test_cs_1_15_2.pickle'),\n",
       " ('train_cs_2_15_2.pickle', 'test_cs_2_15_2.pickle'),\n",
       " ('train_cs_3_15_2.pickle', 'test_cs_3_15_2.pickle'),\n",
       " ('train_cs_4_15_2.pickle', 'test_cs_4_15_2.pickle'),\n",
       " ('train_cs_0_16_2.pickle', 'test_cs_0_16_2.pickle'),\n",
       " ('train_cs_1_16_2.pickle', 'test_cs_1_16_2.pickle'),\n",
       " ('train_cs_2_16_2.pickle', 'test_cs_2_16_2.pickle'),\n",
       " ('train_cs_3_16_2.pickle', 'test_cs_3_16_2.pickle'),\n",
       " ('train_cs_4_16_2.pickle', 'test_cs_4_16_2.pickle'),\n",
       " ('train_cs_0_17_2.pickle', 'test_cs_0_17_2.pickle'),\n",
       " ('train_cs_1_17_2.pickle', 'test_cs_1_17_2.pickle'),\n",
       " ('train_cs_2_17_2.pickle', 'test_cs_2_17_2.pickle'),\n",
       " ('train_cs_3_17_2.pickle', 'test_cs_3_17_2.pickle'),\n",
       " ('train_cs_4_17_2.pickle', 'test_cs_4_17_2.pickle'),\n",
       " ('train_cs_0_18_2.pickle', 'test_cs_0_18_2.pickle'),\n",
       " ('train_cs_1_18_2.pickle', 'test_cs_1_18_2.pickle'),\n",
       " ('train_cs_2_18_2.pickle', 'test_cs_2_18_2.pickle'),\n",
       " ('train_cs_3_18_2.pickle', 'test_cs_3_18_2.pickle'),\n",
       " ('train_cs_4_18_2.pickle', 'test_cs_4_18_2.pickle'),\n",
       " ('train_cs_0_19_2.pickle', 'test_cs_0_19_2.pickle'),\n",
       " ('train_cs_1_19_2.pickle', 'test_cs_1_19_2.pickle'),\n",
       " ('train_cs_2_19_2.pickle', 'test_cs_2_19_2.pickle'),\n",
       " ('train_cs_3_19_2.pickle', 'test_cs_3_19_2.pickle'),\n",
       " ('train_cs_4_19_2.pickle', 'test_cs_4_19_2.pickle')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    output = y_tr[:, :6]\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16))\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 23]) torch.Size([32, 10])\n",
      "tensor([1.0142, 1.0139, 1.0137, 0.9916, 0.9919, 0.9922, 0.0000, 0.0000, 1.0000,\n",
      "        3.0000], dtype=torch.float64)\n",
      "tensor([1.0142, 1.0139, 1.0137, 0.9916, 0.9919, 0.9922, 1.0000, 0.0000, 0.0000,\n",
      "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    for tasks in range(130):\n",
    "        X_train, y_train, X_test, y_test = sample_task()\n",
    "        \n",
    "        for batch in zip(X_train, y_train):\n",
    "            X_tr, y_tr = batch[0], batch[1]\n",
    "            print(X_tr.shape, y_tr.shape)\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c821b3ac4508363ab26790ea772768a10c1768efd11bb9891ab119aba586fc55"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

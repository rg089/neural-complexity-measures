{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"ds_with_nc_lstm_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"ds\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = True\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if data_type == 'cs' else 360\n",
    "xtrain_feat_dim = 18 if data_type == \"ds\" else None\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, test_x_pred, train_x_pred, train_x_y_pred, gap, train_loss):\n",
    "        if not hasattr(self, \"test_x_pred\"): # if adding the first sample\n",
    "            self.test_x_pred = test_x_pred\n",
    "            self.train_x_pred = train_x_pred\n",
    "            self.train_x_y_pred = train_x_y_pred\n",
    "            self.gap = gap\n",
    "            self.train_loss = train_loss\n",
    "        else:\n",
    "            self.test_x_pred = torch.cat([self.test_x_pred, test_x_pred], dim=0)\n",
    "            self.train_x_pred = torch.cat([self.train_x_pred, train_x_pred], dim=0)\n",
    "            self.train_x_y_pred = torch.cat([self.train_x_y_pred, train_x_y_pred], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.train_loss = torch.cat([self.train_loss, train_loss], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.test_x_pred.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"test_x_pred\": self.test_x_pred[idxs].to(device),\n",
    "            \"train_x_pred\": self.train_x_pred[idxs].to(device),\n",
    "            \"train_x_y_pred\": self.train_x_y_pred[idxs].to(device),\n",
    "            \"tr_loss\": self.train_loss[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )\n",
    "    elif task == 'seq':\n",
    "        return TimeSeries(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_value = mlp(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_query_key = mlp(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"train_x_y_pred\"][:, :xtrain_dim], inputs[\"train_x_y_pred\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"train_x_y_pred\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_query_key(inputs[\"test_x_pred\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_query_key(inputs[\"train_x_pred\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_value(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = mlp(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = mlp(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)\n",
    "\n",
    "\n",
    "class TimeSeries(torch.nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size, seq_length=20, batch_size=32):\n",
    "        super(TimeSeries, self).__init__()\n",
    "        self.init_dim = init_dim\n",
    "        self.seq_len = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.lstm = torch.nn.LSTM(input_size = self.init_dim, \n",
    "                                 hidden_size = self.hidden_size,\n",
    "                                 num_layers = self.num_layers, \n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.linear_reg = torch.nn.Linear(self.hidden_size*self.seq_len, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        cell_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, _, _ = x.size()\n",
    "        lstm_out, _ = self.lstm(x,self.hidden)\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining train_loss to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, train_loss = get_task_loss(preds_train, y_train, crit_reg, crit_cls)\n",
    "\n",
    "        test_x_pred = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "        train_x_pred = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "        train_x_y_pred = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"test_x_pred\": test_x_pred, \"train_x_pred\": train_x_pred, \"train_x_y_pred\": train_x_y_pred, \"tr_loss\": train_loss}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, test_loss = get_task_loss(preds_test, y_test, crit_reg, crit_cls)\n",
    "\n",
    "        # train_loss and test_loss are used to compute the gap\n",
    "        \n",
    "        gap = test_loss.mean(-1) - train_loss.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                test_x_pred=test_x_pred.cpu().detach(),\n",
    "                train_x_pred=train_x_pred.cpu().detach(),\n",
    "                train_x_y_pred=train_x_y_pred.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                train_loss=train_loss.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, train_loss = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, test_loss = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = test_loss.mean(-1) - train_loss.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"test_loss\": [test_loss.mean(-1).detach().cpu()],\n",
    "                    \"train_loss\": [train_loss.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"test_loss\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"train_loss\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"test_loss {mean_l_test:.2e} train_loss {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"test_loss\": mean_l_test.item(),\n",
    "        \"train_loss\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 18:28:29.957 | INFO     | __main__:<module>:19 - Populate time: 0.0483896117657423\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 18:28:39.051 | INFO     | __main__:<module>:4 - Epoch 0\n",
      "2022-04-28 18:28:39.052 | INFO     | __main__:<module>:5 - Bank size: 14240\n",
      "2022-04-28 18:28:39.252 | INFO     | __main__:train:53 - Train Step 1\n",
      "2022-04-28 18:28:39.253 | INFO     | __main__:train:55 - mae 3.45e-01 loss 1.01e-01 R 0.136 gap 0.15023639798164368 preds -0.028999246656894684\n",
      "2022-04-28 18:28:39.346 | INFO     | __main__:train:53 - Train Step 2\n",
      "2022-04-28 18:28:39.347 | INFO     | __main__:train:55 - mae 3.94e-01 loss 1.31e-01 R 0.064 gap 0.15745531022548676 preds -0.03396043926477432\n",
      "2022-04-28 18:28:39.434 | INFO     | __main__:train:53 - Train Step 3\n",
      "2022-04-28 18:28:39.435 | INFO     | __main__:train:55 - mae 4.45e-01 loss 1.54e-01 R 0.154 gap 0.15011103451251984 preds -0.020058175548911095\n",
      "2022-04-28 18:28:39.521 | INFO     | __main__:train:53 - Train Step 4\n",
      "2022-04-28 18:28:39.522 | INFO     | __main__:train:55 - mae 4.77e-01 loss 1.79e-01 R 0.150 gap 0.15894322097301483 preds -0.012519856914877892\n",
      "2022-04-28 18:28:39.605 | INFO     | __main__:train:53 - Train Step 5\n",
      "2022-04-28 18:28:39.606 | INFO     | __main__:train:55 - mae 4.82e-01 loss 1.83e-01 R 0.182 gap 0.1616400182247162 preds -0.010644635185599327\n",
      "2022-04-28 18:28:39.701 | INFO     | __main__:train:53 - Train Step 6\n",
      "2022-04-28 18:28:39.702 | INFO     | __main__:train:55 - mae 4.73e-01 loss 1.75e-01 R 0.162 gap 0.15743517875671387 preds -0.005884508136659861\n",
      "2022-04-28 18:28:39.797 | INFO     | __main__:train:53 - Train Step 7\n",
      "2022-04-28 18:28:39.798 | INFO     | __main__:train:55 - mae 4.70e-01 loss 1.72e-01 R 0.186 gap 0.15955713391304016 preds 0.0029799214098602533\n",
      "2022-04-28 18:28:39.885 | INFO     | __main__:train:53 - Train Step 8\n",
      "2022-04-28 18:28:39.885 | INFO     | __main__:train:55 - mae 4.71e-01 loss 1.73e-01 R 0.165 gap 0.15683773159980774 preds 0.006831811740994453\n",
      "2022-04-28 18:28:39.967 | INFO     | __main__:train:53 - Train Step 9\n",
      "2022-04-28 18:28:39.968 | INFO     | __main__:train:55 - mae 4.88e-01 loss 1.82e-01 R 0.194 gap 0.15341795980930328 preds 0.006684174295514822\n",
      "2022-04-28 18:28:40.302 | INFO     | __main__:train:53 - Train Step 10\n",
      "2022-04-28 18:28:40.303 | INFO     | __main__:train:55 - mae 4.81e-01 loss 1.80e-01 R 0.202 gap 0.14976052939891815 preds 0.007969506084918976\n",
      "2022-04-28 18:28:40.392 | INFO     | __main__:train:53 - Train Step 11\n",
      "2022-04-28 18:28:40.393 | INFO     | __main__:train:55 - mae 4.84e-01 loss 1.81e-01 R 0.188 gap 0.14974838495254517 preds 0.009769177995622158\n",
      "2022-04-28 18:28:40.481 | INFO     | __main__:train:53 - Train Step 12\n",
      "2022-04-28 18:28:40.481 | INFO     | __main__:train:55 - mae 4.81e-01 loss 1.81e-01 R 0.172 gap 0.1499306559562683 preds 0.01219309400767088\n",
      "2022-04-28 18:28:40.569 | INFO     | __main__:train:53 - Train Step 13\n",
      "2022-04-28 18:28:40.569 | INFO     | __main__:train:55 - mae 4.78e-01 loss 1.80e-01 R 0.160 gap 0.14821340143680573 preds 0.01294010691344738\n",
      "2022-04-28 18:28:40.661 | INFO     | __main__:train:53 - Train Step 14\n",
      "2022-04-28 18:28:40.662 | INFO     | __main__:train:55 - mae 4.67e-01 loss 1.72e-01 R 0.173 gap 0.15025761723518372 preds 0.016223570331931114\n",
      "2022-04-28 18:28:40.751 | INFO     | __main__:train:53 - Train Step 15\n",
      "2022-04-28 18:28:40.752 | INFO     | __main__:train:55 - mae 4.66e-01 loss 1.72e-01 R 0.165 gap 0.15021514892578125 preds 0.02030745893716812\n",
      "2022-04-28 18:28:40.839 | INFO     | __main__:train:53 - Train Step 16\n",
      "2022-04-28 18:28:40.840 | INFO     | __main__:train:55 - mae 4.63e-01 loss 1.68e-01 R 0.169 gap 0.14928162097930908 preds 0.022085053846240044\n",
      "2022-04-28 18:28:40.930 | INFO     | __main__:train:53 - Train Step 17\n",
      "2022-04-28 18:28:40.930 | INFO     | __main__:train:55 - mae 4.66e-01 loss 1.71e-01 R 0.153 gap 0.15165197849273682 preds 0.02455596998333931\n",
      "2022-04-28 18:28:41.016 | INFO     | __main__:train:53 - Train Step 18\n",
      "2022-04-28 18:28:41.017 | INFO     | __main__:train:55 - mae 4.65e-01 loss 1.70e-01 R 0.156 gap 0.15204592049121857 preds 0.02682965248823166\n",
      "2022-04-28 18:28:41.102 | INFO     | __main__:train:53 - Train Step 19\n",
      "2022-04-28 18:28:41.103 | INFO     | __main__:train:55 - mae 4.65e-01 loss 1.70e-01 R 0.153 gap 0.1534302979707718 preds 0.03239431977272034\n",
      "2022-04-28 18:28:41.444 | INFO     | __main__:train:53 - Train Step 20\n",
      "2022-04-28 18:28:41.444 | INFO     | __main__:train:55 - mae 4.57e-01 loss 1.65e-01 R 0.164 gap 0.15568508207798004 preds 0.03651007264852524\n",
      "2022-04-28 18:28:41.531 | INFO     | __main__:train:53 - Train Step 21\n",
      "2022-04-28 18:28:41.532 | INFO     | __main__:train:55 - mae 4.51e-01 loss 1.61e-01 R 0.166 gap 0.15376436710357666 preds 0.04020842909812927\n",
      "2022-04-28 18:28:41.619 | INFO     | __main__:train:53 - Train Step 22\n",
      "2022-04-28 18:28:41.620 | INFO     | __main__:train:55 - mae 4.51e-01 loss 1.61e-01 R 0.163 gap 0.15257646143436432 preds 0.04154272377490997\n",
      "2022-04-28 18:28:41.706 | INFO     | __main__:train:53 - Train Step 23\n",
      "2022-04-28 18:28:41.706 | INFO     | __main__:train:55 - mae 4.51e-01 loss 1.61e-01 R 0.161 gap 0.1519172042608261 preds 0.04614803567528725\n",
      "2022-04-28 18:28:41.793 | INFO     | __main__:train:53 - Train Step 24\n",
      "2022-04-28 18:28:41.794 | INFO     | __main__:train:55 - mae 4.49e-01 loss 1.59e-01 R 0.159 gap 0.1509736031293869 preds 0.04840512201189995\n",
      "2022-04-28 18:28:41.879 | INFO     | __main__:train:53 - Train Step 25\n",
      "2022-04-28 18:28:41.879 | INFO     | __main__:train:55 - mae 4.42e-01 loss 1.55e-01 R 0.158 gap 0.1517287641763687 preds 0.05141017958521843\n",
      "2022-04-28 18:28:41.965 | INFO     | __main__:train:53 - Train Step 26\n",
      "2022-04-28 18:28:41.966 | INFO     | __main__:train:55 - mae 4.38e-01 loss 1.53e-01 R 0.160 gap 0.1531226485967636 preds 0.052848026156425476\n",
      "2022-04-28 18:28:42.052 | INFO     | __main__:train:53 - Train Step 27\n",
      "2022-04-28 18:28:42.052 | INFO     | __main__:train:55 - mae 4.36e-01 loss 1.51e-01 R 0.165 gap 0.15307042002677917 preds 0.05368071421980858\n",
      "2022-04-28 18:28:42.139 | INFO     | __main__:train:53 - Train Step 28\n",
      "2022-04-28 18:28:42.140 | INFO     | __main__:train:55 - mae 4.34e-01 loss 1.50e-01 R 0.157 gap 0.15086233615875244 preds 0.058328885585069656\n",
      "2022-04-28 18:28:42.225 | INFO     | __main__:train:53 - Train Step 29\n",
      "2022-04-28 18:28:42.226 | INFO     | __main__:train:55 - mae 4.31e-01 loss 1.49e-01 R 0.162 gap 0.1494465172290802 preds 0.06221423298120499\n",
      "2022-04-28 18:28:42.567 | INFO     | __main__:train:53 - Train Step 30\n",
      "2022-04-28 18:28:42.568 | INFO     | __main__:train:55 - mae 4.30e-01 loss 1.48e-01 R 0.163 gap 0.15288864076137543 preds 0.06436503678560257\n",
      "2022-04-28 18:28:42.658 | INFO     | __main__:train:53 - Train Step 31\n",
      "2022-04-28 18:28:42.659 | INFO     | __main__:train:55 - mae 4.26e-01 loss 1.46e-01 R 0.164 gap 0.15343432128429413 preds 0.0677374079823494\n",
      "2022-04-28 18:28:42.747 | INFO     | __main__:train:53 - Train Step 32\n",
      "2022-04-28 18:28:42.748 | INFO     | __main__:train:55 - mae 4.22e-01 loss 1.43e-01 R 0.175 gap 0.15345852077007294 preds 0.06958170980215073\n",
      "2022-04-28 18:28:42.835 | INFO     | __main__:train:53 - Train Step 33\n",
      "2022-04-28 18:28:42.835 | INFO     | __main__:train:55 - mae 4.17e-01 loss 1.41e-01 R 0.179 gap 0.15332424640655518 preds 0.07059448212385178\n",
      "2022-04-28 18:28:42.920 | INFO     | __main__:train:53 - Train Step 34\n",
      "2022-04-28 18:28:42.920 | INFO     | __main__:train:55 - mae 4.13e-01 loss 1.38e-01 R 0.186 gap 0.1536492556333542 preds 0.07374624162912369\n",
      "2022-04-28 18:28:43.006 | INFO     | __main__:train:53 - Train Step 35\n",
      "2022-04-28 18:28:43.007 | INFO     | __main__:train:55 - mae 4.10e-01 loss 1.37e-01 R 0.182 gap 0.15348085761070251 preds 0.07456398755311966\n",
      "2022-04-28 18:28:43.093 | INFO     | __main__:train:53 - Train Step 36\n",
      "2022-04-28 18:28:43.093 | INFO     | __main__:train:55 - mae 4.08e-01 loss 1.35e-01 R 0.177 gap 0.15197999775409698 preds 0.07678478211164474\n",
      "2022-04-28 18:28:43.178 | INFO     | __main__:train:53 - Train Step 37\n",
      "2022-04-28 18:28:43.179 | INFO     | __main__:train:55 - mae 4.04e-01 loss 1.33e-01 R 0.178 gap 0.1509527713060379 preds 0.07912667095661163\n",
      "2022-04-28 18:28:43.264 | INFO     | __main__:train:53 - Train Step 38\n",
      "2022-04-28 18:28:43.265 | INFO     | __main__:train:55 - mae 4.03e-01 loss 1.32e-01 R 0.176 gap 0.1508464515209198 preds 0.08020053058862686\n",
      "2022-04-28 18:28:43.350 | INFO     | __main__:train:53 - Train Step 39\n",
      "2022-04-28 18:28:43.351 | INFO     | __main__:train:55 - mae 4.02e-01 loss 1.32e-01 R 0.165 gap 0.15093958377838135 preds 0.08226437866687775\n",
      "2022-04-28 18:28:43.701 | INFO     | __main__:train:53 - Train Step 40\n",
      "2022-04-28 18:28:43.702 | INFO     | __main__:train:55 - mae 4.00e-01 loss 1.31e-01 R 0.167 gap 0.15072835981845856 preds 0.08314463496208191\n",
      "2022-04-28 18:28:43.786 | INFO     | __main__:train:53 - Train Step 41\n",
      "2022-04-28 18:28:43.786 | INFO     | __main__:train:55 - mae 3.98e-01 loss 1.29e-01 R 0.170 gap 0.1499522477388382 preds 0.08576628565788269\n",
      "2022-04-28 18:28:43.870 | INFO     | __main__:train:53 - Train Step 42\n",
      "2022-04-28 18:28:43.870 | INFO     | __main__:train:55 - mae 3.96e-01 loss 1.28e-01 R 0.168 gap 0.15004824101924896 preds 0.08773151785135269\n",
      "2022-04-28 18:28:43.957 | INFO     | __main__:train:53 - Train Step 43\n",
      "2022-04-28 18:28:43.958 | INFO     | __main__:train:55 - mae 3.95e-01 loss 1.28e-01 R 0.171 gap 0.15043748915195465 preds 0.08903708308935165\n",
      "2022-04-28 18:28:44.045 | INFO     | __main__:train:53 - Train Step 44\n",
      "2022-04-28 18:28:44.046 | INFO     | __main__:train:55 - mae 3.92e-01 loss 1.26e-01 R 0.179 gap 0.1495106816291809 preds 0.08971171081066132\n",
      "2022-04-28 18:28:44.136 | INFO     | __main__:train:53 - Train Step 45\n",
      "2022-04-28 18:28:44.137 | INFO     | __main__:train:55 - mae 3.90e-01 loss 1.25e-01 R 0.177 gap 0.14961721003055573 preds 0.09092874825000763\n",
      "2022-04-28 18:28:44.217 | INFO     | __main__:train:53 - Train Step 46\n",
      "2022-04-28 18:28:44.218 | INFO     | __main__:train:55 - mae 3.90e-01 loss 1.25e-01 R 0.172 gap 0.1493832916021347 preds 0.09266199916601181\n",
      "2022-04-28 18:28:44.297 | INFO     | __main__:train:53 - Train Step 47\n",
      "2022-04-28 18:28:44.298 | INFO     | __main__:train:55 - mae 3.88e-01 loss 1.24e-01 R 0.170 gap 0.1483137011528015 preds 0.0922015979886055\n",
      "2022-04-28 18:28:44.380 | INFO     | __main__:train:53 - Train Step 48\n",
      "2022-04-28 18:28:44.381 | INFO     | __main__:train:55 - mae 3.84e-01 loss 1.22e-01 R 0.168 gap 0.14651717245578766 preds 0.09380396455526352\n",
      "2022-04-28 18:28:44.461 | INFO     | __main__:train:53 - Train Step 49\n",
      "2022-04-28 18:28:44.462 | INFO     | __main__:train:55 - mae 3.84e-01 loss 1.21e-01 R 0.166 gap 0.14718562364578247 preds 0.09428028017282486\n",
      "2022-04-28 18:28:44.775 | INFO     | __main__:train:53 - Train Step 50\n",
      "2022-04-28 18:28:44.776 | INFO     | __main__:train:55 - mae 3.82e-01 loss 1.20e-01 R 0.172 gap 0.14829804003238678 preds 0.09484311193227768\n",
      "2022-04-28 18:28:44.852 | INFO     | __main__:train:53 - Train Step 51\n",
      "2022-04-28 18:28:44.853 | INFO     | __main__:train:55 - mae 3.80e-01 loss 1.19e-01 R 0.172 gap 0.1471584588289261 preds 0.09564466029405594\n",
      "2022-04-28 18:28:44.929 | INFO     | __main__:train:53 - Train Step 52\n",
      "2022-04-28 18:28:44.930 | INFO     | __main__:train:55 - mae 3.77e-01 loss 1.18e-01 R 0.165 gap 0.14772777259349823 preds 0.09599755704402924\n",
      "2022-04-28 18:28:45.009 | INFO     | __main__:train:53 - Train Step 53\n",
      "2022-04-28 18:28:45.010 | INFO     | __main__:train:55 - mae 3.77e-01 loss 1.18e-01 R 0.163 gap 0.14837221801280975 preds 0.09737753123044968\n",
      "2022-04-28 18:28:45.089 | INFO     | __main__:train:53 - Train Step 54\n",
      "2022-04-28 18:28:45.090 | INFO     | __main__:train:55 - mae 3.75e-01 loss 1.17e-01 R 0.165 gap 0.14782142639160156 preds 0.09765678644180298\n",
      "2022-04-28 18:28:45.177 | INFO     | __main__:train:53 - Train Step 55\n",
      "2022-04-28 18:28:45.178 | INFO     | __main__:train:55 - mae 3.73e-01 loss 1.17e-01 R 0.166 gap 0.14859411120414734 preds 0.09795738756656647\n",
      "2022-04-28 18:28:45.266 | INFO     | __main__:train:53 - Train Step 56\n",
      "2022-04-28 18:28:45.266 | INFO     | __main__:train:55 - mae 3.72e-01 loss 1.16e-01 R 0.165 gap 0.14806590974330902 preds 0.09834090620279312\n",
      "2022-04-28 18:28:45.352 | INFO     | __main__:train:53 - Train Step 57\n",
      "2022-04-28 18:28:45.352 | INFO     | __main__:train:55 - mae 3.70e-01 loss 1.14e-01 R 0.173 gap 0.14944984018802643 preds 0.09884370118379593\n",
      "2022-04-28 18:28:45.438 | INFO     | __main__:train:53 - Train Step 58\n",
      "2022-04-28 18:28:45.439 | INFO     | __main__:train:55 - mae 3.68e-01 loss 1.13e-01 R 0.172 gap 0.1489819437265396 preds 0.09948448836803436\n",
      "2022-04-28 18:28:45.525 | INFO     | __main__:train:53 - Train Step 59\n",
      "2022-04-28 18:28:45.526 | INFO     | __main__:train:55 - mae 3.68e-01 loss 1.13e-01 R 0.167 gap 0.14905913174152374 preds 0.10044252872467041\n",
      "2022-04-28 18:28:45.868 | INFO     | __main__:train:53 - Train Step 60\n",
      "2022-04-28 18:28:45.869 | INFO     | __main__:train:55 - mae 3.65e-01 loss 1.12e-01 R 0.169 gap 0.1486836075782776 preds 0.10125134140253067\n",
      "2022-04-28 18:28:45.955 | INFO     | __main__:train:53 - Train Step 61\n",
      "2022-04-28 18:28:45.956 | INFO     | __main__:train:55 - mae 3.63e-01 loss 1.11e-01 R 0.169 gap 0.14802314341068268 preds 0.10220211744308472\n",
      "2022-04-28 18:28:46.042 | INFO     | __main__:train:53 - Train Step 62\n",
      "2022-04-28 18:28:46.043 | INFO     | __main__:train:55 - mae 3.62e-01 loss 1.10e-01 R 0.171 gap 0.14783233404159546 preds 0.10239548236131668\n",
      "2022-04-28 18:28:46.129 | INFO     | __main__:train:53 - Train Step 63\n",
      "2022-04-28 18:28:46.129 | INFO     | __main__:train:55 - mae 3.59e-01 loss 1.09e-01 R 0.172 gap 0.14763513207435608 preds 0.10337451100349426\n",
      "2022-04-28 18:28:46.216 | INFO     | __main__:train:53 - Train Step 64\n",
      "2022-04-28 18:28:46.217 | INFO     | __main__:train:55 - mae 3.59e-01 loss 1.09e-01 R 0.172 gap 0.14718224108219147 preds 0.10418306291103363\n",
      "2022-04-28 18:28:46.303 | INFO     | __main__:train:53 - Train Step 65\n",
      "2022-04-28 18:28:46.304 | INFO     | __main__:train:55 - mae 3.57e-01 loss 1.08e-01 R 0.175 gap 0.1471855789422989 preds 0.10428248345851898\n",
      "2022-04-28 18:28:46.390 | INFO     | __main__:train:53 - Train Step 66\n",
      "2022-04-28 18:28:46.391 | INFO     | __main__:train:55 - mae 3.55e-01 loss 1.07e-01 R 0.176 gap 0.14710326492786407 preds 0.10475301742553711\n",
      "2022-04-28 18:28:46.477 | INFO     | __main__:train:53 - Train Step 67\n",
      "2022-04-28 18:28:46.478 | INFO     | __main__:train:55 - mae 3.53e-01 loss 1.06e-01 R 0.177 gap 0.14687378704547882 preds 0.10634587705135345\n",
      "2022-04-28 18:28:46.567 | INFO     | __main__:train:53 - Train Step 68\n",
      "2022-04-28 18:28:46.567 | INFO     | __main__:train:55 - mae 3.50e-01 loss 1.04e-01 R 0.178 gap 0.14685802161693573 preds 0.10656347125768661\n",
      "2022-04-28 18:28:46.657 | INFO     | __main__:train:53 - Train Step 69\n",
      "2022-04-28 18:28:46.658 | INFO     | __main__:train:55 - mae 3.49e-01 loss 1.04e-01 R 0.178 gap 0.14733350276947021 preds 0.10735619813203812\n",
      "2022-04-28 18:28:46.993 | INFO     | __main__:train:53 - Train Step 70\n",
      "2022-04-28 18:28:46.994 | INFO     | __main__:train:55 - mae 3.48e-01 loss 1.03e-01 R 0.176 gap 0.14678969979286194 preds 0.1084383875131607\n",
      "2022-04-28 18:28:47.081 | INFO     | __main__:train:53 - Train Step 71\n",
      "2022-04-28 18:28:47.082 | INFO     | __main__:train:55 - mae 3.46e-01 loss 1.02e-01 R 0.175 gap 0.14611274003982544 preds 0.10871189087629318\n",
      "2022-04-28 18:28:47.168 | INFO     | __main__:train:53 - Train Step 72\n",
      "2022-04-28 18:28:47.169 | INFO     | __main__:train:55 - mae 3.45e-01 loss 1.02e-01 R 0.173 gap 0.1459728330373764 preds 0.10936718434095383\n",
      "2022-04-28 18:28:47.311 | INFO     | __main__:train:53 - Train Step 73\n",
      "2022-04-28 18:28:47.312 | INFO     | __main__:train:55 - mae 3.42e-01 loss 1.01e-01 R 0.173 gap 0.1461135894060135 preds 0.10970866680145264\n",
      "2022-04-28 18:28:47.448 | INFO     | __main__:train:53 - Train Step 74\n",
      "2022-04-28 18:28:47.449 | INFO     | __main__:train:55 - mae 3.41e-01 loss 9.99e-02 R 0.171 gap 0.14520521461963654 preds 0.11005113273859024\n",
      "2022-04-28 18:28:47.580 | INFO     | __main__:train:53 - Train Step 75\n",
      "2022-04-28 18:28:47.581 | INFO     | __main__:train:55 - mae 3.39e-01 loss 9.92e-02 R 0.171 gap 0.1450718194246292 preds 0.11013610661029816\n",
      "2022-04-28 18:28:47.712 | INFO     | __main__:train:53 - Train Step 76\n",
      "2022-04-28 18:28:47.713 | INFO     | __main__:train:55 - mae 3.38e-01 loss 9.85e-02 R 0.171 gap 0.14524638652801514 preds 0.11028212308883667\n",
      "2022-04-28 18:28:47.842 | INFO     | __main__:train:53 - Train Step 77\n",
      "2022-04-28 18:28:47.842 | INFO     | __main__:train:55 - mae 3.36e-01 loss 9.78e-02 R 0.170 gap 0.14521925151348114 preds 0.11133898794651031\n",
      "2022-04-28 18:28:47.978 | INFO     | __main__:train:53 - Train Step 78\n",
      "2022-04-28 18:28:47.979 | INFO     | __main__:train:55 - mae 3.35e-01 loss 9.72e-02 R 0.168 gap 0.14547862112522125 preds 0.1114342138171196\n",
      "2022-04-28 18:28:48.114 | INFO     | __main__:train:53 - Train Step 79\n",
      "2022-04-28 18:28:48.115 | INFO     | __main__:train:55 - mae 3.34e-01 loss 9.66e-02 R 0.168 gap 0.14522486925125122 preds 0.11154869943857193\n",
      "2022-04-28 18:28:48.503 | INFO     | __main__:train:53 - Train Step 80\n",
      "2022-04-28 18:28:48.503 | INFO     | __main__:train:55 - mae 3.33e-01 loss 9.58e-02 R 0.168 gap 0.14554540812969208 preds 0.11183992773294449\n",
      "2022-04-28 18:28:48.644 | INFO     | __main__:train:53 - Train Step 81\n",
      "2022-04-28 18:28:48.644 | INFO     | __main__:train:55 - mae 3.32e-01 loss 9.52e-02 R 0.167 gap 0.1459917277097702 preds 0.11169687658548355\n",
      "2022-04-28 18:28:48.786 | INFO     | __main__:train:53 - Train Step 82\n",
      "2022-04-28 18:28:48.787 | INFO     | __main__:train:55 - mae 3.30e-01 loss 9.45e-02 R 0.167 gap 0.14658282697200775 preds 0.11222554743289948\n",
      "2022-04-28 18:28:48.926 | INFO     | __main__:train:53 - Train Step 83\n",
      "2022-04-28 18:28:48.927 | INFO     | __main__:train:55 - mae 3.28e-01 loss 9.37e-02 R 0.167 gap 0.14641691744327545 preds 0.11287104338407516\n",
      "2022-04-28 18:28:49.067 | INFO     | __main__:train:53 - Train Step 84\n",
      "2022-04-28 18:28:49.068 | INFO     | __main__:train:55 - mae 3.27e-01 loss 9.31e-02 R 0.171 gap 0.14623308181762695 preds 0.11271117627620697\n",
      "2022-04-28 18:28:49.209 | INFO     | __main__:train:53 - Train Step 85\n",
      "2022-04-28 18:28:49.210 | INFO     | __main__:train:55 - mae 3.26e-01 loss 9.22e-02 R 0.172 gap 0.14601707458496094 preds 0.11316918581724167\n",
      "2022-04-28 18:28:49.352 | INFO     | __main__:train:53 - Train Step 86\n",
      "2022-04-28 18:28:49.353 | INFO     | __main__:train:55 - mae 3.24e-01 loss 9.14e-02 R 0.172 gap 0.14577874541282654 preds 0.11329852789640427\n",
      "2022-04-28 18:28:49.495 | INFO     | __main__:train:53 - Train Step 87\n",
      "2022-04-28 18:28:49.495 | INFO     | __main__:train:55 - mae 3.23e-01 loss 9.08e-02 R 0.171 gap 0.14544181525707245 preds 0.11378160864114761\n",
      "2022-04-28 18:28:49.629 | INFO     | __main__:train:53 - Train Step 88\n",
      "2022-04-28 18:28:49.629 | INFO     | __main__:train:55 - mae 3.21e-01 loss 9.01e-02 R 0.172 gap 0.14513792097568512 preds 0.11371421813964844\n",
      "2022-04-28 18:28:49.759 | INFO     | __main__:train:53 - Train Step 89\n",
      "2022-04-28 18:28:49.760 | INFO     | __main__:train:55 - mae 3.19e-01 loss 8.94e-02 R 0.172 gap 0.1448856145143509 preds 0.11395012587308884\n",
      "2022-04-28 18:29:12.032 | INFO     | __main__:test:61 - Test epoch 0\n",
      "2022-04-28 18:29:12.033 | INFO     | __main__:test:63 - mae 6.39e-01 loss 4.24e-01 R -0.013 test_loss 7.15e-01 train_loss 2.00e-01 \n",
      "2022-04-28 18:29:12.034 | INFO     | __main__:<module>:4 - Epoch 1\n",
      "2022-04-28 18:29:12.034 | INFO     | __main__:<module>:5 - Bank size: 15520\n",
      "2022-04-28 18:29:12.532 | INFO     | __main__:train:53 - Train Step 90\n",
      "2022-04-28 18:29:12.533 | INFO     | __main__:train:55 - mae 3.19e-01 loss 8.90e-02 R 0.170 gap 0.14510053396224976 preds 0.1138375923037529\n",
      "2022-04-28 18:29:12.676 | INFO     | __main__:train:53 - Train Step 91\n",
      "2022-04-28 18:29:12.676 | INFO     | __main__:train:55 - mae 3.18e-01 loss 8.84e-02 R 0.172 gap 0.14625030755996704 preds 0.11420322209596634\n",
      "2022-04-28 18:29:12.820 | INFO     | __main__:train:53 - Train Step 92\n",
      "2022-04-28 18:29:12.821 | INFO     | __main__:train:55 - mae 3.16e-01 loss 8.78e-02 R 0.175 gap 0.14684012532234192 preds 0.11401977390050888\n",
      "2022-04-28 18:29:12.957 | INFO     | __main__:train:53 - Train Step 93\n",
      "2022-04-28 18:29:12.958 | INFO     | __main__:train:55 - mae 3.15e-01 loss 8.71e-02 R 0.175 gap 0.14660225808620453 preds 0.11447424441576004\n",
      "2022-04-28 18:29:13.090 | INFO     | __main__:train:53 - Train Step 94\n",
      "2022-04-28 18:29:13.090 | INFO     | __main__:train:55 - mae 3.14e-01 loss 8.66e-02 R 0.176 gap 0.14697550237178802 preds 0.11479885876178741\n",
      "2022-04-28 18:29:13.224 | INFO     | __main__:train:53 - Train Step 95\n",
      "2022-04-28 18:29:13.224 | INFO     | __main__:train:55 - mae 3.12e-01 loss 8.59e-02 R 0.179 gap 0.14706727862358093 preds 0.11531418561935425\n",
      "2022-04-28 18:29:13.356 | INFO     | __main__:train:53 - Train Step 96\n",
      "2022-04-28 18:29:13.357 | INFO     | __main__:train:55 - mae 3.12e-01 loss 8.55e-02 R 0.178 gap 0.1469259411096573 preds 0.11579547077417374\n",
      "2022-04-28 18:29:13.494 | INFO     | __main__:train:53 - Train Step 97\n",
      "2022-04-28 18:29:13.494 | INFO     | __main__:train:55 - mae 3.10e-01 loss 8.48e-02 R 0.178 gap 0.14697524905204773 preds 0.1164822056889534\n",
      "2022-04-28 18:29:13.627 | INFO     | __main__:train:53 - Train Step 98\n",
      "2022-04-28 18:29:13.627 | INFO     | __main__:train:55 - mae 3.09e-01 loss 8.42e-02 R 0.178 gap 0.14734841883182526 preds 0.11715042591094971\n",
      "2022-04-28 18:29:13.756 | INFO     | __main__:train:53 - Train Step 99\n",
      "2022-04-28 18:29:13.757 | INFO     | __main__:train:55 - mae 3.07e-01 loss 8.35e-02 R 0.178 gap 0.14742232859134674 preds 0.11762573570013046\n",
      "2022-04-28 18:29:14.143 | INFO     | __main__:train:53 - Train Step 100\n",
      "2022-04-28 18:29:14.143 | INFO     | __main__:train:55 - mae 3.06e-01 loss 8.29e-02 R 0.179 gap 0.14740456640720367 preds 0.1181861013174057\n",
      "2022-04-28 18:29:14.284 | INFO     | __main__:train:53 - Train Step 101\n",
      "2022-04-28 18:29:14.284 | INFO     | __main__:train:55 - mae 3.05e-01 loss 8.27e-02 R 0.176 gap 0.14757104218006134 preds 0.1181829571723938\n",
      "2022-04-28 18:29:14.422 | INFO     | __main__:train:53 - Train Step 102\n",
      "2022-04-28 18:29:14.422 | INFO     | __main__:train:55 - mae 3.04e-01 loss 8.22e-02 R 0.175 gap 0.14738431572914124 preds 0.11878817528486252\n",
      "2022-04-28 18:29:14.564 | INFO     | __main__:train:53 - Train Step 103\n",
      "2022-04-28 18:29:14.564 | INFO     | __main__:train:55 - mae 3.03e-01 loss 8.17e-02 R 0.176 gap 0.147447407245636 preds 0.1191621795296669\n",
      "2022-04-28 18:29:14.700 | INFO     | __main__:train:53 - Train Step 104\n",
      "2022-04-28 18:29:14.700 | INFO     | __main__:train:55 - mae 3.02e-01 loss 8.13e-02 R 0.176 gap 0.1470811516046524 preds 0.11933983117341995\n",
      "2022-04-28 18:29:14.833 | INFO     | __main__:train:53 - Train Step 105\n",
      "2022-04-28 18:29:14.834 | INFO     | __main__:train:55 - mae 3.01e-01 loss 8.08e-02 R 0.176 gap 0.1471000462770462 preds 0.1195259541273117\n",
      "2022-04-28 18:29:14.968 | INFO     | __main__:train:53 - Train Step 106\n",
      "2022-04-28 18:29:14.968 | INFO     | __main__:train:55 - mae 3.00e-01 loss 8.02e-02 R 0.176 gap 0.14662589132785797 preds 0.11957783252000809\n",
      "2022-04-28 18:29:15.105 | INFO     | __main__:train:53 - Train Step 107\n",
      "2022-04-28 18:29:15.105 | INFO     | __main__:train:55 - mae 2.98e-01 loss 7.96e-02 R 0.178 gap 0.14689016342163086 preds 0.12002509087324142\n",
      "2022-04-28 18:29:15.242 | INFO     | __main__:train:53 - Train Step 108\n",
      "2022-04-28 18:29:15.242 | INFO     | __main__:train:55 - mae 2.97e-01 loss 7.91e-02 R 0.180 gap 0.1468261331319809 preds 0.1204841285943985\n",
      "2022-04-28 18:29:15.372 | INFO     | __main__:train:53 - Train Step 109\n",
      "2022-04-28 18:29:15.373 | INFO     | __main__:train:55 - mae 2.96e-01 loss 7.86e-02 R 0.181 gap 0.14679959416389465 preds 0.12118351459503174\n",
      "2022-04-28 18:29:15.764 | INFO     | __main__:train:53 - Train Step 110\n",
      "2022-04-28 18:29:15.765 | INFO     | __main__:train:55 - mae 2.95e-01 loss 7.81e-02 R 0.183 gap 0.14680172502994537 preds 0.12137772142887115\n",
      "2022-04-28 18:29:15.899 | INFO     | __main__:train:53 - Train Step 111\n",
      "2022-04-28 18:29:15.900 | INFO     | __main__:train:55 - mae 2.93e-01 loss 7.76e-02 R 0.185 gap 0.14661073684692383 preds 0.12150835990905762\n",
      "2022-04-28 18:29:16.032 | INFO     | __main__:train:53 - Train Step 112\n",
      "2022-04-28 18:29:16.033 | INFO     | __main__:train:55 - mae 2.92e-01 loss 7.71e-02 R 0.186 gap 0.14632165431976318 preds 0.12201196700334549\n",
      "2022-04-28 18:29:16.169 | INFO     | __main__:train:53 - Train Step 113\n",
      "2022-04-28 18:29:16.170 | INFO     | __main__:train:55 - mae 2.91e-01 loss 7.66e-02 R 0.187 gap 0.1462094932794571 preds 0.12223012745380402\n",
      "2022-04-28 18:29:16.306 | INFO     | __main__:train:53 - Train Step 114\n",
      "2022-04-28 18:29:16.307 | INFO     | __main__:train:55 - mae 2.90e-01 loss 7.61e-02 R 0.187 gap 0.14595381915569305 preds 0.12233585119247437\n",
      "2022-04-28 18:29:16.448 | INFO     | __main__:train:53 - Train Step 115\n",
      "2022-04-28 18:29:16.449 | INFO     | __main__:train:55 - mae 2.89e-01 loss 7.57e-02 R 0.187 gap 0.14558446407318115 preds 0.1226634755730629\n",
      "2022-04-28 18:29:16.591 | INFO     | __main__:train:53 - Train Step 116\n",
      "2022-04-28 18:29:16.592 | INFO     | __main__:train:55 - mae 2.88e-01 loss 7.53e-02 R 0.187 gap 0.14519992470741272 preds 0.12290795147418976\n",
      "2022-04-28 18:29:16.735 | INFO     | __main__:train:53 - Train Step 117\n",
      "2022-04-28 18:29:16.735 | INFO     | __main__:train:55 - mae 2.87e-01 loss 7.48e-02 R 0.188 gap 0.14493250846862793 preds 0.12308048456907272\n",
      "2022-04-28 18:29:16.878 | INFO     | __main__:train:53 - Train Step 118\n",
      "2022-04-28 18:29:16.879 | INFO     | __main__:train:55 - mae 2.86e-01 loss 7.44e-02 R 0.189 gap 0.14467473328113556 preds 0.1231003925204277\n",
      "2022-04-28 18:29:17.025 | INFO     | __main__:train:53 - Train Step 119\n",
      "2022-04-28 18:29:17.026 | INFO     | __main__:train:55 - mae 2.85e-01 loss 7.41e-02 R 0.189 gap 0.14450329542160034 preds 0.12301385402679443\n",
      "2022-04-28 18:29:17.432 | INFO     | __main__:train:53 - Train Step 120\n",
      "2022-04-28 18:29:17.433 | INFO     | __main__:train:55 - mae 2.85e-01 loss 7.37e-02 R 0.189 gap 0.1440417766571045 preds 0.12314547598361969\n",
      "2022-04-28 18:29:17.575 | INFO     | __main__:train:53 - Train Step 121\n",
      "2022-04-28 18:29:17.576 | INFO     | __main__:train:55 - mae 2.84e-01 loss 7.33e-02 R 0.190 gap 0.14397543668746948 preds 0.12290117144584656\n",
      "2022-04-28 18:29:17.723 | INFO     | __main__:train:53 - Train Step 122\n",
      "2022-04-28 18:29:17.724 | INFO     | __main__:train:55 - mae 2.83e-01 loss 7.30e-02 R 0.189 gap 0.14362643659114838 preds 0.12258482724428177\n",
      "2022-04-28 18:29:17.869 | INFO     | __main__:train:53 - Train Step 123\n",
      "2022-04-28 18:29:17.870 | INFO     | __main__:train:55 - mae 2.83e-01 loss 7.27e-02 R 0.188 gap 0.14355459809303284 preds 0.12276744097471237\n",
      "2022-04-28 18:29:18.012 | INFO     | __main__:train:53 - Train Step 124\n",
      "2022-04-28 18:29:18.013 | INFO     | __main__:train:55 - mae 2.82e-01 loss 7.23e-02 R 0.189 gap 0.14311963319778442 preds 0.12262971699237823\n",
      "2022-04-28 18:29:18.155 | INFO     | __main__:train:53 - Train Step 125\n",
      "2022-04-28 18:29:18.156 | INFO     | __main__:train:55 - mae 2.81e-01 loss 7.19e-02 R 0.189 gap 0.14303144812583923 preds 0.12254860252141953\n",
      "2022-04-28 18:29:18.299 | INFO     | __main__:train:53 - Train Step 126\n",
      "2022-04-28 18:29:18.300 | INFO     | __main__:train:55 - mae 2.80e-01 loss 7.15e-02 R 0.188 gap 0.14283713698387146 preds 0.12271708250045776\n",
      "2022-04-28 18:29:18.442 | INFO     | __main__:train:53 - Train Step 127\n",
      "2022-04-28 18:29:18.443 | INFO     | __main__:train:55 - mae 2.79e-01 loss 7.11e-02 R 0.188 gap 0.14265994727611542 preds 0.12274901568889618\n",
      "2022-04-28 18:29:18.583 | INFO     | __main__:train:53 - Train Step 128\n",
      "2022-04-28 18:29:18.584 | INFO     | __main__:train:55 - mae 2.78e-01 loss 7.08e-02 R 0.188 gap 0.14257343113422394 preds 0.12272800505161285\n",
      "2022-04-28 18:29:18.725 | INFO     | __main__:train:53 - Train Step 129\n",
      "2022-04-28 18:29:18.726 | INFO     | __main__:train:55 - mae 2.78e-01 loss 7.04e-02 R 0.188 gap 0.1426863819360733 preds 0.12256142497062683\n",
      "2022-04-28 18:29:19.139 | INFO     | __main__:train:53 - Train Step 130\n",
      "2022-04-28 18:29:19.140 | INFO     | __main__:train:55 - mae 2.77e-01 loss 7.01e-02 R 0.189 gap 0.14246705174446106 preds 0.12252479046583176\n",
      "2022-04-28 18:29:19.282 | INFO     | __main__:train:53 - Train Step 131\n",
      "2022-04-28 18:29:19.283 | INFO     | __main__:train:55 - mae 2.76e-01 loss 6.97e-02 R 0.191 gap 0.1427759826183319 preds 0.12240034341812134\n",
      "2022-04-28 18:29:19.425 | INFO     | __main__:train:53 - Train Step 132\n",
      "2022-04-28 18:29:19.426 | INFO     | __main__:train:55 - mae 2.75e-01 loss 6.93e-02 R 0.192 gap 0.14249543845653534 preds 0.12223372608423233\n",
      "2022-04-28 18:29:19.571 | INFO     | __main__:train:53 - Train Step 133\n",
      "2022-04-28 18:29:19.572 | INFO     | __main__:train:55 - mae 2.74e-01 loss 6.89e-02 R 0.193 gap 0.14233076572418213 preds 0.12257508933544159\n",
      "2022-04-28 18:29:19.703 | INFO     | __main__:train:53 - Train Step 134\n",
      "2022-04-28 18:29:19.704 | INFO     | __main__:train:55 - mae 2.74e-01 loss 6.87e-02 R 0.193 gap 0.14250294864177704 preds 0.12209651619195938\n",
      "2022-04-28 18:29:19.847 | INFO     | __main__:train:53 - Train Step 135\n",
      "2022-04-28 18:29:19.848 | INFO     | __main__:train:55 - mae 2.73e-01 loss 6.83e-02 R 0.194 gap 0.1421176642179489 preds 0.12187902629375458\n",
      "2022-04-28 18:29:19.983 | INFO     | __main__:train:53 - Train Step 136\n",
      "2022-04-28 18:29:19.984 | INFO     | __main__:train:55 - mae 2.72e-01 loss 6.79e-02 R 0.195 gap 0.1421920508146286 preds 0.1218366026878357\n",
      "2022-04-28 18:29:20.123 | INFO     | __main__:train:53 - Train Step 137\n",
      "2022-04-28 18:29:20.124 | INFO     | __main__:train:55 - mae 2.71e-01 loss 6.76e-02 R 0.197 gap 0.14187489449977875 preds 0.12164774537086487\n",
      "2022-04-28 18:29:20.256 | INFO     | __main__:train:53 - Train Step 138\n",
      "2022-04-28 18:29:20.257 | INFO     | __main__:train:55 - mae 2.71e-01 loss 6.73e-02 R 0.199 gap 0.14196088910102844 preds 0.12168114632368088\n",
      "2022-04-28 18:29:20.387 | INFO     | __main__:train:53 - Train Step 139\n",
      "2022-04-28 18:29:20.388 | INFO     | __main__:train:55 - mae 2.70e-01 loss 6.70e-02 R 0.199 gap 0.14175797998905182 preds 0.12143343687057495\n",
      "2022-04-28 18:29:20.791 | INFO     | __main__:train:53 - Train Step 140\n",
      "2022-04-28 18:29:20.791 | INFO     | __main__:train:55 - mae 2.69e-01 loss 6.66e-02 R 0.199 gap 0.14155662059783936 preds 0.12132938951253891\n",
      "2022-04-28 18:29:20.936 | INFO     | __main__:train:53 - Train Step 141\n",
      "2022-04-28 18:29:20.937 | INFO     | __main__:train:55 - mae 2.68e-01 loss 6.63e-02 R 0.199 gap 0.14124895632266998 preds 0.12131946533918381\n",
      "2022-04-28 18:29:21.080 | INFO     | __main__:train:53 - Train Step 142\n",
      "2022-04-28 18:29:21.081 | INFO     | __main__:train:55 - mae 2.68e-01 loss 6.60e-02 R 0.199 gap 0.14134275913238525 preds 0.12122669816017151\n",
      "2022-04-28 18:29:21.215 | INFO     | __main__:train:53 - Train Step 143\n",
      "2022-04-28 18:29:21.215 | INFO     | __main__:train:55 - mae 2.67e-01 loss 6.57e-02 R 0.199 gap 0.14093811810016632 preds 0.12138797342777252\n",
      "2022-04-28 18:29:21.348 | INFO     | __main__:train:53 - Train Step 144\n",
      "2022-04-28 18:29:21.349 | INFO     | __main__:train:55 - mae 2.66e-01 loss 6.54e-02 R 0.199 gap 0.14092741906642914 preds 0.12114574015140533\n",
      "2022-04-28 18:29:21.485 | INFO     | __main__:train:53 - Train Step 145\n",
      "2022-04-28 18:29:21.485 | INFO     | __main__:train:55 - mae 2.65e-01 loss 6.50e-02 R 0.200 gap 0.14085911214351654 preds 0.1214575543999672\n",
      "2022-04-28 18:29:21.623 | INFO     | __main__:train:53 - Train Step 146\n",
      "2022-04-28 18:29:21.624 | INFO     | __main__:train:55 - mae 2.64e-01 loss 6.48e-02 R 0.198 gap 0.14110128581523895 preds 0.1212494969367981\n",
      "2022-04-28 18:29:21.769 | INFO     | __main__:train:53 - Train Step 147\n",
      "2022-04-28 18:29:21.769 | INFO     | __main__:train:55 - mae 2.64e-01 loss 6.45e-02 R 0.197 gap 0.14053554832935333 preds 0.12113843858242035\n",
      "2022-04-28 18:29:21.912 | INFO     | __main__:train:53 - Train Step 148\n",
      "2022-04-28 18:29:21.913 | INFO     | __main__:train:55 - mae 2.63e-01 loss 6.42e-02 R 0.197 gap 0.1406693011522293 preds 0.12107893079519272\n",
      "2022-04-28 18:29:22.055 | INFO     | __main__:train:53 - Train Step 149\n",
      "2022-04-28 18:29:22.055 | INFO     | __main__:train:55 - mae 2.62e-01 loss 6.39e-02 R 0.198 gap 0.1404908299446106 preds 0.12115811556577682\n",
      "2022-04-28 18:29:22.451 | INFO     | __main__:train:53 - Train Step 150\n",
      "2022-04-28 18:29:22.452 | INFO     | __main__:train:55 - mae 2.61e-01 loss 6.35e-02 R 0.200 gap 0.14017407596111298 preds 0.12100214511156082\n",
      "2022-04-28 18:29:22.588 | INFO     | __main__:train:53 - Train Step 151\n",
      "2022-04-28 18:29:22.589 | INFO     | __main__:train:55 - mae 2.61e-01 loss 6.33e-02 R 0.199 gap 0.1401633471250534 preds 0.12109745293855667\n",
      "2022-04-28 18:29:22.722 | INFO     | __main__:train:53 - Train Step 152\n",
      "2022-04-28 18:29:22.723 | INFO     | __main__:train:55 - mae 2.60e-01 loss 6.30e-02 R 0.200 gap 0.14020852744579315 preds 0.12104454636573792\n",
      "2022-04-28 18:29:22.854 | INFO     | __main__:train:53 - Train Step 153\n",
      "2022-04-28 18:29:22.855 | INFO     | __main__:train:55 - mae 2.59e-01 loss 6.27e-02 R 0.200 gap 0.1401204615831375 preds 0.1210433840751648\n",
      "2022-04-28 18:29:22.985 | INFO     | __main__:train:53 - Train Step 154\n",
      "2022-04-28 18:29:22.985 | INFO     | __main__:train:55 - mae 2.59e-01 loss 6.25e-02 R 0.200 gap 0.14007629454135895 preds 0.12111125141382217\n",
      "2022-04-28 18:29:23.118 | INFO     | __main__:train:53 - Train Step 155\n",
      "2022-04-28 18:29:23.119 | INFO     | __main__:train:55 - mae 2.58e-01 loss 6.22e-02 R 0.201 gap 0.14006733894348145 preds 0.12125677615404129\n",
      "2022-04-28 18:29:23.251 | INFO     | __main__:train:53 - Train Step 156\n",
      "2022-04-28 18:29:23.252 | INFO     | __main__:train:55 - mae 2.57e-01 loss 6.19e-02 R 0.203 gap 0.14010223746299744 preds 0.12131418287754059\n",
      "2022-04-28 18:29:23.381 | INFO     | __main__:train:53 - Train Step 157\n",
      "2022-04-28 18:29:23.382 | INFO     | __main__:train:55 - mae 2.57e-01 loss 6.16e-02 R 0.203 gap 0.1399986296892166 preds 0.1212703213095665\n",
      "2022-04-28 18:29:23.514 | INFO     | __main__:train:53 - Train Step 158\n",
      "2022-04-28 18:29:23.515 | INFO     | __main__:train:55 - mae 2.56e-01 loss 6.14e-02 R 0.204 gap 0.13980750739574432 preds 0.12125089019536972\n",
      "2022-04-28 18:29:23.650 | INFO     | __main__:train:53 - Train Step 159\n",
      "2022-04-28 18:29:23.651 | INFO     | __main__:train:55 - mae 2.55e-01 loss 6.11e-02 R 0.207 gap 0.13984955847263336 preds 0.12123376876115799\n",
      "2022-04-28 18:29:24.046 | INFO     | __main__:train:53 - Train Step 160\n",
      "2022-04-28 18:29:24.047 | INFO     | __main__:train:55 - mae 2.55e-01 loss 6.09e-02 R 0.206 gap 0.1394416093826294 preds 0.1212727278470993\n",
      "2022-04-28 18:29:24.189 | INFO     | __main__:train:53 - Train Step 161\n",
      "2022-04-28 18:29:24.189 | INFO     | __main__:train:55 - mae 2.54e-01 loss 6.06e-02 R 0.207 gap 0.13907042145729065 preds 0.1213148832321167\n",
      "2022-04-28 18:29:24.331 | INFO     | __main__:train:53 - Train Step 162\n",
      "2022-04-28 18:29:24.332 | INFO     | __main__:train:55 - mae 2.53e-01 loss 6.04e-02 R 0.209 gap 0.13925863802433014 preds 0.12137958407402039\n",
      "2022-04-28 18:29:24.475 | INFO     | __main__:train:53 - Train Step 163\n",
      "2022-04-28 18:29:24.476 | INFO     | __main__:train:55 - mae 2.53e-01 loss 6.01e-02 R 0.211 gap 0.13926883041858673 preds 0.12132305651903152\n",
      "2022-04-28 18:29:24.618 | INFO     | __main__:train:53 - Train Step 164\n",
      "2022-04-28 18:29:24.619 | INFO     | __main__:train:55 - mae 2.52e-01 loss 6.00e-02 R 0.210 gap 0.13897797465324402 preds 0.12136293202638626\n",
      "2022-04-28 18:29:24.762 | INFO     | __main__:train:53 - Train Step 165\n",
      "2022-04-28 18:29:24.763 | INFO     | __main__:train:55 - mae 2.52e-01 loss 5.98e-02 R 0.211 gap 0.13925322890281677 preds 0.12140888720750809\n",
      "2022-04-28 18:29:24.906 | INFO     | __main__:train:53 - Train Step 166\n",
      "2022-04-28 18:29:24.906 | INFO     | __main__:train:55 - mae 2.51e-01 loss 5.95e-02 R 0.212 gap 0.13888397812843323 preds 0.12131255865097046\n",
      "2022-04-28 18:29:25.050 | INFO     | __main__:train:53 - Train Step 167\n",
      "2022-04-28 18:29:25.050 | INFO     | __main__:train:55 - mae 2.51e-01 loss 5.93e-02 R 0.212 gap 0.13886260986328125 preds 0.12160807102918625\n",
      "2022-04-28 18:29:25.194 | INFO     | __main__:train:53 - Train Step 168\n",
      "2022-04-28 18:29:25.195 | INFO     | __main__:train:55 - mae 2.50e-01 loss 5.91e-02 R 0.211 gap 0.1389342099428177 preds 0.12158374488353729\n",
      "2022-04-28 18:29:25.338 | INFO     | __main__:train:53 - Train Step 169\n",
      "2022-04-28 18:29:25.339 | INFO     | __main__:train:55 - mae 2.50e-01 loss 5.89e-02 R 0.211 gap 0.1384875774383545 preds 0.12163346260786057\n",
      "2022-04-28 18:29:25.751 | INFO     | __main__:train:53 - Train Step 170\n",
      "2022-04-28 18:29:25.752 | INFO     | __main__:train:55 - mae 2.49e-01 loss 5.86e-02 R 0.212 gap 0.13857536017894745 preds 0.12165433913469315\n",
      "2022-04-28 18:29:25.896 | INFO     | __main__:train:53 - Train Step 171\n",
      "2022-04-28 18:29:25.897 | INFO     | __main__:train:55 - mae 2.49e-01 loss 5.84e-02 R 0.212 gap 0.13845667243003845 preds 0.12170952558517456\n",
      "2022-04-28 18:29:26.045 | INFO     | __main__:train:53 - Train Step 172\n",
      "2022-04-28 18:29:26.046 | INFO     | __main__:train:55 - mae 2.48e-01 loss 5.82e-02 R 0.212 gap 0.13831891119480133 preds 0.12179869413375854\n",
      "2022-04-28 18:29:26.193 | INFO     | __main__:train:53 - Train Step 173\n",
      "2022-04-28 18:29:26.194 | INFO     | __main__:train:55 - mae 2.47e-01 loss 5.79e-02 R 0.212 gap 0.13816145062446594 preds 0.1218731626868248\n",
      "2022-04-28 18:29:26.341 | INFO     | __main__:train:53 - Train Step 174\n",
      "2022-04-28 18:29:26.342 | INFO     | __main__:train:55 - mae 2.47e-01 loss 5.76e-02 R 0.212 gap 0.13823376595973969 preds 0.12199820578098297\n",
      "2022-04-28 18:29:26.489 | INFO     | __main__:train:53 - Train Step 175\n",
      "2022-04-28 18:29:26.490 | INFO     | __main__:train:55 - mae 2.46e-01 loss 5.74e-02 R 0.213 gap 0.13794511556625366 preds 0.12216124683618546\n",
      "2022-04-28 18:29:26.635 | INFO     | __main__:train:53 - Train Step 176\n",
      "2022-04-28 18:29:26.636 | INFO     | __main__:train:55 - mae 2.45e-01 loss 5.72e-02 R 0.213 gap 0.1378631442785263 preds 0.12203199416399002\n",
      "2022-04-28 18:29:26.781 | INFO     | __main__:train:53 - Train Step 177\n",
      "2022-04-28 18:29:26.782 | INFO     | __main__:train:55 - mae 2.45e-01 loss 5.70e-02 R 0.214 gap 0.13780494034290314 preds 0.12212943285703659\n",
      "2022-04-28 18:29:26.928 | INFO     | __main__:train:53 - Train Step 178\n",
      "2022-04-28 18:29:26.929 | INFO     | __main__:train:55 - mae 2.44e-01 loss 5.67e-02 R 0.214 gap 0.1375429332256317 preds 0.12207615375518799\n",
      "2022-04-28 18:29:48.909 | INFO     | __main__:test:61 - Test epoch 1\n",
      "2022-04-28 18:29:48.910 | INFO     | __main__:test:63 - mae 3.30e-01 loss 1.05e-01 R 0.010 test_loss 3.31e-01 train_loss 2.10e-01 \n"
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.test_x_pred.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:51:44.684 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_ds_with_nc_lstm.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, train_loss = get_task_loss(preds_train, y_train, crit_reg, crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            test_x_pred = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            train_x_pred = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            train_x_y_pred = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"test_x_pred\": test_x_pred, \"train_x_pred\": train_x_pred, \"train_x_y_pred\": train_x_y_pred, \"tr_loss\": train_loss}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "if data_type == \"ds\":\n",
    "    h.init_hidden()\n",
    "\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        test_loss, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(test_loss.item())\n",
    "        \n",
    "        train_loss, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(train_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:54:33.317 | INFO     | __main__:<module>:9 - Test 0.2908 +- 0.1665\n",
      "2022-04-28 12:54:33.318 | INFO     | __main__:<module>:10 - Train 0.0706 +- 0.0062\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.0255\n",
      "MSE for Dimension 2: 0.0276\n",
      "MSE for Dimension 3: 0.0305\n",
      "MSE for Dimension 4: 0.0300\n",
      "MSE for Dimension 5: 0.0248\n",
      "MSE for Dimension 6: 0.0312\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.48      0.54      6791\n",
      "         1.0       0.13      0.18      0.15      2227\n",
      "         2.0       0.20      0.26      0.23      1755\n",
      "         3.0       0.03      0.05      0.04       299\n",
      "\n",
      "    accuracy                           0.37     11072\n",
      "   macro avg       0.25      0.24      0.24     11072\n",
      "weighted avg       0.44      0.37      0.40     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.73      0.73      8156\n",
      "         1.0       0.00      0.00      0.00       469\n",
      "         2.0       0.19      0.27      0.22       790\n",
      "         3.0       0.13      0.11      0.12      1657\n",
      "\n",
      "    accuracy                           0.57     11072\n",
      "   macro avg       0.26      0.28      0.27     11072\n",
      "weighted avg       0.57      0.57      0.57     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.23      0.09      0.13      2706\n",
      "         1.0       0.36      0.42      0.39      4977\n",
      "         2.0       0.23      0.24      0.24      1293\n",
      "         3.0       0.16      0.22      0.18      2096\n",
      "\n",
      "    accuracy                           0.28     11072\n",
      "   macro avg       0.25      0.24      0.23     11072\n",
      "weighted avg       0.28      0.28      0.27     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.20      0.25      4851\n",
      "         1.0       0.07      0.11      0.09      1442\n",
      "         2.0       0.21      0.23      0.22       561\n",
      "         3.0       0.39      0.51      0.44      4218\n",
      "\n",
      "    accuracy                           0.31     11072\n",
      "   macro avg       0.26      0.26      0.25     11072\n",
      "weighted avg       0.32      0.31      0.30     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

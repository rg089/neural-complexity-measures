{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"ds_with_nc_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"ds\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = True\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if data_type == 'cs' else 360\n",
    "xtrain_feat_dim = 18 if data_type == \"ds\" else None\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )\n",
    "    elif task == 'seq':\n",
    "        return TimeSeries(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)\n",
    "\n",
    "\n",
    "class TimeSeries(torch.nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size, seq_length=20, batch_size=32):\n",
    "        super(TimeSeries, self).__init__()\n",
    "        self.init_dim = init_dim\n",
    "        self.seq_len = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.lstm = torch.nn.LSTM(input_size = self.init_dim, \n",
    "                                 hidden_size = self.hidden_size,\n",
    "                                 num_layers = self.num_layers, \n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.linear_reg = torch.nn.Linear(self.hidden_size*self.seq_len, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        cell_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, _, _ = x.size()\n",
    "        lstm_out, _ = self.lstm(x,self.hidden)\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    \n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:09:40.339 | INFO     | __main__:<module>:19 - Populate time: 0.048970870673656464\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:09:44.847 | INFO     | __main__:<module>:4 - Epoch 0\n",
      "2022-04-26 14:09:44.848 | INFO     | __main__:<module>:5 - Bank size: 14240\n",
      "2022-04-26 14:09:44.926 | INFO     | __main__:train:49 - Train Step 1\n",
      "2022-04-26 14:09:44.926 | INFO     | __main__:train:51 - mae 4.47e-01 loss 1.54e-01 R -0.356 gap 0.010691046714782715 preds 0.00990760326385498\n",
      "2022-04-26 14:09:45.031 | INFO     | __main__:train:49 - Train Step 2\n",
      "2022-04-26 14:09:45.032 | INFO     | __main__:train:51 - mae 5.11e-01 loss 1.95e-01 R -0.164 gap 0.008543393574655056 preds -0.004653267562389374\n",
      "2022-04-26 14:09:45.124 | INFO     | __main__:train:49 - Train Step 3\n",
      "2022-04-26 14:09:45.125 | INFO     | __main__:train:51 - mae 5.63e-01 loss 2.28e-01 R -0.183 gap 0.011010394431650639 preds 0.022747159004211426\n",
      "2022-04-26 14:09:45.217 | INFO     | __main__:train:49 - Train Step 4\n",
      "2022-04-26 14:09:45.219 | INFO     | __main__:train:51 - mae 5.75e-01 loss 2.39e-01 R -0.200 gap 0.010789942927658558 preds 0.02954377420246601\n",
      "2022-04-26 14:09:45.294 | INFO     | __main__:train:49 - Train Step 5\n",
      "2022-04-26 14:09:45.294 | INFO     | __main__:train:51 - mae 5.44e-01 loss 2.20e-01 R -0.189 gap 0.010062215849757195 preds 0.02877225913107395\n",
      "2022-04-26 14:09:45.369 | INFO     | __main__:train:49 - Train Step 6\n",
      "2022-04-26 14:09:45.369 | INFO     | __main__:train:51 - mae 5.39e-01 loss 2.17e-01 R -0.110 gap 0.010304399766027927 preds 0.03344583138823509\n",
      "2022-04-26 14:09:45.449 | INFO     | __main__:train:49 - Train Step 7\n",
      "2022-04-26 14:09:45.450 | INFO     | __main__:train:51 - mae 5.31e-01 loss 2.09e-01 R -0.088 gap 0.009706159122288227 preds 0.031237494200468063\n",
      "2022-04-26 14:09:45.537 | INFO     | __main__:train:49 - Train Step 8\n",
      "2022-04-26 14:09:45.537 | INFO     | __main__:train:51 - mae 5.05e-01 loss 1.95e-01 R -0.075 gap 0.009195903316140175 preds 0.033180978149175644\n",
      "2022-04-26 14:09:45.620 | INFO     | __main__:train:49 - Train Step 9\n",
      "2022-04-26 14:09:45.621 | INFO     | __main__:train:51 - mae 5.03e-01 loss 1.92e-01 R -0.074 gap 0.00903881061822176 preds 0.025895068421959877\n",
      "2022-04-26 14:09:45.967 | INFO     | __main__:train:49 - Train Step 10\n",
      "2022-04-26 14:09:45.968 | INFO     | __main__:train:51 - mae 4.96e-01 loss 1.89e-01 R -0.086 gap 0.00936820823699236 preds 0.02841608226299286\n",
      "2022-04-26 14:09:46.058 | INFO     | __main__:train:49 - Train Step 11\n",
      "2022-04-26 14:09:46.059 | INFO     | __main__:train:51 - mae 4.88e-01 loss 1.83e-01 R -0.083 gap 0.009173455648124218 preds 0.029250813648104668\n",
      "2022-04-26 14:09:46.139 | INFO     | __main__:train:49 - Train Step 12\n",
      "2022-04-26 14:09:46.140 | INFO     | __main__:train:51 - mae 4.76e-01 loss 1.78e-01 R -0.063 gap 0.00879585463553667 preds 0.027743972837924957\n",
      "2022-04-26 14:09:46.216 | INFO     | __main__:train:49 - Train Step 13\n",
      "2022-04-26 14:09:46.217 | INFO     | __main__:train:51 - mae 4.77e-01 loss 1.77e-01 R -0.052 gap 0.008959870785474777 preds 0.027425825595855713\n",
      "2022-04-26 14:09:46.292 | INFO     | __main__:train:49 - Train Step 14\n",
      "2022-04-26 14:09:46.293 | INFO     | __main__:train:51 - mae 4.73e-01 loss 1.73e-01 R -0.052 gap 0.00871039368212223 preds 0.02895832434296608\n",
      "2022-04-26 14:09:46.372 | INFO     | __main__:train:49 - Train Step 15\n",
      "2022-04-26 14:09:46.373 | INFO     | __main__:train:51 - mae 4.71e-01 loss 1.72e-01 R -0.044 gap 0.00872704479843378 preds 0.030938472598791122\n",
      "2022-04-26 14:09:46.450 | INFO     | __main__:train:49 - Train Step 16\n",
      "2022-04-26 14:09:46.451 | INFO     | __main__:train:51 - mae 4.68e-01 loss 1.69e-01 R -0.042 gap 0.009112601168453693 preds 0.029170509427785873\n",
      "2022-04-26 14:09:46.540 | INFO     | __main__:train:49 - Train Step 17\n",
      "2022-04-26 14:09:46.541 | INFO     | __main__:train:51 - mae 4.64e-01 loss 1.68e-01 R -0.040 gap 0.009386197663843632 preds 0.026379704475402832\n",
      "2022-04-26 14:09:46.631 | INFO     | __main__:train:49 - Train Step 18\n",
      "2022-04-26 14:09:46.632 | INFO     | __main__:train:51 - mae 4.64e-01 loss 1.68e-01 R -0.034 gap 0.009508882649242878 preds 0.02617490664124489\n",
      "2022-04-26 14:09:46.717 | INFO     | __main__:train:49 - Train Step 19\n",
      "2022-04-26 14:09:46.718 | INFO     | __main__:train:51 - mae 4.61e-01 loss 1.66e-01 R -0.032 gap 0.009303011000156403 preds 0.024049166589975357\n",
      "2022-04-26 14:09:47.049 | INFO     | __main__:train:49 - Train Step 20\n",
      "2022-04-26 14:09:47.050 | INFO     | __main__:train:51 - mae 4.58e-01 loss 1.64e-01 R -0.034 gap 0.009321214631199837 preds 0.024463247507810593\n",
      "2022-04-26 14:09:47.135 | INFO     | __main__:train:49 - Train Step 21\n",
      "2022-04-26 14:09:47.135 | INFO     | __main__:train:51 - mae 4.56e-01 loss 1.62e-01 R -0.033 gap 0.009464127011597157 preds 0.02391761541366577\n",
      "2022-04-26 14:09:47.220 | INFO     | __main__:train:49 - Train Step 22\n",
      "2022-04-26 14:09:47.220 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.59e-01 R -0.035 gap 0.009539647959172726 preds 0.021072374656796455\n",
      "2022-04-26 14:09:47.298 | INFO     | __main__:train:49 - Train Step 23\n",
      "2022-04-26 14:09:47.298 | INFO     | __main__:train:51 - mae 4.53e-01 loss 1.60e-01 R -0.029 gap 0.009645795449614525 preds 0.01995895244181156\n",
      "2022-04-26 14:09:47.381 | INFO     | __main__:train:49 - Train Step 24\n",
      "2022-04-26 14:09:47.382 | INFO     | __main__:train:51 - mae 4.55e-01 loss 1.62e-01 R -0.029 gap 0.009625536389648914 preds 0.017964573577046394\n",
      "2022-04-26 14:09:47.466 | INFO     | __main__:train:49 - Train Step 25\n",
      "2022-04-26 14:09:47.467 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.60e-01 R -0.032 gap 0.009601805359125137 preds 0.016235677525401115\n",
      "2022-04-26 14:09:47.551 | INFO     | __main__:train:49 - Train Step 26\n",
      "2022-04-26 14:09:47.552 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.59e-01 R -0.024 gap 0.00953223928809166 preds 0.016479674726724625\n",
      "2022-04-26 14:09:47.638 | INFO     | __main__:train:49 - Train Step 27\n",
      "2022-04-26 14:09:47.639 | INFO     | __main__:train:51 - mae 4.50e-01 loss 1.57e-01 R -0.035 gap 0.009591654874384403 preds 0.015103872865438461\n",
      "2022-04-26 14:09:47.721 | INFO     | __main__:train:49 - Train Step 28\n",
      "2022-04-26 14:09:47.722 | INFO     | __main__:train:51 - mae 4.47e-01 loss 1.55e-01 R -0.025 gap 0.009695478715002537 preds 0.014313817955553532\n",
      "2022-04-26 14:09:47.822 | INFO     | __main__:train:49 - Train Step 29\n",
      "2022-04-26 14:09:47.823 | INFO     | __main__:train:51 - mae 4.45e-01 loss 1.54e-01 R -0.020 gap 0.00965841580182314 preds 0.012795012444257736\n",
      "2022-04-26 14:09:48.147 | INFO     | __main__:train:49 - Train Step 30\n",
      "2022-04-26 14:09:48.148 | INFO     | __main__:train:51 - mae 4.43e-01 loss 1.52e-01 R -0.012 gap 0.009535347111523151 preds 0.012421122752130032\n",
      "2022-04-26 14:09:48.230 | INFO     | __main__:train:49 - Train Step 31\n",
      "2022-04-26 14:09:48.231 | INFO     | __main__:train:51 - mae 4.37e-01 loss 1.49e-01 R -0.005 gap 0.00960548035800457 preds 0.01213536411523819\n",
      "2022-04-26 14:09:48.315 | INFO     | __main__:train:49 - Train Step 32\n",
      "2022-04-26 14:09:48.316 | INFO     | __main__:train:51 - mae 4.37e-01 loss 1.48e-01 R -0.018 gap 0.009633762761950493 preds 0.012612031772732735\n",
      "2022-04-26 14:09:48.399 | INFO     | __main__:train:49 - Train Step 33\n",
      "2022-04-26 14:09:48.399 | INFO     | __main__:train:51 - mae 4.35e-01 loss 1.47e-01 R -0.021 gap 0.009807875379920006 preds 0.012846296653151512\n",
      "2022-04-26 14:09:48.482 | INFO     | __main__:train:49 - Train Step 34\n",
      "2022-04-26 14:09:48.483 | INFO     | __main__:train:51 - mae 4.32e-01 loss 1.45e-01 R -0.017 gap 0.010115692391991615 preds 0.013313055969774723\n",
      "2022-04-26 14:09:48.568 | INFO     | __main__:train:49 - Train Step 35\n",
      "2022-04-26 14:09:48.569 | INFO     | __main__:train:51 - mae 4.26e-01 loss 1.43e-01 R -0.015 gap 0.010060813277959824 preds 0.012380254454910755\n",
      "2022-04-26 14:09:48.651 | INFO     | __main__:train:49 - Train Step 36\n",
      "2022-04-26 14:09:48.652 | INFO     | __main__:train:51 - mae 4.21e-01 loss 1.40e-01 R -0.012 gap 0.009956317022442818 preds 0.013064093887805939\n",
      "2022-04-26 14:09:48.736 | INFO     | __main__:train:49 - Train Step 37\n",
      "2022-04-26 14:09:48.737 | INFO     | __main__:train:51 - mae 4.16e-01 loss 1.38e-01 R -0.008 gap 0.010074797086417675 preds 0.01291937567293644\n",
      "2022-04-26 14:09:48.817 | INFO     | __main__:train:49 - Train Step 38\n",
      "2022-04-26 14:09:48.817 | INFO     | __main__:train:51 - mae 4.12e-01 loss 1.36e-01 R -0.007 gap 0.010006190277636051 preds 0.013730262406170368\n",
      "2022-04-26 14:09:48.899 | INFO     | __main__:train:49 - Train Step 39\n",
      "2022-04-26 14:09:48.900 | INFO     | __main__:train:51 - mae 4.07e-01 loss 1.33e-01 R -0.008 gap 0.010199090465903282 preds 0.013438989408314228\n",
      "2022-04-26 14:09:49.226 | INFO     | __main__:train:49 - Train Step 40\n",
      "2022-04-26 14:09:49.227 | INFO     | __main__:train:51 - mae 4.06e-01 loss 1.32e-01 R -0.013 gap 0.009995566681027412 preds 0.013180620968341827\n",
      "2022-04-26 14:09:49.311 | INFO     | __main__:train:49 - Train Step 41\n",
      "2022-04-26 14:09:49.312 | INFO     | __main__:train:51 - mae 4.03e-01 loss 1.30e-01 R -0.008 gap 0.010075002908706665 preds 0.013499814085662365\n",
      "2022-04-26 14:09:49.395 | INFO     | __main__:train:49 - Train Step 42\n",
      "2022-04-26 14:09:49.396 | INFO     | __main__:train:51 - mae 3.98e-01 loss 1.28e-01 R -0.008 gap 0.010182539001107216 preds 0.012693716213107109\n",
      "2022-04-26 14:09:49.481 | INFO     | __main__:train:49 - Train Step 43\n",
      "2022-04-26 14:09:49.481 | INFO     | __main__:train:51 - mae 3.95e-01 loss 1.26e-01 R -0.002 gap 0.009990985505282879 preds 0.011557983234524727\n",
      "2022-04-26 14:09:49.567 | INFO     | __main__:train:49 - Train Step 44\n",
      "2022-04-26 14:09:49.568 | INFO     | __main__:train:51 - mae 3.92e-01 loss 1.25e-01 R -0.001 gap 0.009867644868791103 preds 0.012679332867264748\n",
      "2022-04-26 14:09:49.656 | INFO     | __main__:train:49 - Train Step 45\n",
      "2022-04-26 14:09:49.656 | INFO     | __main__:train:51 - mae 3.91e-01 loss 1.24e-01 R -0.005 gap 0.009847410954535007 preds 0.012875852175056934\n",
      "2022-04-26 14:09:49.740 | INFO     | __main__:train:49 - Train Step 46\n",
      "2022-04-26 14:09:49.741 | INFO     | __main__:train:51 - mae 3.87e-01 loss 1.22e-01 R -0.001 gap 0.009881227277219296 preds 0.013133466243743896\n",
      "2022-04-26 14:09:49.826 | INFO     | __main__:train:49 - Train Step 47\n",
      "2022-04-26 14:09:49.827 | INFO     | __main__:train:51 - mae 3.84e-01 loss 1.20e-01 R 0.003 gap 0.009830137714743614 preds 0.013966417871415615\n",
      "2022-04-26 14:09:49.905 | INFO     | __main__:train:49 - Train Step 48\n",
      "2022-04-26 14:09:49.906 | INFO     | __main__:train:51 - mae 3.82e-01 loss 1.19e-01 R 0.001 gap 0.009832832030951977 preds 0.014268450438976288\n",
      "2022-04-26 14:09:49.984 | INFO     | __main__:train:49 - Train Step 49\n",
      "2022-04-26 14:09:49.985 | INFO     | __main__:train:51 - mae 3.80e-01 loss 1.18e-01 R 0.001 gap 0.009650932624936104 preds 0.014606147073209286\n",
      "2022-04-26 14:09:50.311 | INFO     | __main__:train:49 - Train Step 50\n",
      "2022-04-26 14:09:50.312 | INFO     | __main__:train:51 - mae 3.77e-01 loss 1.17e-01 R 0.005 gap 0.00978623703122139 preds 0.014768715016543865\n",
      "2022-04-26 14:09:50.396 | INFO     | __main__:train:49 - Train Step 51\n",
      "2022-04-26 14:09:50.397 | INFO     | __main__:train:51 - mae 3.75e-01 loss 1.15e-01 R 0.009 gap 0.009832006879150867 preds 0.014220447279512882\n",
      "2022-04-26 14:09:50.481 | INFO     | __main__:train:49 - Train Step 52\n",
      "2022-04-26 14:09:50.481 | INFO     | __main__:train:51 - mae 3.73e-01 loss 1.14e-01 R 0.007 gap 0.00978753063827753 preds 0.013851453550159931\n",
      "2022-04-26 14:09:50.567 | INFO     | __main__:train:49 - Train Step 53\n",
      "2022-04-26 14:09:50.568 | INFO     | __main__:train:51 - mae 3.70e-01 loss 1.13e-01 R 0.007 gap 0.00963086262345314 preds 0.014222826808691025\n",
      "2022-04-26 14:09:50.653 | INFO     | __main__:train:49 - Train Step 54\n",
      "2022-04-26 14:09:50.653 | INFO     | __main__:train:51 - mae 3.68e-01 loss 1.12e-01 R 0.011 gap 0.00961339846253395 preds 0.015113799832761288\n",
      "2022-04-26 14:09:50.738 | INFO     | __main__:train:49 - Train Step 55\n",
      "2022-04-26 14:09:50.739 | INFO     | __main__:train:51 - mae 3.66e-01 loss 1.11e-01 R 0.007 gap 0.009529389441013336 preds 0.014625848270952702\n",
      "2022-04-26 14:09:50.824 | INFO     | __main__:train:49 - Train Step 56\n",
      "2022-04-26 14:09:50.824 | INFO     | __main__:train:51 - mae 3.63e-01 loss 1.09e-01 R 0.005 gap 0.00950856413692236 preds 0.014218838885426521\n",
      "2022-04-26 14:09:50.905 | INFO     | __main__:train:49 - Train Step 57\n",
      "2022-04-26 14:09:50.906 | INFO     | __main__:train:51 - mae 3.60e-01 loss 1.08e-01 R 0.007 gap 0.009520218707621098 preds 0.01373262144625187\n",
      "2022-04-26 14:09:50.989 | INFO     | __main__:train:49 - Train Step 58\n",
      "2022-04-26 14:09:50.990 | INFO     | __main__:train:51 - mae 3.56e-01 loss 1.06e-01 R 0.010 gap 0.009496393613517284 preds 0.013425331562757492\n",
      "2022-04-26 14:09:51.073 | INFO     | __main__:train:49 - Train Step 59\n",
      "2022-04-26 14:09:51.074 | INFO     | __main__:train:51 - mae 3.54e-01 loss 1.05e-01 R 0.005 gap 0.00950838066637516 preds 0.013215573504567146\n",
      "2022-04-26 14:09:51.419 | INFO     | __main__:train:49 - Train Step 60\n",
      "2022-04-26 14:09:51.420 | INFO     | __main__:train:51 - mae 3.52e-01 loss 1.04e-01 R 0.006 gap 0.009566368535161018 preds 0.013053636066615582\n",
      "2022-04-26 14:09:51.506 | INFO     | __main__:train:49 - Train Step 61\n",
      "2022-04-26 14:09:51.507 | INFO     | __main__:train:51 - mae 3.49e-01 loss 1.03e-01 R 0.010 gap 0.009535671211779118 preds 0.01269760262221098\n",
      "2022-04-26 14:09:51.595 | INFO     | __main__:train:49 - Train Step 62\n",
      "2022-04-26 14:09:51.596 | INFO     | __main__:train:51 - mae 3.46e-01 loss 1.02e-01 R 0.012 gap 0.009559566155076027 preds 0.012621410191059113\n",
      "2022-04-26 14:09:51.679 | INFO     | __main__:train:49 - Train Step 63\n",
      "2022-04-26 14:09:51.680 | INFO     | __main__:train:51 - mae 3.44e-01 loss 1.00e-01 R 0.012 gap 0.009615330025553703 preds 0.012605172581970692\n",
      "2022-04-26 14:09:51.759 | INFO     | __main__:train:49 - Train Step 64\n",
      "2022-04-26 14:09:51.760 | INFO     | __main__:train:51 - mae 3.42e-01 loss 9.94e-02 R 0.010 gap 0.009656557813286781 preds 0.013419980183243752\n",
      "2022-04-26 14:09:51.841 | INFO     | __main__:train:49 - Train Step 65\n",
      "2022-04-26 14:09:51.842 | INFO     | __main__:train:51 - mae 3.40e-01 loss 9.84e-02 R 0.009 gap 0.009610779583454132 preds 0.014330883510410786\n",
      "2022-04-26 14:09:51.921 | INFO     | __main__:train:49 - Train Step 66\n",
      "2022-04-26 14:09:51.922 | INFO     | __main__:train:51 - mae 3.37e-01 loss 9.72e-02 R 0.009 gap 0.009604188613593578 preds 0.014629208482801914\n",
      "2022-04-26 14:09:51.998 | INFO     | __main__:train:49 - Train Step 67\n",
      "2022-04-26 14:09:51.998 | INFO     | __main__:train:51 - mae 3.34e-01 loss 9.60e-02 R 0.012 gap 0.009728124365210533 preds 0.014764221385121346\n",
      "2022-04-26 14:09:52.077 | INFO     | __main__:train:49 - Train Step 68\n",
      "2022-04-26 14:09:52.078 | INFO     | __main__:train:51 - mae 3.32e-01 loss 9.50e-02 R 0.015 gap 0.009660356678068638 preds 0.014338687993586063\n",
      "2022-04-26 14:09:52.164 | INFO     | __main__:train:49 - Train Step 69\n",
      "2022-04-26 14:09:52.164 | INFO     | __main__:train:51 - mae 3.29e-01 loss 9.39e-02 R 0.013 gap 0.009696906432509422 preds 0.014317745342850685\n",
      "2022-04-26 14:09:52.480 | INFO     | __main__:train:49 - Train Step 70\n",
      "2022-04-26 14:09:52.481 | INFO     | __main__:train:51 - mae 3.28e-01 loss 9.31e-02 R 0.013 gap 0.009723171591758728 preds 0.01416220422834158\n",
      "2022-04-26 14:09:52.568 | INFO     | __main__:train:49 - Train Step 71\n",
      "2022-04-26 14:09:52.569 | INFO     | __main__:train:51 - mae 3.25e-01 loss 9.22e-02 R 0.014 gap 0.009844101034104824 preds 0.013509456068277359\n",
      "2022-04-26 14:09:52.648 | INFO     | __main__:train:49 - Train Step 72\n",
      "2022-04-26 14:09:52.648 | INFO     | __main__:train:51 - mae 3.23e-01 loss 9.13e-02 R 0.015 gap 0.009785688482224941 preds 0.013401927426457405\n",
      "2022-04-26 14:09:52.726 | INFO     | __main__:train:49 - Train Step 73\n",
      "2022-04-26 14:09:52.727 | INFO     | __main__:train:51 - mae 3.21e-01 loss 9.04e-02 R 0.017 gap 0.009790904819965363 preds 0.013337558135390282\n",
      "2022-04-26 14:09:52.804 | INFO     | __main__:train:49 - Train Step 74\n",
      "2022-04-26 14:09:52.805 | INFO     | __main__:train:51 - mae 3.19e-01 loss 8.97e-02 R 0.017 gap 0.009751061908900738 preds 0.012590655125677586\n",
      "2022-04-26 14:09:52.881 | INFO     | __main__:train:49 - Train Step 75\n",
      "2022-04-26 14:09:52.882 | INFO     | __main__:train:51 - mae 3.18e-01 loss 8.90e-02 R 0.019 gap 0.009718772955238819 preds 0.011811284348368645\n",
      "2022-04-26 14:09:52.963 | INFO     | __main__:train:49 - Train Step 76\n",
      "2022-04-26 14:09:52.963 | INFO     | __main__:train:51 - mae 3.16e-01 loss 8.82e-02 R 0.020 gap 0.009716865606606007 preds 0.011315525509417057\n",
      "2022-04-26 14:09:53.044 | INFO     | __main__:train:49 - Train Step 77\n",
      "2022-04-26 14:09:53.045 | INFO     | __main__:train:51 - mae 3.14e-01 loss 8.75e-02 R 0.018 gap 0.009789185598492622 preds 0.010933627374470234\n",
      "2022-04-26 14:09:53.124 | INFO     | __main__:train:49 - Train Step 78\n",
      "2022-04-26 14:09:53.125 | INFO     | __main__:train:51 - mae 3.13e-01 loss 8.67e-02 R 0.019 gap 0.009741874411702156 preds 0.01073919702321291\n",
      "2022-04-26 14:09:53.204 | INFO     | __main__:train:49 - Train Step 79\n",
      "2022-04-26 14:09:53.205 | INFO     | __main__:train:51 - mae 3.11e-01 loss 8.60e-02 R 0.019 gap 0.009720311500132084 preds 0.010212235152721405\n",
      "2022-04-26 14:09:53.510 | INFO     | __main__:train:49 - Train Step 80\n",
      "2022-04-26 14:09:53.511 | INFO     | __main__:train:51 - mae 3.10e-01 loss 8.54e-02 R 0.013 gap 0.009746791794896126 preds 0.010089725255966187\n",
      "2022-04-26 14:09:53.593 | INFO     | __main__:train:49 - Train Step 81\n",
      "2022-04-26 14:09:53.594 | INFO     | __main__:train:51 - mae 3.09e-01 loss 8.49e-02 R 0.011 gap 0.009747661650180817 preds 0.009931462816894054\n",
      "2022-04-26 14:09:53.673 | INFO     | __main__:train:49 - Train Step 82\n",
      "2022-04-26 14:09:53.674 | INFO     | __main__:train:51 - mae 3.07e-01 loss 8.41e-02 R 0.011 gap 0.009738363325595856 preds 0.009765234775841236\n",
      "2022-04-26 14:09:53.748 | INFO     | __main__:train:49 - Train Step 83\n",
      "2022-04-26 14:09:53.749 | INFO     | __main__:train:51 - mae 3.06e-01 loss 8.36e-02 R 0.014 gap 0.009874267503619194 preds 0.009977382607758045\n",
      "2022-04-26 14:09:53.823 | INFO     | __main__:train:49 - Train Step 84\n",
      "2022-04-26 14:09:53.824 | INFO     | __main__:train:51 - mae 3.05e-01 loss 8.30e-02 R 0.016 gap 0.009965146891772747 preds 0.009802517481148243\n",
      "2022-04-26 14:09:53.900 | INFO     | __main__:train:49 - Train Step 85\n",
      "2022-04-26 14:09:53.900 | INFO     | __main__:train:51 - mae 3.03e-01 loss 8.22e-02 R 0.016 gap 0.009839922189712524 preds 0.009742122143507004\n",
      "2022-04-26 14:09:53.975 | INFO     | __main__:train:49 - Train Step 86\n",
      "2022-04-26 14:09:53.976 | INFO     | __main__:train:51 - mae 3.01e-01 loss 8.15e-02 R 0.016 gap 0.009795043617486954 preds 0.009930548258125782\n",
      "2022-04-26 14:09:54.055 | INFO     | __main__:train:49 - Train Step 87\n",
      "2022-04-26 14:09:54.055 | INFO     | __main__:train:51 - mae 2.99e-01 loss 8.08e-02 R 0.016 gap 0.009769131429493427 preds 0.009935279376804829\n",
      "2022-04-26 14:09:54.136 | INFO     | __main__:train:49 - Train Step 88\n",
      "2022-04-26 14:09:54.137 | INFO     | __main__:train:51 - mae 2.98e-01 loss 8.01e-02 R 0.016 gap 0.009724400006234646 preds 0.00965889636427164\n",
      "2022-04-26 14:09:54.216 | INFO     | __main__:train:49 - Train Step 89\n",
      "2022-04-26 14:09:54.217 | INFO     | __main__:train:51 - mae 2.96e-01 loss 7.93e-02 R 0.016 gap 0.009822453372180462 preds 0.009583651088178158\n",
      "2022-04-26 14:10:15.860 | INFO     | __main__:test:57 - Test epoch 0\n",
      "2022-04-26 14:10:15.861 | INFO     | __main__:test:59 - mae 1.46e-01 loss 1.83e-02 R -0.020 l_test 9.15e-01 l_train 8.96e-01 \n",
      "2022-04-26 14:10:15.863 | INFO     | __main__:<module>:4 - Epoch 1\n",
      "2022-04-26 14:10:15.863 | INFO     | __main__:<module>:5 - Bank size: 15520\n",
      "2022-04-26 14:10:16.199 | INFO     | __main__:train:49 - Train Step 90\n",
      "2022-04-26 14:10:16.200 | INFO     | __main__:train:51 - mae 2.94e-01 loss 7.87e-02 R 0.014 gap 0.00980132445693016 preds 0.009129748679697514\n",
      "2022-04-26 14:10:16.289 | INFO     | __main__:train:49 - Train Step 91\n",
      "2022-04-26 14:10:16.290 | INFO     | __main__:train:51 - mae 2.93e-01 loss 7.80e-02 R 0.015 gap 0.009785423055291176 preds 0.009424682706594467\n",
      "2022-04-26 14:10:16.373 | INFO     | __main__:train:49 - Train Step 92\n",
      "2022-04-26 14:10:16.374 | INFO     | __main__:train:51 - mae 2.92e-01 loss 7.74e-02 R 0.015 gap 0.0098248440772295 preds 0.009421930648386478\n",
      "2022-04-26 14:10:16.456 | INFO     | __main__:train:49 - Train Step 93\n",
      "2022-04-26 14:10:16.456 | INFO     | __main__:train:51 - mae 2.90e-01 loss 7.67e-02 R 0.015 gap 0.009692821651697159 preds 0.009429804980754852\n",
      "2022-04-26 14:10:16.540 | INFO     | __main__:train:49 - Train Step 94\n",
      "2022-04-26 14:10:16.541 | INFO     | __main__:train:51 - mae 2.89e-01 loss 7.62e-02 R 0.017 gap 0.009649696759879589 preds 0.009272300638258457\n",
      "2022-04-26 14:10:16.626 | INFO     | __main__:train:49 - Train Step 95\n",
      "2022-04-26 14:10:16.626 | INFO     | __main__:train:51 - mae 2.88e-01 loss 7.56e-02 R 0.015 gap 0.009613236412405968 preds 0.009510635398328304\n",
      "2022-04-26 14:10:16.710 | INFO     | __main__:train:49 - Train Step 96\n",
      "2022-04-26 14:10:16.711 | INFO     | __main__:train:51 - mae 2.86e-01 loss 7.50e-02 R 0.014 gap 0.00964098796248436 preds 0.009631244465708733\n",
      "2022-04-26 14:10:16.795 | INFO     | __main__:train:49 - Train Step 97\n",
      "2022-04-26 14:10:16.796 | INFO     | __main__:train:51 - mae 2.85e-01 loss 7.44e-02 R 0.014 gap 0.009675970301032066 preds 0.009850570932030678\n",
      "2022-04-26 14:10:16.878 | INFO     | __main__:train:49 - Train Step 98\n",
      "2022-04-26 14:10:16.879 | INFO     | __main__:train:51 - mae 2.83e-01 loss 7.38e-02 R 0.014 gap 0.009647908620536327 preds 0.00990293174982071\n",
      "2022-04-26 14:10:16.964 | INFO     | __main__:train:49 - Train Step 99\n",
      "2022-04-26 14:10:16.965 | INFO     | __main__:train:51 - mae 2.82e-01 loss 7.32e-02 R 0.017 gap 0.009676225483417511 preds 0.01022128015756607\n",
      "2022-04-26 14:10:17.296 | INFO     | __main__:train:49 - Train Step 100\n",
      "2022-04-26 14:10:17.296 | INFO     | __main__:train:51 - mae 2.80e-01 loss 7.26e-02 R 0.018 gap 0.009640143252909184 preds 0.010635746642947197\n",
      "2022-04-26 14:10:17.380 | INFO     | __main__:train:49 - Train Step 101\n",
      "2022-04-26 14:10:17.381 | INFO     | __main__:train:51 - mae 2.79e-01 loss 7.20e-02 R 0.018 gap 0.009607885964214802 preds 0.011205925606191158\n",
      "2022-04-26 14:10:17.467 | INFO     | __main__:train:49 - Train Step 102\n",
      "2022-04-26 14:10:17.468 | INFO     | __main__:train:51 - mae 2.77e-01 loss 7.14e-02 R 0.019 gap 0.009610768407583237 preds 0.011227156966924667\n",
      "2022-04-26 14:10:17.553 | INFO     | __main__:train:49 - Train Step 103\n",
      "2022-04-26 14:10:17.554 | INFO     | __main__:train:51 - mae 2.76e-01 loss 7.09e-02 R 0.017 gap 0.009596442803740501 preds 0.011291330680251122\n",
      "2022-04-26 14:10:17.636 | INFO     | __main__:train:49 - Train Step 104\n",
      "2022-04-26 14:10:17.637 | INFO     | __main__:train:51 - mae 2.74e-01 loss 7.04e-02 R 0.017 gap 0.009607781656086445 preds 0.011314786039292812\n",
      "2022-04-26 14:10:17.718 | INFO     | __main__:train:49 - Train Step 105\n",
      "2022-04-26 14:10:17.719 | INFO     | __main__:train:51 - mae 2.73e-01 loss 6.99e-02 R 0.017 gap 0.009609920904040337 preds 0.011593939736485481\n",
      "2022-04-26 14:10:17.805 | INFO     | __main__:train:49 - Train Step 106\n",
      "2022-04-26 14:10:17.805 | INFO     | __main__:train:51 - mae 2.72e-01 loss 6.94e-02 R 0.018 gap 0.009632574394345284 preds 0.011647569015622139\n",
      "2022-04-26 14:10:17.884 | INFO     | __main__:train:49 - Train Step 107\n",
      "2022-04-26 14:10:17.885 | INFO     | __main__:train:51 - mae 2.71e-01 loss 6.89e-02 R 0.018 gap 0.009636582806706429 preds 0.011518892832100391\n",
      "2022-04-26 14:10:17.966 | INFO     | __main__:train:49 - Train Step 108\n",
      "2022-04-26 14:10:17.967 | INFO     | __main__:train:51 - mae 2.69e-01 loss 6.84e-02 R 0.018 gap 0.009537285193800926 preds 0.011098450981080532\n",
      "2022-04-26 14:10:18.051 | INFO     | __main__:train:49 - Train Step 109\n",
      "2022-04-26 14:10:18.052 | INFO     | __main__:train:51 - mae 2.68e-01 loss 6.80e-02 R 0.017 gap 0.009587647393345833 preds 0.011131169274449348\n",
      "2022-04-26 14:10:18.383 | INFO     | __main__:train:49 - Train Step 110\n",
      "2022-04-26 14:10:18.384 | INFO     | __main__:train:51 - mae 2.68e-01 loss 6.76e-02 R 0.015 gap 0.009601244702935219 preds 0.011027295142412186\n",
      "2022-04-26 14:10:18.464 | INFO     | __main__:train:49 - Train Step 111\n",
      "2022-04-26 14:10:18.464 | INFO     | __main__:train:51 - mae 2.66e-01 loss 6.71e-02 R 0.014 gap 0.009577344171702862 preds 0.011229349300265312\n",
      "2022-04-26 14:10:18.547 | INFO     | __main__:train:49 - Train Step 112\n",
      "2022-04-26 14:10:18.548 | INFO     | __main__:train:51 - mae 2.65e-01 loss 6.66e-02 R 0.014 gap 0.00960086565464735 preds 0.011019364930689335\n",
      "2022-04-26 14:10:18.631 | INFO     | __main__:train:49 - Train Step 113\n",
      "2022-04-26 14:10:18.632 | INFO     | __main__:train:51 - mae 2.63e-01 loss 6.61e-02 R 0.015 gap 0.009582464583218098 preds 0.01097731851041317\n",
      "2022-04-26 14:10:18.717 | INFO     | __main__:train:49 - Train Step 114\n",
      "2022-04-26 14:10:18.718 | INFO     | __main__:train:51 - mae 2.62e-01 loss 6.56e-02 R 0.015 gap 0.009566936641931534 preds 0.010975656099617481\n",
      "2022-04-26 14:10:18.799 | INFO     | __main__:train:49 - Train Step 115\n",
      "2022-04-26 14:10:18.800 | INFO     | __main__:train:51 - mae 2.61e-01 loss 6.52e-02 R 0.016 gap 0.00958009622991085 preds 0.010784023441374302\n",
      "2022-04-26 14:10:18.888 | INFO     | __main__:train:49 - Train Step 116\n",
      "2022-04-26 14:10:18.889 | INFO     | __main__:train:51 - mae 2.59e-01 loss 6.47e-02 R 0.018 gap 0.009593278169631958 preds 0.01079596858471632\n",
      "2022-04-26 14:10:18.974 | INFO     | __main__:train:49 - Train Step 117\n",
      "2022-04-26 14:10:18.974 | INFO     | __main__:train:51 - mae 2.58e-01 loss 6.43e-02 R 0.018 gap 0.00960321445018053 preds 0.011049509048461914\n",
      "2022-04-26 14:10:19.060 | INFO     | __main__:train:49 - Train Step 118\n",
      "2022-04-26 14:10:19.061 | INFO     | __main__:train:51 - mae 2.57e-01 loss 6.38e-02 R 0.019 gap 0.009586537256836891 preds 0.010664520785212517\n",
      "2022-04-26 14:10:19.142 | INFO     | __main__:train:49 - Train Step 119\n",
      "2022-04-26 14:10:19.143 | INFO     | __main__:train:51 - mae 2.56e-01 loss 6.34e-02 R 0.018 gap 0.009618443436920643 preds 0.010555369779467583\n",
      "2022-04-26 14:10:19.477 | INFO     | __main__:train:49 - Train Step 120\n",
      "2022-04-26 14:10:19.478 | INFO     | __main__:train:51 - mae 2.55e-01 loss 6.29e-02 R 0.017 gap 0.009589490480720997 preds 0.010638768784701824\n",
      "2022-04-26 14:10:19.562 | INFO     | __main__:train:49 - Train Step 121\n",
      "2022-04-26 14:10:19.562 | INFO     | __main__:train:51 - mae 2.54e-01 loss 6.25e-02 R 0.017 gap 0.009589756838977337 preds 0.01027639675885439\n",
      "2022-04-26 14:10:19.645 | INFO     | __main__:train:49 - Train Step 122\n",
      "2022-04-26 14:10:19.646 | INFO     | __main__:train:51 - mae 2.52e-01 loss 6.21e-02 R 0.017 gap 0.009515821933746338 preds 0.010258572176098824\n",
      "2022-04-26 14:10:19.727 | INFO     | __main__:train:49 - Train Step 123\n",
      "2022-04-26 14:10:19.728 | INFO     | __main__:train:51 - mae 2.52e-01 loss 6.18e-02 R 0.017 gap 0.009547124616801739 preds 0.009944731369614601\n",
      "2022-04-26 14:10:19.806 | INFO     | __main__:train:49 - Train Step 124\n",
      "2022-04-26 14:10:19.806 | INFO     | __main__:train:51 - mae 2.50e-01 loss 6.14e-02 R 0.018 gap 0.00959252379834652 preds 0.010276724584400654\n",
      "2022-04-26 14:10:19.889 | INFO     | __main__:train:49 - Train Step 125\n",
      "2022-04-26 14:10:19.890 | INFO     | __main__:train:51 - mae 2.49e-01 loss 6.10e-02 R 0.017 gap 0.009581828489899635 preds 0.009949705563485622\n",
      "2022-04-26 14:10:19.974 | INFO     | __main__:train:49 - Train Step 126\n",
      "2022-04-26 14:10:19.975 | INFO     | __main__:train:51 - mae 2.48e-01 loss 6.06e-02 R 0.017 gap 0.009563826024532318 preds 0.009817786514759064\n",
      "2022-04-26 14:10:20.053 | INFO     | __main__:train:49 - Train Step 127\n",
      "2022-04-26 14:10:20.054 | INFO     | __main__:train:51 - mae 2.47e-01 loss 6.02e-02 R 0.016 gap 0.009600811637938023 preds 0.009904637932777405\n",
      "2022-04-26 14:10:20.135 | INFO     | __main__:train:49 - Train Step 128\n",
      "2022-04-26 14:10:20.136 | INFO     | __main__:train:51 - mae 2.46e-01 loss 5.98e-02 R 0.017 gap 0.009560860693454742 preds 0.009895042516291142\n",
      "2022-04-26 14:10:20.223 | INFO     | __main__:train:49 - Train Step 129\n",
      "2022-04-26 14:10:20.224 | INFO     | __main__:train:51 - mae 2.45e-01 loss 5.94e-02 R 0.017 gap 0.009548060595989227 preds 0.009659970179200172\n",
      "2022-04-26 14:10:20.564 | INFO     | __main__:train:49 - Train Step 130\n",
      "2022-04-26 14:10:20.565 | INFO     | __main__:train:51 - mae 2.44e-01 loss 5.90e-02 R 0.018 gap 0.00959730800241232 preds 0.00971370842307806\n",
      "2022-04-26 14:10:20.650 | INFO     | __main__:train:49 - Train Step 131\n",
      "2022-04-26 14:10:20.651 | INFO     | __main__:train:51 - mae 2.43e-01 loss 5.86e-02 R 0.019 gap 0.00962680671364069 preds 0.009661952964961529\n",
      "2022-04-26 14:10:20.731 | INFO     | __main__:train:49 - Train Step 132\n",
      "2022-04-26 14:10:20.732 | INFO     | __main__:train:51 - mae 2.42e-01 loss 5.83e-02 R 0.020 gap 0.009702910669147968 preds 0.009509450756013393\n",
      "2022-04-26 14:10:20.814 | INFO     | __main__:train:49 - Train Step 133\n",
      "2022-04-26 14:10:20.815 | INFO     | __main__:train:51 - mae 2.41e-01 loss 5.80e-02 R 0.020 gap 0.009677983820438385 preds 0.009633095934987068\n",
      "2022-04-26 14:10:20.899 | INFO     | __main__:train:49 - Train Step 134\n",
      "2022-04-26 14:10:20.900 | INFO     | __main__:train:51 - mae 2.40e-01 loss 5.77e-02 R 0.019 gap 0.009652834385633469 preds 0.00959691684693098\n",
      "2022-04-26 14:10:20.980 | INFO     | __main__:train:49 - Train Step 135\n",
      "2022-04-26 14:10:20.981 | INFO     | __main__:train:51 - mae 2.39e-01 loss 5.73e-02 R 0.020 gap 0.009637216106057167 preds 0.01001318171620369\n",
      "2022-04-26 14:10:21.066 | INFO     | __main__:train:49 - Train Step 136\n",
      "2022-04-26 14:10:21.067 | INFO     | __main__:train:51 - mae 2.39e-01 loss 5.70e-02 R 0.020 gap 0.009621887467801571 preds 0.009970247745513916\n",
      "2022-04-26 14:10:21.150 | INFO     | __main__:train:49 - Train Step 137\n",
      "2022-04-26 14:10:21.151 | INFO     | __main__:train:51 - mae 2.38e-01 loss 5.67e-02 R 0.019 gap 0.009622904472053051 preds 0.009726537391543388\n",
      "2022-04-26 14:10:21.230 | INFO     | __main__:train:49 - Train Step 138\n",
      "2022-04-26 14:10:21.230 | INFO     | __main__:train:51 - mae 2.37e-01 loss 5.64e-02 R 0.019 gap 0.009655148722231388 preds 0.009573211893439293\n",
      "2022-04-26 14:10:21.311 | INFO     | __main__:train:49 - Train Step 139\n",
      "2022-04-26 14:10:21.312 | INFO     | __main__:train:51 - mae 2.36e-01 loss 5.60e-02 R 0.019 gap 0.009639810770750046 preds 0.009674766100943089\n",
      "2022-04-26 14:10:21.652 | INFO     | __main__:train:49 - Train Step 140\n",
      "2022-04-26 14:10:21.653 | INFO     | __main__:train:51 - mae 2.35e-01 loss 5.57e-02 R 0.020 gap 0.00965897087007761 preds 0.009817982092499733\n",
      "2022-04-26 14:10:21.738 | INFO     | __main__:train:49 - Train Step 141\n",
      "2022-04-26 14:10:21.739 | INFO     | __main__:train:51 - mae 2.34e-01 loss 5.55e-02 R 0.020 gap 0.009671556763350964 preds 0.010264662094414234\n",
      "2022-04-26 14:10:21.825 | INFO     | __main__:train:49 - Train Step 142\n",
      "2022-04-26 14:10:21.826 | INFO     | __main__:train:51 - mae 2.34e-01 loss 5.52e-02 R 0.020 gap 0.009724521078169346 preds 0.010321295820176601\n",
      "2022-04-26 14:10:21.910 | INFO     | __main__:train:49 - Train Step 143\n",
      "2022-04-26 14:10:21.911 | INFO     | __main__:train:51 - mae 2.33e-01 loss 5.49e-02 R 0.020 gap 0.009734174236655235 preds 0.010370125994086266\n",
      "2022-04-26 14:10:21.997 | INFO     | __main__:train:49 - Train Step 144\n",
      "2022-04-26 14:10:21.998 | INFO     | __main__:train:51 - mae 2.32e-01 loss 5.46e-02 R 0.019 gap 0.009756321087479591 preds 0.010378766804933548\n",
      "2022-04-26 14:10:22.087 | INFO     | __main__:train:49 - Train Step 145\n",
      "2022-04-26 14:10:22.088 | INFO     | __main__:train:51 - mae 2.31e-01 loss 5.44e-02 R 0.019 gap 0.009722773917019367 preds 0.010313876904547215\n",
      "2022-04-26 14:10:22.174 | INFO     | __main__:train:49 - Train Step 146\n",
      "2022-04-26 14:10:22.175 | INFO     | __main__:train:51 - mae 2.31e-01 loss 5.41e-02 R 0.019 gap 0.009708241559565067 preds 0.010380569845438004\n",
      "2022-04-26 14:10:22.260 | INFO     | __main__:train:49 - Train Step 147\n",
      "2022-04-26 14:10:22.261 | INFO     | __main__:train:51 - mae 2.30e-01 loss 5.39e-02 R 0.019 gap 0.009699338115751743 preds 0.010270942002534866\n",
      "2022-04-26 14:10:22.347 | INFO     | __main__:train:49 - Train Step 148\n",
      "2022-04-26 14:10:22.348 | INFO     | __main__:train:51 - mae 2.29e-01 loss 5.36e-02 R 0.018 gap 0.009682881645858288 preds 0.010121696628630161\n",
      "2022-04-26 14:10:22.433 | INFO     | __main__:train:49 - Train Step 149\n",
      "2022-04-26 14:10:22.434 | INFO     | __main__:train:51 - mae 2.28e-01 loss 5.34e-02 R 0.019 gap 0.009640298783779144 preds 0.010350558906793594\n",
      "2022-04-26 14:10:22.775 | INFO     | __main__:train:49 - Train Step 150\n",
      "2022-04-26 14:10:22.776 | INFO     | __main__:train:51 - mae 2.28e-01 loss 5.31e-02 R 0.019 gap 0.009675467386841774 preds 0.010176105424761772\n",
      "2022-04-26 14:10:22.860 | INFO     | __main__:train:49 - Train Step 151\n",
      "2022-04-26 14:10:22.861 | INFO     | __main__:train:51 - mae 2.27e-01 loss 5.28e-02 R 0.019 gap 0.009714575484395027 preds 0.010424256324768066\n",
      "2022-04-26 14:10:22.946 | INFO     | __main__:train:49 - Train Step 152\n",
      "2022-04-26 14:10:22.947 | INFO     | __main__:train:51 - mae 2.26e-01 loss 5.26e-02 R 0.019 gap 0.009717931970953941 preds 0.010137414559721947\n",
      "2022-04-26 14:10:23.033 | INFO     | __main__:train:49 - Train Step 153\n",
      "2022-04-26 14:10:23.034 | INFO     | __main__:train:51 - mae 2.26e-01 loss 5.24e-02 R 0.019 gap 0.009741765446960926 preds 0.010251044295728207\n",
      "2022-04-26 14:10:23.121 | INFO     | __main__:train:49 - Train Step 154\n",
      "2022-04-26 14:10:23.122 | INFO     | __main__:train:51 - mae 2.25e-01 loss 5.22e-02 R 0.018 gap 0.009750321507453918 preds 0.010261605493724346\n",
      "2022-04-26 14:10:23.210 | INFO     | __main__:train:49 - Train Step 155\n",
      "2022-04-26 14:10:23.211 | INFO     | __main__:train:51 - mae 2.24e-01 loss 5.19e-02 R 0.017 gap 0.00972975604236126 preds 0.010271390900015831\n",
      "2022-04-26 14:10:23.298 | INFO     | __main__:train:49 - Train Step 156\n",
      "2022-04-26 14:10:23.298 | INFO     | __main__:train:51 - mae 2.24e-01 loss 5.17e-02 R 0.016 gap 0.009758283384144306 preds 0.010141866281628609\n",
      "2022-04-26 14:10:23.386 | INFO     | __main__:train:49 - Train Step 157\n",
      "2022-04-26 14:10:23.387 | INFO     | __main__:train:51 - mae 2.23e-01 loss 5.14e-02 R 0.016 gap 0.009754867292940617 preds 0.01008358784019947\n",
      "2022-04-26 14:10:23.473 | INFO     | __main__:train:49 - Train Step 158\n",
      "2022-04-26 14:10:23.474 | INFO     | __main__:train:51 - mae 2.22e-01 loss 5.12e-02 R 0.016 gap 0.009776972234249115 preds 0.009923805482685566\n",
      "2022-04-26 14:10:23.560 | INFO     | __main__:train:49 - Train Step 159\n",
      "2022-04-26 14:10:23.561 | INFO     | __main__:train:51 - mae 2.21e-01 loss 5.09e-02 R 0.017 gap 0.009772001765668392 preds 0.009846129454672337\n",
      "2022-04-26 14:10:23.898 | INFO     | __main__:train:49 - Train Step 160\n",
      "2022-04-26 14:10:23.899 | INFO     | __main__:train:51 - mae 2.20e-01 loss 5.06e-02 R 0.017 gap 0.009782286360859871 preds 0.010086679831147194\n",
      "2022-04-26 14:10:23.983 | INFO     | __main__:train:49 - Train Step 161\n",
      "2022-04-26 14:10:23.984 | INFO     | __main__:train:51 - mae 2.19e-01 loss 5.04e-02 R 0.018 gap 0.009763144887983799 preds 0.010165579617023468\n",
      "2022-04-26 14:10:24.071 | INFO     | __main__:train:49 - Train Step 162\n",
      "2022-04-26 14:10:24.072 | INFO     | __main__:train:51 - mae 2.19e-01 loss 5.01e-02 R 0.017 gap 0.009775874204933643 preds 0.010108229704201221\n",
      "2022-04-26 14:10:24.160 | INFO     | __main__:train:49 - Train Step 163\n",
      "2022-04-26 14:10:24.161 | INFO     | __main__:train:51 - mae 2.18e-01 loss 4.99e-02 R 0.017 gap 0.009788289666175842 preds 0.010020322166383266\n",
      "2022-04-26 14:10:24.250 | INFO     | __main__:train:49 - Train Step 164\n",
      "2022-04-26 14:10:24.251 | INFO     | __main__:train:51 - mae 2.17e-01 loss 4.96e-02 R 0.018 gap 0.009791215881705284 preds 0.010007106699049473\n",
      "2022-04-26 14:10:24.339 | INFO     | __main__:train:49 - Train Step 165\n",
      "2022-04-26 14:10:24.340 | INFO     | __main__:train:51 - mae 2.16e-01 loss 4.94e-02 R 0.018 gap 0.009774457663297653 preds 0.009961274452507496\n",
      "2022-04-26 14:10:24.428 | INFO     | __main__:train:49 - Train Step 166\n",
      "2022-04-26 14:10:24.429 | INFO     | __main__:train:51 - mae 2.16e-01 loss 4.91e-02 R 0.018 gap 0.00977952592074871 preds 0.010008004494011402\n",
      "2022-04-26 14:10:24.516 | INFO     | __main__:train:49 - Train Step 167\n",
      "2022-04-26 14:10:24.517 | INFO     | __main__:train:51 - mae 2.15e-01 loss 4.89e-02 R 0.019 gap 0.009779942221939564 preds 0.010007587261497974\n",
      "2022-04-26 14:10:24.609 | INFO     | __main__:train:49 - Train Step 168\n",
      "2022-04-26 14:10:24.610 | INFO     | __main__:train:51 - mae 2.14e-01 loss 4.87e-02 R 0.019 gap 0.00979638658463955 preds 0.010178283788263798\n",
      "2022-04-26 14:10:24.693 | INFO     | __main__:train:49 - Train Step 169\n",
      "2022-04-26 14:10:24.693 | INFO     | __main__:train:51 - mae 2.14e-01 loss 4.85e-02 R 0.019 gap 0.009815951809287071 preds 0.010378360748291016\n",
      "2022-04-26 14:10:25.036 | INFO     | __main__:train:49 - Train Step 170\n",
      "2022-04-26 14:10:25.037 | INFO     | __main__:train:51 - mae 2.13e-01 loss 4.83e-02 R 0.018 gap 0.009850293397903442 preds 0.010336294770240784\n",
      "2022-04-26 14:10:25.126 | INFO     | __main__:train:49 - Train Step 171\n",
      "2022-04-26 14:10:25.127 | INFO     | __main__:train:51 - mae 2.12e-01 loss 4.81e-02 R 0.018 gap 0.009887916035950184 preds 0.010254980064928532\n",
      "2022-04-26 14:10:25.214 | INFO     | __main__:train:49 - Train Step 172\n",
      "2022-04-26 14:10:25.215 | INFO     | __main__:train:51 - mae 2.12e-01 loss 4.79e-02 R 0.019 gap 0.009924301877617836 preds 0.010168427601456642\n",
      "2022-04-26 14:10:25.307 | INFO     | __main__:train:49 - Train Step 173\n",
      "2022-04-26 14:10:25.308 | INFO     | __main__:train:51 - mae 2.11e-01 loss 4.77e-02 R 0.017 gap 0.009908402338624 preds 0.010462847538292408\n",
      "2022-04-26 14:10:25.398 | INFO     | __main__:train:49 - Train Step 174\n",
      "2022-04-26 14:10:25.399 | INFO     | __main__:train:51 - mae 2.11e-01 loss 4.75e-02 R 0.017 gap 0.009928010404109955 preds 0.010542797856032848\n",
      "2022-04-26 14:10:25.489 | INFO     | __main__:train:49 - Train Step 175\n",
      "2022-04-26 14:10:25.490 | INFO     | __main__:train:51 - mae 2.10e-01 loss 4.73e-02 R 0.016 gap 0.009947075508534908 preds 0.010646476410329342\n",
      "2022-04-26 14:10:25.578 | INFO     | __main__:train:49 - Train Step 176\n",
      "2022-04-26 14:10:25.579 | INFO     | __main__:train:51 - mae 2.10e-01 loss 4.71e-02 R 0.016 gap 0.009925562888383865 preds 0.010752552188932896\n",
      "2022-04-26 14:10:25.671 | INFO     | __main__:train:49 - Train Step 177\n",
      "2022-04-26 14:10:25.672 | INFO     | __main__:train:51 - mae 2.09e-01 loss 4.69e-02 R 0.016 gap 0.009967883117496967 preds 0.010567664168775082\n",
      "2022-04-26 14:10:25.761 | INFO     | __main__:train:49 - Train Step 178\n",
      "2022-04-26 14:10:25.762 | INFO     | __main__:train:51 - mae 2.09e-01 loss 4.68e-02 R 0.015 gap 0.009957835078239441 preds 0.010672615841031075\n",
      "2022-04-26 14:10:47.243 | INFO     | __main__:test:57 - Test epoch 1\n",
      "2022-04-26 14:10:47.244 | INFO     | __main__:test:59 - mae 1.12e-01 loss 1.15e-02 R -0.010 l_test 9.30e-01 l_train 9.22e-01 \n"
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:38.091 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"\"#\"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(model_path).to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        cls1_loss_te = ce_criterion(preds_test[:, 6:10].squeeze(), y_test[:, 6].squeeze().long())\n",
    "        cls2_loss_te = ce_criterion(preds_test[:, 10:14].squeeze(), y_test[:, 7].squeeze().long())\n",
    "        cls3_loss_te = ce_criterion(preds_test[:, 14:18].squeeze(), y_test[:, 8].squeeze().long())\n",
    "        cls4_loss_te = ce_criterion(preds_test[:, 18:22].squeeze(), y_test[:, 9].squeeze().long())\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        l_test = (reg_loss_te.mean(-1).sum() + cls1_loss_te.mean(-1).sum() + cls2_loss_te.mean(-1).sum() + cls3_loss_te.mean(-1).sum() + cls4_loss_te.mean(-1).sum())/160\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        reg_loss_tr = mse_criterion(preds_train[:, :6].squeeze(), y_train[:, :6].squeeze())\n",
    "        cls1_loss_tr = ce_criterion(preds_train[:, 6:10].squeeze(), y_train[:, 6].squeeze().long())\n",
    "        cls2_loss_tr = ce_criterion(preds_train[:, 10:14].squeeze(), y_train[:, 7].squeeze().long())\n",
    "        cls3_loss_tr = ce_criterion(preds_train[:, 14:18].squeeze(), y_train[:, 8].squeeze().long())\n",
    "        cls4_loss_tr = ce_criterion(preds_train[:, 18:22].squeeze(), y_train[:, 9].squeeze().long())\n",
    "        \n",
    "        l_train =  (reg_loss_tr.mean(-1).sum() + cls1_loss_tr.mean(-1).sum() + cls2_loss_tr.mean(-1).sum() + cls3_loss_tr.mean(-1).sum() + cls4_loss_tr.mean(-1).sum())/160\n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:47.543 | INFO     | __main__:<module>:9 - Test 0.0844 +- 0.0053\n",
      "2022-04-25 18:54:47.544 | INFO     | __main__:<module>:10 - Train 0.0779 +- 0.0052\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.2027\n",
      "MSE for Dimension 2: 0.2800\n",
      "MSE for Dimension 3: 0.2775\n",
      "MSE for Dimension 4: 0.2494\n",
      "MSE for Dimension 5: 0.2589\n",
      "MSE for Dimension 6: 0.1913\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.26      0.35      6826\n",
      "         1.0       0.24      0.14      0.18      2121\n",
      "         2.0       0.07      0.02      0.03      1717\n",
      "         3.0       0.04      0.66      0.08       408\n",
      "\n",
      "    accuracy                           0.21     11072\n",
      "   macro avg       0.23      0.27      0.16     11072\n",
      "weighted avg       0.41      0.21      0.26     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.00      0.00      8096\n",
      "         1.0       0.06      0.02      0.03       469\n",
      "         2.0       0.07      1.00      0.14       790\n",
      "         3.0       0.68      0.02      0.04      1717\n",
      "\n",
      "    accuracy                           0.08     11072\n",
      "   macro avg       0.45      0.26      0.05     11072\n",
      "weighted avg       0.84      0.08      0.02     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.01      2716\n",
      "         1.0       0.46      0.27      0.34      4925\n",
      "         2.0       0.03      0.01      0.01      1293\n",
      "         3.0       0.21      0.79      0.34      2138\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.30      0.27      0.17     11072\n",
      "weighted avg       0.37      0.27      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.08      0.13      4859\n",
      "         1.0       0.11      0.20      0.14      1442\n",
      "         2.0       0.05      0.23      0.08       561\n",
      "         3.0       0.35      0.41      0.38      4210\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.22      0.23      0.18     11072\n",
      "weighted avg       0.32      0.23      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

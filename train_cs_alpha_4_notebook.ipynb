{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:10:37.760 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=1.2, kappa=1500)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:00:28.250 | INFO     | __main__:<module>:9 - Test 0.6112 +- 0.1774\n",
      "2022-04-28 14:00:28.251 | INFO     | __main__:<module>:10 - Train 0.1415 +- 0.0152\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 2.5398\n",
      "MSE for Dimension 2: 2.7373\n",
      "MSE for Dimension 3: 1.0603\n",
      "MSE for Dimension 4: 1.8169\n",
      "MSE for Dimension 5: 2.6640\n",
      "MSE for Dimension 6: 3.7036\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.23      0.34      6826\n",
      "         1.0       0.17      0.30      0.21      2121\n",
      "         2.0       0.18      0.24      0.21      1717\n",
      "         3.0       0.01      0.09      0.03       408\n",
      "\n",
      "    accuracy                           0.24     11072\n",
      "   macro avg       0.26      0.22      0.20     11072\n",
      "weighted avg       0.47      0.24      0.28     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.35      0.47      8096\n",
      "         1.0       0.02      0.06      0.03       469\n",
      "         2.0       0.10      0.15      0.12       790\n",
      "         3.0       0.11      0.28      0.16      1717\n",
      "\n",
      "    accuracy                           0.32     11072\n",
      "   macro avg       0.23      0.21      0.19     11072\n",
      "weighted avg       0.54      0.32      0.38     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.12      0.04      0.06      2716\n",
      "         1.0       0.34      0.34      0.34      4925\n",
      "         2.0       0.06      0.03      0.04      1293\n",
      "         3.0       0.17      0.35      0.23      2138\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.17      0.19      0.17     11072\n",
      "weighted avg       0.22      0.23      0.21     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.21      0.29      4859\n",
      "         1.0       0.12      0.25      0.16      1442\n",
      "         2.0       0.01      0.01      0.01       561\n",
      "         3.0       0.41      0.54      0.47      4210\n",
      "\n",
      "    accuracy                           0.33     11072\n",
      "   macro avg       0.25      0.25      0.23     11072\n",
      "weighted avg       0.38      0.33      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

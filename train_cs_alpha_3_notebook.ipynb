{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:09:20.596 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=1.2, kappa=500)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:56:37.875 | INFO     | __main__:<module>:9 - Test 0.4097 +- 0.0605\n",
      "2022-04-28 13:56:37.876 | INFO     | __main__:<module>:10 - Train 0.1737 +- 0.0171\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 1.5109\n",
      "MSE for Dimension 2: 1.3061\n",
      "MSE for Dimension 3: 0.8641\n",
      "MSE for Dimension 4: 1.1307\n",
      "MSE for Dimension 5: 1.2936\n",
      "MSE for Dimension 6: 2.4056\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.30      0.40      6826\n",
      "         1.0       0.24      0.23      0.24      2121\n",
      "         2.0       0.26      0.29      0.27      1717\n",
      "         3.0       0.01      0.12      0.02       408\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.28      0.23      0.23     11072\n",
      "weighted avg       0.46      0.27      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.68      0.73      8096\n",
      "         1.0       0.01      0.01      0.01       469\n",
      "         2.0       0.15      0.08      0.10       790\n",
      "         3.0       0.12      0.18      0.14      1717\n",
      "\n",
      "    accuracy                           0.53     11072\n",
      "   macro avg       0.27      0.24      0.25     11072\n",
      "weighted avg       0.61      0.53      0.56     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.15      0.08      0.10      2716\n",
      "         1.0       0.47      0.55      0.51      4925\n",
      "         2.0       0.13      0.17      0.14      1293\n",
      "         3.0       0.20      0.20      0.20      2138\n",
      "\n",
      "    accuracy                           0.32     11072\n",
      "   macro avg       0.24      0.25      0.24     11072\n",
      "weighted avg       0.30      0.32      0.31     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.15      0.25      4859\n",
      "         1.0       0.14      0.25      0.17      1442\n",
      "         2.0       0.00      0.01      0.00       561\n",
      "         3.0       0.39      0.60      0.47      4210\n",
      "\n",
      "    accuracy                           0.33     11072\n",
      "   macro avg       0.31      0.25      0.23     11072\n",
      "weighted avg       0.47      0.33      0.31     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

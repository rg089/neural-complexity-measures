{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:12:49.746 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=0.9, kappa=500)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:02:43.992 | INFO     | __main__:<module>:9 - Test 0.5864 +- 0.1206\n",
      "2022-04-28 14:02:43.993 | INFO     | __main__:<module>:10 - Train 0.1876 +- 0.0163\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 1.2167\n",
      "MSE for Dimension 2: 2.1316\n",
      "MSE for Dimension 3: 2.4325\n",
      "MSE for Dimension 4: 1.7605\n",
      "MSE for Dimension 5: 3.0457\n",
      "MSE for Dimension 6: 2.0809\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.29      0.41      6826\n",
      "         1.0       0.22      0.56      0.31      2121\n",
      "         2.0       0.14      0.10      0.11      1717\n",
      "         3.0       0.11      0.43      0.17       408\n",
      "\n",
      "    accuracy                           0.32     11072\n",
      "   macro avg       0.29      0.34      0.25     11072\n",
      "weighted avg       0.49      0.32      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.20      0.30      8096\n",
      "         1.0       0.03      0.25      0.05       469\n",
      "         2.0       0.02      0.01      0.02       790\n",
      "         3.0       0.10      0.24      0.15      1717\n",
      "\n",
      "    accuracy                           0.19     11072\n",
      "   macro avg       0.20      0.18      0.13     11072\n",
      "weighted avg       0.48      0.19      0.25     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.17      0.23      2716\n",
      "         1.0       0.46      0.46      0.46      4925\n",
      "         2.0       0.09      0.17      0.12      1293\n",
      "         3.0       0.31      0.35      0.33      2138\n",
      "\n",
      "    accuracy                           0.33     11072\n",
      "   macro avg       0.30      0.29      0.28     11072\n",
      "weighted avg       0.36      0.33      0.34     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.19      0.26      4859\n",
      "         1.0       0.12      0.37      0.18      1442\n",
      "         2.0       0.02      0.04      0.02       561\n",
      "         3.0       0.30      0.21      0.25      4210\n",
      "\n",
      "    accuracy                           0.21     11072\n",
      "   macro avg       0.21      0.20      0.17     11072\n",
      "weighted avg       0.31      0.21      0.23     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

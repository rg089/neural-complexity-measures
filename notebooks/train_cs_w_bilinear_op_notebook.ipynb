{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_w_bilinear_op\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.projector = fc_stack(3, bilinear_output_dim, dim, dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output) + self.projector(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())*5.0/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 08:46:30.675 | INFO     | __main__:<module>:19 - Populate time: 0.04209376685321331\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:46:03.120 | INFO     | __main__:<module>:4 - Epoch 0\n",
      "2022-04-25 18:46:03.120 | INFO     | __main__:<module>:5 - Bank size: 14880\n",
      "2022-04-25 18:46:03.252 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-25 18:46:03.253 | INFO     | __main__:train:49 - mae 5.11e-01 loss 1.93e-01 R 0.330 gap -0.006546972319483757 preds -0.027155369520187378\n",
      "2022-04-25 18:46:03.330 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-25 18:46:03.331 | INFO     | __main__:train:49 - mae 5.90e-01 loss 2.46e-01 R 0.091 gap -0.004185333847999573 preds -0.04200897365808487\n",
      "2022-04-25 18:46:03.398 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-25 18:46:03.399 | INFO     | __main__:train:49 - mae 5.51e-01 loss 2.18e-01 R 0.145 gap 0.0009324792772531509 preds -0.014336398802697659\n",
      "2022-04-25 18:46:03.468 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-25 18:46:03.469 | INFO     | __main__:train:49 - mae 5.96e-01 loss 2.45e-01 R 0.029 gap 0.012885267846286297 preds -0.021728243678808212\n",
      "2022-04-25 18:46:03.534 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-25 18:46:03.535 | INFO     | __main__:train:49 - mae 5.88e-01 loss 2.46e-01 R 0.010 gap 0.016330037266016006 preds -0.019997818395495415\n",
      "2022-04-25 18:46:03.597 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-25 18:46:03.598 | INFO     | __main__:train:49 - mae 5.75e-01 loss 2.39e-01 R 0.062 gap 0.020957479253411293 preds -0.011363725177943707\n",
      "2022-04-25 18:46:03.659 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-25 18:46:03.659 | INFO     | __main__:train:49 - mae 5.74e-01 loss 2.40e-01 R 0.044 gap 0.019055286422371864 preds -0.006276439409703016\n",
      "2022-04-25 18:46:03.720 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-25 18:46:03.721 | INFO     | __main__:train:49 - mae 5.63e-01 loss 2.32e-01 R 0.025 gap 0.020903123542666435 preds -0.010051131248474121\n",
      "2022-04-25 18:46:03.784 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-25 18:46:03.785 | INFO     | __main__:train:49 - mae 5.57e-01 loss 2.26e-01 R -0.003 gap 0.01788203977048397 preds -0.01507425494492054\n",
      "2022-04-25 18:46:04.072 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-25 18:46:04.072 | INFO     | __main__:train:49 - mae 5.50e-01 loss 2.21e-01 R 0.009 gap 0.020825928077101707 preds -0.01099025085568428\n",
      "2022-04-25 18:46:04.136 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-25 18:46:04.137 | INFO     | __main__:train:49 - mae 5.40e-01 loss 2.15e-01 R 0.029 gap 0.019430609419941902 preds -0.011181415989995003\n",
      "2022-04-25 18:46:04.200 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-25 18:46:04.201 | INFO     | __main__:train:49 - mae 5.35e-01 loss 2.10e-01 R 0.022 gap 0.019430913031101227 preds -0.015862220898270607\n",
      "2022-04-25 18:46:04.264 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-25 18:46:04.265 | INFO     | __main__:train:49 - mae 5.29e-01 loss 2.04e-01 R 0.020 gap 0.021726273000240326 preds -0.012630214914679527\n",
      "2022-04-25 18:46:04.325 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-25 18:46:04.326 | INFO     | __main__:train:49 - mae 5.22e-01 loss 1.99e-01 R 0.012 gap 0.022746291011571884 preds -0.013697822578251362\n",
      "2022-04-25 18:46:04.389 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-25 18:46:04.390 | INFO     | __main__:train:49 - mae 5.18e-01 loss 1.97e-01 R -0.004 gap 0.022237880155444145 preds -0.012149875983595848\n",
      "2022-04-25 18:46:04.451 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-25 18:46:04.452 | INFO     | __main__:train:49 - mae 5.14e-01 loss 1.94e-01 R 0.006 gap 0.02272442728281021 preds -0.013476644642651081\n",
      "2022-04-25 18:46:04.512 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-25 18:46:04.513 | INFO     | __main__:train:49 - mae 5.03e-01 loss 1.88e-01 R 0.003 gap 0.02248752862215042 preds -0.009005004540085793\n",
      "2022-04-25 18:46:04.572 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-25 18:46:04.573 | INFO     | __main__:train:49 - mae 4.96e-01 loss 1.85e-01 R 0.005 gap 0.022417927160859108 preds -0.007820391096174717\n",
      "2022-04-25 18:46:04.635 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-25 18:46:04.636 | INFO     | __main__:train:49 - mae 4.95e-01 loss 1.85e-01 R 0.002 gap 0.023372754454612732 preds -0.006072927266359329\n",
      "2022-04-25 18:46:04.925 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-25 18:46:04.926 | INFO     | __main__:train:49 - mae 4.94e-01 loss 1.84e-01 R -0.005 gap 0.02335342764854431 preds -0.0012804928701370955\n",
      "2022-04-25 18:46:04.985 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-25 18:46:04.986 | INFO     | __main__:train:49 - mae 4.90e-01 loss 1.82e-01 R 0.013 gap 0.0245254747569561 preds 0.0008987092296592891\n",
      "2022-04-25 18:46:05.045 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-25 18:46:05.046 | INFO     | __main__:train:49 - mae 4.90e-01 loss 1.82e-01 R 0.008 gap 0.02391302026808262 preds -0.0039057256653904915\n",
      "2022-04-25 18:46:05.103 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-25 18:46:05.104 | INFO     | __main__:train:49 - mae 4.85e-01 loss 1.78e-01 R 0.020 gap 0.024192361161112785 preds -0.0028154244646430016\n",
      "2022-04-25 18:46:05.164 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-25 18:46:05.164 | INFO     | __main__:train:49 - mae 4.78e-01 loss 1.74e-01 R 0.019 gap 0.023447751998901367 preds -0.00026856735348701477\n",
      "2022-04-25 18:46:05.223 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-25 18:46:05.223 | INFO     | __main__:train:49 - mae 4.71e-01 loss 1.70e-01 R 0.022 gap 0.02378670498728752 preds 8.438855729764327e-05\n",
      "2022-04-25 18:46:05.280 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-25 18:46:05.281 | INFO     | __main__:train:49 - mae 4.67e-01 loss 1.68e-01 R 0.018 gap 0.023640768602490425 preds 0.002337711863219738\n",
      "2022-04-25 18:46:05.341 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-25 18:46:05.342 | INFO     | __main__:train:49 - mae 4.64e-01 loss 1.66e-01 R 0.013 gap 0.024098487570881844 preds 0.004529865458607674\n",
      "2022-04-25 18:46:05.405 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-25 18:46:05.406 | INFO     | __main__:train:49 - mae 4.59e-01 loss 1.63e-01 R 0.006 gap 0.02319319359958172 preds 0.0066358596086502075\n",
      "2022-04-25 18:46:05.465 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-25 18:46:05.466 | INFO     | __main__:train:49 - mae 4.56e-01 loss 1.61e-01 R 0.006 gap 0.023742351680994034 preds 0.005429270677268505\n",
      "2022-04-25 18:46:05.740 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-25 18:46:05.741 | INFO     | __main__:train:49 - mae 4.53e-01 loss 1.59e-01 R 0.011 gap 0.02339117042720318 preds 0.004706201143562794\n",
      "2022-04-25 18:46:05.801 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-25 18:46:05.802 | INFO     | __main__:train:49 - mae 4.46e-01 loss 1.55e-01 R 0.014 gap 0.023432057350873947 preds 0.00529055017977953\n",
      "2022-04-25 18:46:05.861 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-25 18:46:05.862 | INFO     | __main__:train:49 - mae 4.42e-01 loss 1.53e-01 R 0.025 gap 0.02342154085636139 preds 0.005791566334664822\n",
      "2022-04-25 18:46:05.923 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-25 18:46:05.924 | INFO     | __main__:train:49 - mae 4.40e-01 loss 1.51e-01 R 0.022 gap 0.02256717160344124 preds 0.005736146587878466\n",
      "2022-04-25 18:46:05.988 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-25 18:46:05.989 | INFO     | __main__:train:49 - mae 4.39e-01 loss 1.51e-01 R 0.030 gap 0.02175883948802948 preds 0.006337704136967659\n",
      "2022-04-25 18:46:06.052 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-25 18:46:06.052 | INFO     | __main__:train:49 - mae 4.37e-01 loss 1.50e-01 R 0.029 gap 0.021703070029616356 preds 0.007544808555394411\n",
      "2022-04-25 18:46:06.113 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-25 18:46:06.114 | INFO     | __main__:train:49 - mae 4.36e-01 loss 1.49e-01 R 0.032 gap 0.020897088572382927 preds 0.009254815056920052\n",
      "2022-04-25 18:46:06.172 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-25 18:46:06.173 | INFO     | __main__:train:49 - mae 4.33e-01 loss 1.47e-01 R 0.035 gap 0.01998855546116829 preds 0.01010412909090519\n",
      "2022-04-25 18:46:06.233 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-25 18:46:06.233 | INFO     | __main__:train:49 - mae 4.30e-01 loss 1.46e-01 R 0.037 gap 0.019664570689201355 preds 0.011096124537289143\n",
      "2022-04-25 18:46:06.297 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-25 18:46:06.298 | INFO     | __main__:train:49 - mae 4.25e-01 loss 1.43e-01 R 0.039 gap 0.02062087319791317 preds 0.012546878308057785\n",
      "2022-04-25 18:46:06.588 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-25 18:46:06.588 | INFO     | __main__:train:49 - mae 4.21e-01 loss 1.41e-01 R 0.040 gap 0.020335575565695763 preds 0.012290021404623985\n",
      "2022-04-25 18:46:06.650 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-25 18:46:06.651 | INFO     | __main__:train:49 - mae 4.18e-01 loss 1.40e-01 R 0.042 gap 0.02051592245697975 preds 0.010952058248221874\n",
      "2022-04-25 18:46:06.712 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-25 18:46:06.713 | INFO     | __main__:train:49 - mae 4.15e-01 loss 1.38e-01 R 0.042 gap 0.020362934097647667 preds 0.01080743595957756\n",
      "2022-04-25 18:46:06.773 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-25 18:46:06.774 | INFO     | __main__:train:49 - mae 4.11e-01 loss 1.36e-01 R 0.043 gap 0.020381001755595207 preds 0.010594530031085014\n",
      "2022-04-25 18:46:06.837 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-25 18:46:06.838 | INFO     | __main__:train:49 - mae 4.07e-01 loss 1.33e-01 R 0.043 gap 0.02072213962674141 preds 0.010194221511483192\n",
      "2022-04-25 18:46:06.899 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-25 18:46:06.899 | INFO     | __main__:train:49 - mae 4.04e-01 loss 1.32e-01 R 0.041 gap 0.02060067094862461 preds 0.010048993863165379\n",
      "2022-04-25 18:46:06.962 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-25 18:46:06.963 | INFO     | __main__:train:49 - mae 4.02e-01 loss 1.30e-01 R 0.044 gap 0.02107788622379303 preds 0.009699052199721336\n",
      "2022-04-25 18:46:07.026 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-25 18:46:07.027 | INFO     | __main__:train:49 - mae 3.97e-01 loss 1.28e-01 R 0.043 gap 0.02114422805607319 preds 0.009844240732491016\n",
      "2022-04-25 18:46:07.088 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-25 18:46:07.089 | INFO     | __main__:train:49 - mae 3.95e-01 loss 1.27e-01 R 0.042 gap 0.02106129191815853 preds 0.009630086831748486\n",
      "2022-04-25 18:46:07.152 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-25 18:46:07.153 | INFO     | __main__:train:49 - mae 3.92e-01 loss 1.25e-01 R 0.044 gap 0.02172810770571232 preds 0.009847413748502731\n",
      "2022-04-25 18:46:07.439 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-25 18:46:07.439 | INFO     | __main__:train:49 - mae 3.89e-01 loss 1.23e-01 R 0.047 gap 0.020664632320404053 preds 0.01004046481102705\n",
      "2022-04-25 18:46:07.502 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-25 18:46:07.502 | INFO     | __main__:train:49 - mae 3.86e-01 loss 1.22e-01 R 0.046 gap 0.020714623853564262 preds 0.010345188900828362\n",
      "2022-04-25 18:46:07.563 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-25 18:46:07.564 | INFO     | __main__:train:49 - mae 3.83e-01 loss 1.20e-01 R 0.046 gap 0.020645877346396446 preds 0.01060873456299305\n",
      "2022-04-25 18:46:07.624 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-25 18:46:07.625 | INFO     | __main__:train:49 - mae 3.81e-01 loss 1.19e-01 R 0.049 gap 0.020697420462965965 preds 0.011448048986494541\n",
      "2022-04-25 18:46:07.684 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-25 18:46:07.684 | INFO     | __main__:train:49 - mae 3.78e-01 loss 1.17e-01 R 0.050 gap 0.020684652030467987 preds 0.012213238514959812\n",
      "2022-04-25 18:46:07.745 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-25 18:46:07.746 | INFO     | __main__:train:49 - mae 3.76e-01 loss 1.16e-01 R 0.048 gap 0.02022785320878029 preds 0.01222827099263668\n",
      "2022-04-25 18:46:07.807 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-25 18:46:07.808 | INFO     | __main__:train:49 - mae 3.74e-01 loss 1.15e-01 R 0.048 gap 0.020052244886755943 preds 0.012400004081428051\n",
      "2022-04-25 18:46:07.868 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-25 18:46:07.869 | INFO     | __main__:train:49 - mae 3.72e-01 loss 1.14e-01 R 0.048 gap 0.019977571442723274 preds 0.013088865205645561\n",
      "2022-04-25 18:46:07.932 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-25 18:46:07.933 | INFO     | __main__:train:49 - mae 3.69e-01 loss 1.13e-01 R 0.047 gap 0.019812243059277534 preds 0.012766174040734768\n",
      "2022-04-25 18:46:08.006 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-25 18:46:08.007 | INFO     | __main__:train:49 - mae 3.67e-01 loss 1.12e-01 R 0.050 gap 0.0201601330190897 preds 0.012878523208200932\n",
      "2022-04-25 18:46:08.288 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-25 18:46:08.289 | INFO     | __main__:train:49 - mae 3.64e-01 loss 1.10e-01 R 0.049 gap 0.02043718472123146 preds 0.012788618914783001\n",
      "2022-04-25 18:46:08.352 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-25 18:46:08.353 | INFO     | __main__:train:49 - mae 3.62e-01 loss 1.09e-01 R 0.055 gap 0.020442405715584755 preds 0.012916439212858677\n",
      "2022-04-25 18:46:08.414 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-25 18:46:08.414 | INFO     | __main__:train:49 - mae 3.59e-01 loss 1.08e-01 R 0.056 gap 0.02064584568142891 preds 0.012840958312153816\n",
      "2022-04-25 18:46:08.468 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-25 18:46:08.469 | INFO     | __main__:train:49 - mae 3.56e-01 loss 1.07e-01 R 0.061 gap 0.02109553851187229 preds 0.013158312067389488\n",
      "2022-04-25 18:46:08.533 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-25 18:46:08.534 | INFO     | __main__:train:49 - mae 3.54e-01 loss 1.06e-01 R 0.060 gap 0.021064385771751404 preds 0.013570377603173256\n",
      "2022-04-25 18:46:08.592 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-25 18:46:08.593 | INFO     | __main__:train:49 - mae 3.52e-01 loss 1.05e-01 R 0.059 gap 0.021137263625860214 preds 0.013631387613713741\n",
      "2022-04-25 18:46:08.650 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-25 18:46:08.651 | INFO     | __main__:train:49 - mae 3.49e-01 loss 1.04e-01 R 0.060 gap 0.02112521417438984 preds 0.013987302780151367\n",
      "2022-04-25 18:46:08.711 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-25 18:46:08.712 | INFO     | __main__:train:49 - mae 3.47e-01 loss 1.03e-01 R 0.058 gap 0.02172817476093769 preds 0.01372525840997696\n",
      "2022-04-25 18:46:08.771 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-25 18:46:08.772 | INFO     | __main__:train:49 - mae 3.45e-01 loss 1.01e-01 R 0.060 gap 0.02181960828602314 preds 0.014501411467790604\n",
      "2022-04-25 18:46:08.832 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-25 18:46:08.832 | INFO     | __main__:train:49 - mae 3.43e-01 loss 1.00e-01 R 0.060 gap 0.021377453580498695 preds 0.015085569582879543\n",
      "2022-04-25 18:46:09.108 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-25 18:46:09.109 | INFO     | __main__:train:49 - mae 3.40e-01 loss 9.93e-02 R 0.063 gap 0.021369507536292076 preds 0.01545215304940939\n",
      "2022-04-25 18:46:09.173 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-25 18:46:09.174 | INFO     | __main__:train:49 - mae 3.38e-01 loss 9.83e-02 R 0.063 gap 0.021338216960430145 preds 0.015461070463061333\n",
      "2022-04-25 18:46:09.236 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-25 18:46:09.237 | INFO     | __main__:train:49 - mae 3.36e-01 loss 9.75e-02 R 0.063 gap 0.02106383815407753 preds 0.01547740027308464\n",
      "2022-04-25 18:46:09.298 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-25 18:46:09.298 | INFO     | __main__:train:49 - mae 3.34e-01 loss 9.68e-02 R 0.062 gap 0.020935462787747383 preds 0.015362230129539967\n",
      "2022-04-25 18:46:09.359 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-25 18:46:09.360 | INFO     | __main__:train:49 - mae 3.33e-01 loss 9.62e-02 R 0.062 gap 0.020908966660499573 preds 0.015261477790772915\n",
      "2022-04-25 18:46:09.423 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-25 18:46:09.424 | INFO     | __main__:train:49 - mae 3.31e-01 loss 9.54e-02 R 0.063 gap 0.020984264090657234 preds 0.014992599375545979\n",
      "2022-04-25 18:46:09.485 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-25 18:46:09.486 | INFO     | __main__:train:49 - mae 3.29e-01 loss 9.45e-02 R 0.063 gap 0.02084917388856411 preds 0.014815178699791431\n",
      "2022-04-25 18:46:09.546 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-25 18:46:09.547 | INFO     | __main__:train:49 - mae 3.27e-01 loss 9.36e-02 R 0.064 gap 0.020541496574878693 preds 0.014713633805513382\n",
      "2022-04-25 18:46:09.608 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-25 18:46:09.609 | INFO     | __main__:train:49 - mae 3.26e-01 loss 9.28e-02 R 0.065 gap 0.02030080184340477 preds 0.014677333645522594\n",
      "2022-04-25 18:46:09.676 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-25 18:46:09.677 | INFO     | __main__:train:49 - mae 3.24e-01 loss 9.20e-02 R 0.068 gap 0.020653720945119858 preds 0.015127391554415226\n",
      "2022-04-25 18:46:09.959 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-25 18:46:09.959 | INFO     | __main__:train:49 - mae 3.23e-01 loss 9.13e-02 R 0.069 gap 0.02074701525270939 preds 0.014964893460273743\n",
      "2022-04-25 18:46:10.018 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-25 18:46:10.018 | INFO     | __main__:train:49 - mae 3.22e-01 loss 9.08e-02 R 0.068 gap 0.020766718313097954 preds 0.014602062292397022\n",
      "2022-04-25 18:46:10.075 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-25 18:46:10.076 | INFO     | __main__:train:49 - mae 3.20e-01 loss 8.99e-02 R 0.067 gap 0.020932523533701897 preds 0.01453801803290844\n",
      "2022-04-25 18:46:10.132 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-25 18:46:10.132 | INFO     | __main__:train:49 - mae 3.18e-01 loss 8.91e-02 R 0.067 gap 0.020897917449474335 preds 0.01473234687000513\n",
      "2022-04-25 18:46:10.191 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-25 18:46:10.192 | INFO     | __main__:train:49 - mae 3.17e-01 loss 8.85e-02 R 0.064 gap 0.02043875865638256 preds 0.014396938495337963\n",
      "2022-04-25 18:46:10.252 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-25 18:46:10.253 | INFO     | __main__:train:49 - mae 3.16e-01 loss 8.80e-02 R 0.066 gap 0.02001195214688778 preds 0.014373443089425564\n",
      "2022-04-25 18:46:10.313 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-25 18:46:10.314 | INFO     | __main__:train:49 - mae 3.14e-01 loss 8.73e-02 R 0.065 gap 0.019806288182735443 preds 0.013891107402741909\n",
      "2022-04-25 18:46:10.377 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-25 18:46:10.377 | INFO     | __main__:train:49 - mae 3.12e-01 loss 8.65e-02 R 0.068 gap 0.01974695920944214 preds 0.014062374830245972\n",
      "2022-04-25 18:46:10.440 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-25 18:46:10.441 | INFO     | __main__:train:49 - mae 3.11e-01 loss 8.58e-02 R 0.070 gap 0.019951369613409042 preds 0.013840227387845516\n",
      "2022-04-25 18:46:10.499 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-25 18:46:10.499 | INFO     | __main__:train:49 - mae 3.09e-01 loss 8.51e-02 R 0.071 gap 0.020257923752069473 preds 0.01348432432860136\n",
      "2022-04-25 18:46:10.790 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-25 18:46:10.791 | INFO     | __main__:train:49 - mae 3.07e-01 loss 8.44e-02 R 0.070 gap 0.02001023478806019 preds 0.013043416664004326\n",
      "2022-04-25 18:46:10.850 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-25 18:46:10.851 | INFO     | __main__:train:49 - mae 3.06e-01 loss 8.39e-02 R 0.071 gap 0.020075134932994843 preds 0.012763631530106068\n",
      "2022-04-25 18:46:10.908 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-25 18:46:10.909 | INFO     | __main__:train:49 - mae 3.04e-01 loss 8.31e-02 R 0.072 gap 0.01968812383711338 preds 0.012761272490024567\n",
      "2022-04-25 18:46:10.967 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-25 18:46:10.968 | INFO     | __main__:train:49 - mae 3.03e-01 loss 8.26e-02 R 0.070 gap 0.019711220636963844 preds 0.013116979040205479\n",
      "2022-04-25 18:46:31.460 | INFO     | __main__:test:55 - Test epoch 0\n",
      "2022-04-25 18:46:31.461 | INFO     | __main__:test:57 - mae 1.67e-01 loss 2.48e-02 R 0.007 l_test 1.02e+00 l_train 1.00e+00 \n",
      "2022-04-25 18:46:31.462 | INFO     | __main__:<module>:4 - Epoch 1\n",
      "2022-04-25 18:46:31.463 | INFO     | __main__:<module>:5 - Bank size: 16320\n",
      "2022-04-25 18:46:31.555 | INFO     | __main__:train:47 - Train Step 94\n",
      "2022-04-25 18:46:31.556 | INFO     | __main__:train:49 - mae 3.02e-01 loss 8.20e-02 R 0.068 gap 0.01945924386382103 preds 0.013343680649995804\n",
      "2022-04-25 18:46:31.636 | INFO     | __main__:train:47 - Train Step 95\n",
      "2022-04-25 18:46:31.637 | INFO     | __main__:train:49 - mae 3.00e-01 loss 8.14e-02 R 0.067 gap 0.01930660754442215 preds 0.013832107186317444\n",
      "2022-04-25 18:46:31.703 | INFO     | __main__:train:47 - Train Step 96\n",
      "2022-04-25 18:46:31.704 | INFO     | __main__:train:49 - mae 2.99e-01 loss 8.08e-02 R 0.066 gap 0.01918913424015045 preds 0.014147733338177204\n",
      "2022-04-25 18:46:31.777 | INFO     | __main__:train:47 - Train Step 97\n",
      "2022-04-25 18:46:31.778 | INFO     | __main__:train:49 - mae 2.98e-01 loss 8.02e-02 R 0.068 gap 0.019161291420459747 preds 0.014561149291694164\n",
      "2022-04-25 18:46:31.845 | INFO     | __main__:train:47 - Train Step 98\n",
      "2022-04-25 18:46:31.846 | INFO     | __main__:train:49 - mae 2.96e-01 loss 7.99e-02 R 0.068 gap 0.01927216723561287 preds 0.015165391378104687\n",
      "2022-04-25 18:46:31.903 | INFO     | __main__:train:47 - Train Step 99\n",
      "2022-04-25 18:46:31.904 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.94e-02 R 0.067 gap 0.019347265362739563 preds 0.015505064278841019\n",
      "2022-04-25 18:46:32.164 | INFO     | __main__:train:47 - Train Step 100\n",
      "2022-04-25 18:46:32.165 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.93e-02 R 0.064 gap 0.018952418118715286 preds 0.015406173653900623\n",
      "2022-04-25 18:46:32.224 | INFO     | __main__:train:47 - Train Step 101\n",
      "2022-04-25 18:46:32.225 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.93e-02 R 0.062 gap 0.018557878211140633 preds 0.01542012207210064\n",
      "2022-04-25 18:46:32.285 | INFO     | __main__:train:47 - Train Step 102\n",
      "2022-04-25 18:46:32.285 | INFO     | __main__:train:49 - mae 2.94e-01 loss 7.90e-02 R 0.062 gap 0.01856388710439205 preds 0.015579369850456715\n",
      "2022-04-25 18:46:32.343 | INFO     | __main__:train:47 - Train Step 103\n",
      "2022-04-25 18:46:32.343 | INFO     | __main__:train:49 - mae 2.93e-01 loss 7.85e-02 R 0.061 gap 0.018374798819422722 preds 0.015807798132300377\n",
      "2022-04-25 18:46:32.405 | INFO     | __main__:train:47 - Train Step 104\n",
      "2022-04-25 18:46:32.406 | INFO     | __main__:train:49 - mae 2.92e-01 loss 7.80e-02 R 0.062 gap 0.018042175099253654 preds 0.01537636574357748\n",
      "2022-04-25 18:46:32.469 | INFO     | __main__:train:47 - Train Step 105\n",
      "2022-04-25 18:46:32.470 | INFO     | __main__:train:49 - mae 2.91e-01 loss 7.74e-02 R 0.063 gap 0.0179001335054636 preds 0.015088628977537155\n",
      "2022-04-25 18:46:32.534 | INFO     | __main__:train:47 - Train Step 106\n",
      "2022-04-25 18:46:32.535 | INFO     | __main__:train:49 - mae 2.89e-01 loss 7.69e-02 R 0.066 gap 0.017435675486922264 preds 0.014842847362160683\n",
      "2022-04-25 18:46:32.597 | INFO     | __main__:train:47 - Train Step 107\n",
      "2022-04-25 18:46:32.598 | INFO     | __main__:train:49 - mae 2.89e-01 loss 7.66e-02 R 0.064 gap 0.01731480471789837 preds 0.014796802774071693\n",
      "2022-04-25 18:46:32.661 | INFO     | __main__:train:47 - Train Step 108\n",
      "2022-04-25 18:46:32.662 | INFO     | __main__:train:49 - mae 2.88e-01 loss 7.61e-02 R 0.064 gap 0.017461324110627174 preds 0.01467945147305727\n",
      "2022-04-25 18:46:32.726 | INFO     | __main__:train:47 - Train Step 109\n",
      "2022-04-25 18:46:32.727 | INFO     | __main__:train:49 - mae 2.87e-01 loss 7.58e-02 R 0.064 gap 0.017456717789173126 preds 0.014737553894519806\n",
      "2022-04-25 18:46:33.022 | INFO     | __main__:train:47 - Train Step 110\n",
      "2022-04-25 18:46:33.023 | INFO     | __main__:train:49 - mae 2.86e-01 loss 7.54e-02 R 0.064 gap 0.017375413328409195 preds 0.01478674914687872\n",
      "2022-04-25 18:46:33.082 | INFO     | __main__:train:47 - Train Step 111\n",
      "2022-04-25 18:46:33.083 | INFO     | __main__:train:49 - mae 2.85e-01 loss 7.49e-02 R 0.063 gap 0.017244970425963402 preds 0.014456511475145817\n",
      "2022-04-25 18:46:33.146 | INFO     | __main__:train:47 - Train Step 112\n",
      "2022-04-25 18:46:33.147 | INFO     | __main__:train:49 - mae 2.84e-01 loss 7.45e-02 R 0.063 gap 0.01724710874259472 preds 0.014274945482611656\n",
      "2022-04-25 18:46:33.209 | INFO     | __main__:train:47 - Train Step 113\n",
      "2022-04-25 18:46:33.210 | INFO     | __main__:train:49 - mae 2.84e-01 loss 7.41e-02 R 0.063 gap 0.01685686968266964 preds 0.014090200886130333\n",
      "2022-04-25 18:46:33.275 | INFO     | __main__:train:47 - Train Step 114\n",
      "2022-04-25 18:46:33.276 | INFO     | __main__:train:49 - mae 2.83e-01 loss 7.36e-02 R 0.064 gap 0.016650276258587837 preds 0.01387648843228817\n",
      "2022-04-25 18:46:33.338 | INFO     | __main__:train:47 - Train Step 115\n",
      "2022-04-25 18:46:33.339 | INFO     | __main__:train:49 - mae 2.81e-01 loss 7.32e-02 R 0.063 gap 0.016467951238155365 preds 0.013785761781036854\n",
      "2022-04-25 18:46:33.400 | INFO     | __main__:train:47 - Train Step 116\n",
      "2022-04-25 18:46:33.401 | INFO     | __main__:train:49 - mae 2.81e-01 loss 7.29e-02 R 0.062 gap 0.016538502648472786 preds 0.014155583456158638\n",
      "2022-04-25 18:46:33.460 | INFO     | __main__:train:47 - Train Step 117\n",
      "2022-04-25 18:46:33.461 | INFO     | __main__:train:49 - mae 2.80e-01 loss 7.24e-02 R 0.062 gap 0.016464760527014732 preds 0.014065781608223915\n",
      "2022-04-25 18:46:33.522 | INFO     | __main__:train:47 - Train Step 118\n",
      "2022-04-25 18:46:33.523 | INFO     | __main__:train:49 - mae 2.78e-01 loss 7.20e-02 R 0.061 gap 0.016499945893883705 preds 0.013986397534608841\n",
      "2022-04-25 18:46:33.585 | INFO     | __main__:train:47 - Train Step 119\n",
      "2022-04-25 18:46:33.586 | INFO     | __main__:train:49 - mae 2.77e-01 loss 7.16e-02 R 0.059 gap 0.016516556963324547 preds 0.013928821310400963\n",
      "2022-04-25 18:46:33.871 | INFO     | __main__:train:47 - Train Step 120\n",
      "2022-04-25 18:46:33.871 | INFO     | __main__:train:49 - mae 2.76e-01 loss 7.11e-02 R 0.060 gap 0.01650412008166313 preds 0.013676728121936321\n",
      "2022-04-25 18:46:33.930 | INFO     | __main__:train:47 - Train Step 121\n",
      "2022-04-25 18:46:33.931 | INFO     | __main__:train:49 - mae 2.75e-01 loss 7.07e-02 R 0.060 gap 0.016711629927158356 preds 0.0133744515478611\n",
      "2022-04-25 18:46:33.995 | INFO     | __main__:train:47 - Train Step 122\n",
      "2022-04-25 18:46:33.996 | INFO     | __main__:train:49 - mae 2.74e-01 loss 7.03e-02 R 0.059 gap 0.01663872040808201 preds 0.01324339397251606\n",
      "2022-04-25 18:46:34.055 | INFO     | __main__:train:47 - Train Step 123\n",
      "2022-04-25 18:46:34.056 | INFO     | __main__:train:49 - mae 2.73e-01 loss 6.98e-02 R 0.060 gap 0.016886617988348007 preds 0.01329066976904869\n",
      "2022-04-25 18:46:34.118 | INFO     | __main__:train:47 - Train Step 124\n",
      "2022-04-25 18:46:34.119 | INFO     | __main__:train:49 - mae 2.72e-01 loss 6.94e-02 R 0.061 gap 0.016744663938879967 preds 0.013532279059290886\n",
      "2022-04-25 18:46:34.185 | INFO     | __main__:train:47 - Train Step 125\n",
      "2022-04-25 18:46:34.185 | INFO     | __main__:train:49 - mae 2.71e-01 loss 6.90e-02 R 0.061 gap 0.016631972044706345 preds 0.013422956690192223\n",
      "2022-04-25 18:46:34.250 | INFO     | __main__:train:47 - Train Step 126\n",
      "2022-04-25 18:46:34.251 | INFO     | __main__:train:49 - mae 2.70e-01 loss 6.87e-02 R 0.063 gap 0.016504919156432152 preds 0.013156179338693619\n",
      "2022-04-25 18:46:34.317 | INFO     | __main__:train:47 - Train Step 127\n",
      "2022-04-25 18:46:34.318 | INFO     | __main__:train:49 - mae 2.69e-01 loss 6.83e-02 R 0.064 gap 0.01653626747429371 preds 0.013087878935039043\n",
      "2022-04-25 18:46:34.381 | INFO     | __main__:train:47 - Train Step 128\n",
      "2022-04-25 18:46:34.382 | INFO     | __main__:train:49 - mae 2.68e-01 loss 6.79e-02 R 0.065 gap 0.016763456165790558 preds 0.013248217292129993\n",
      "2022-04-25 18:46:34.447 | INFO     | __main__:train:47 - Train Step 129\n",
      "2022-04-25 18:46:34.448 | INFO     | __main__:train:49 - mae 2.67e-01 loss 6.75e-02 R 0.064 gap 0.016751747578382492 preds 0.013282835483551025\n",
      "2022-04-25 18:46:34.751 | INFO     | __main__:train:47 - Train Step 130\n",
      "2022-04-25 18:46:34.752 | INFO     | __main__:train:49 - mae 2.66e-01 loss 6.72e-02 R 0.065 gap 0.016949648037552834 preds 0.013270361348986626\n",
      "2022-04-25 18:46:34.816 | INFO     | __main__:train:47 - Train Step 131\n",
      "2022-04-25 18:46:34.816 | INFO     | __main__:train:49 - mae 2.65e-01 loss 6.68e-02 R 0.065 gap 0.016903432086110115 preds 0.013380986638367176\n",
      "2022-04-25 18:46:34.877 | INFO     | __main__:train:47 - Train Step 132\n",
      "2022-04-25 18:46:34.877 | INFO     | __main__:train:49 - mae 2.64e-01 loss 6.64e-02 R 0.065 gap 0.016928313300013542 preds 0.013484465889632702\n",
      "2022-04-25 18:46:34.940 | INFO     | __main__:train:47 - Train Step 133\n",
      "2022-04-25 18:46:34.941 | INFO     | __main__:train:49 - mae 2.63e-01 loss 6.60e-02 R 0.065 gap 0.016794610768556595 preds 0.013185963965952396\n",
      "2022-04-25 18:46:35.003 | INFO     | __main__:train:47 - Train Step 134\n",
      "2022-04-25 18:46:35.004 | INFO     | __main__:train:49 - mae 2.62e-01 loss 6.56e-02 R 0.065 gap 0.01674964465200901 preds 0.012773009948432446\n",
      "2022-04-25 18:46:35.070 | INFO     | __main__:train:47 - Train Step 135\n",
      "2022-04-25 18:46:35.070 | INFO     | __main__:train:49 - mae 2.62e-01 loss 6.53e-02 R 0.064 gap 0.01679118163883686 preds 0.012763652950525284\n",
      "2022-04-25 18:46:35.132 | INFO     | __main__:train:47 - Train Step 136\n",
      "2022-04-25 18:46:35.132 | INFO     | __main__:train:49 - mae 2.60e-01 loss 6.49e-02 R 0.066 gap 0.01678684912621975 preds 0.012888862751424313\n",
      "2022-04-25 18:46:35.193 | INFO     | __main__:train:47 - Train Step 137\n",
      "2022-04-25 18:46:35.194 | INFO     | __main__:train:49 - mae 2.60e-01 loss 6.47e-02 R 0.066 gap 0.01667865924537182 preds 0.012977281585335732\n",
      "2022-04-25 18:46:35.260 | INFO     | __main__:train:47 - Train Step 138\n",
      "2022-04-25 18:46:35.261 | INFO     | __main__:train:49 - mae 2.59e-01 loss 6.43e-02 R 0.067 gap 0.016712404787540436 preds 0.012942316010594368\n",
      "2022-04-25 18:46:35.330 | INFO     | __main__:train:47 - Train Step 139\n",
      "2022-04-25 18:46:35.331 | INFO     | __main__:train:49 - mae 2.58e-01 loss 6.39e-02 R 0.067 gap 0.016610583290457726 preds 0.013222308829426765\n",
      "2022-04-25 18:46:35.609 | INFO     | __main__:train:47 - Train Step 140\n",
      "2022-04-25 18:46:35.610 | INFO     | __main__:train:49 - mae 2.57e-01 loss 6.36e-02 R 0.068 gap 0.016775403171777725 preds 0.013306062668561935\n",
      "2022-04-25 18:46:35.673 | INFO     | __main__:train:47 - Train Step 141\n",
      "2022-04-25 18:46:35.673 | INFO     | __main__:train:49 - mae 2.56e-01 loss 6.33e-02 R 0.067 gap 0.016528859734535217 preds 0.013418583199381828\n",
      "2022-04-25 18:46:35.738 | INFO     | __main__:train:47 - Train Step 142\n",
      "2022-04-25 18:46:35.738 | INFO     | __main__:train:49 - mae 2.56e-01 loss 6.31e-02 R 0.068 gap 0.01639971137046814 preds 0.013519964180886745\n",
      "2022-04-25 18:46:35.801 | INFO     | __main__:train:47 - Train Step 143\n",
      "2022-04-25 18:46:35.802 | INFO     | __main__:train:49 - mae 2.55e-01 loss 6.28e-02 R 0.068 gap 0.01644117757678032 preds 0.01357389148324728\n",
      "2022-04-25 18:46:35.873 | INFO     | __main__:train:47 - Train Step 144\n",
      "2022-04-25 18:46:35.874 | INFO     | __main__:train:49 - mae 2.54e-01 loss 6.24e-02 R 0.069 gap 0.016461960971355438 preds 0.013662455603480339\n",
      "2022-04-25 18:46:35.941 | INFO     | __main__:train:47 - Train Step 145\n",
      "2022-04-25 18:46:35.942 | INFO     | __main__:train:49 - mae 2.53e-01 loss 6.21e-02 R 0.070 gap 0.016534946858882904 preds 0.013726109638810158\n",
      "2022-04-25 18:46:36.010 | INFO     | __main__:train:47 - Train Step 146\n",
      "2022-04-25 18:46:36.011 | INFO     | __main__:train:49 - mae 2.52e-01 loss 6.18e-02 R 0.069 gap 0.016612783074378967 preds 0.013790094293653965\n",
      "2022-04-25 18:46:36.073 | INFO     | __main__:train:47 - Train Step 147\n",
      "2022-04-25 18:46:36.074 | INFO     | __main__:train:49 - mae 2.52e-01 loss 6.15e-02 R 0.069 gap 0.01651226170361042 preds 0.013791987672448158\n",
      "2022-04-25 18:46:36.137 | INFO     | __main__:train:47 - Train Step 148\n",
      "2022-04-25 18:46:36.138 | INFO     | __main__:train:49 - mae 2.51e-01 loss 6.12e-02 R 0.070 gap 0.016527943313121796 preds 0.013983044773340225\n",
      "2022-04-25 18:46:36.201 | INFO     | __main__:train:47 - Train Step 149\n",
      "2022-04-25 18:46:36.202 | INFO     | __main__:train:49 - mae 2.50e-01 loss 6.10e-02 R 0.069 gap 0.016271712258458138 preds 0.014259389601647854\n",
      "2022-04-25 18:46:36.494 | INFO     | __main__:train:47 - Train Step 150\n",
      "2022-04-25 18:46:36.495 | INFO     | __main__:train:49 - mae 2.49e-01 loss 6.08e-02 R 0.069 gap 0.015971751883625984 preds 0.014279073104262352\n",
      "2022-04-25 18:46:36.555 | INFO     | __main__:train:47 - Train Step 151\n",
      "2022-04-25 18:46:36.556 | INFO     | __main__:train:49 - mae 2.49e-01 loss 6.04e-02 R 0.070 gap 0.01597633771598339 preds 0.014395847916603088\n",
      "2022-04-25 18:46:36.619 | INFO     | __main__:train:47 - Train Step 152\n",
      "2022-04-25 18:46:36.619 | INFO     | __main__:train:49 - mae 2.48e-01 loss 6.01e-02 R 0.071 gap 0.015891563147306442 preds 0.014626608230173588\n",
      "2022-04-25 18:46:36.677 | INFO     | __main__:train:47 - Train Step 153\n",
      "2022-04-25 18:46:36.678 | INFO     | __main__:train:49 - mae 2.47e-01 loss 5.99e-02 R 0.071 gap 0.01602254807949066 preds 0.014755147509276867\n",
      "2022-04-25 18:46:36.741 | INFO     | __main__:train:47 - Train Step 154\n",
      "2022-04-25 18:46:36.742 | INFO     | __main__:train:49 - mae 2.46e-01 loss 5.96e-02 R 0.071 gap 0.01605181209743023 preds 0.014841100201010704\n",
      "2022-04-25 18:46:36.807 | INFO     | __main__:train:47 - Train Step 155\n",
      "2022-04-25 18:46:36.808 | INFO     | __main__:train:49 - mae 2.46e-01 loss 5.93e-02 R 0.071 gap 0.016066255047917366 preds 0.014759805053472519\n",
      "2022-04-25 18:46:36.871 | INFO     | __main__:train:47 - Train Step 156\n",
      "2022-04-25 18:46:36.871 | INFO     | __main__:train:49 - mae 2.45e-01 loss 5.90e-02 R 0.071 gap 0.01601598598062992 preds 0.014724061824381351\n",
      "2022-04-25 18:46:36.936 | INFO     | __main__:train:47 - Train Step 157\n",
      "2022-04-25 18:46:36.937 | INFO     | __main__:train:49 - mae 2.44e-01 loss 5.87e-02 R 0.073 gap 0.01604919508099556 preds 0.014659267850220203\n",
      "2022-04-25 18:46:36.996 | INFO     | __main__:train:47 - Train Step 158\n",
      "2022-04-25 18:46:36.997 | INFO     | __main__:train:49 - mae 2.43e-01 loss 5.84e-02 R 0.072 gap 0.01605760119855404 preds 0.014490415342152119\n",
      "2022-04-25 18:46:37.066 | INFO     | __main__:train:47 - Train Step 159\n",
      "2022-04-25 18:46:37.067 | INFO     | __main__:train:49 - mae 2.42e-01 loss 5.81e-02 R 0.073 gap 0.015808425843715668 preds 0.014249720610678196\n",
      "2022-04-25 18:46:37.361 | INFO     | __main__:train:47 - Train Step 160\n",
      "2022-04-25 18:46:37.362 | INFO     | __main__:train:49 - mae 2.42e-01 loss 5.79e-02 R 0.073 gap 0.015925999730825424 preds 0.014363879337906837\n",
      "2022-04-25 18:46:37.424 | INFO     | __main__:train:47 - Train Step 161\n",
      "2022-04-25 18:46:37.425 | INFO     | __main__:train:49 - mae 2.41e-01 loss 5.76e-02 R 0.073 gap 0.015878716483712196 preds 0.014309991151094437\n",
      "2022-04-25 18:46:37.483 | INFO     | __main__:train:47 - Train Step 162\n",
      "2022-04-25 18:46:37.484 | INFO     | __main__:train:49 - mae 2.40e-01 loss 5.74e-02 R 0.074 gap 0.01588575355708599 preds 0.014573434367775917\n",
      "2022-04-25 18:46:37.545 | INFO     | __main__:train:47 - Train Step 163\n",
      "2022-04-25 18:46:37.546 | INFO     | __main__:train:49 - mae 2.40e-01 loss 5.72e-02 R 0.073 gap 0.015877284109592438 preds 0.014787886291742325\n",
      "2022-04-25 18:46:37.605 | INFO     | __main__:train:47 - Train Step 164\n",
      "2022-04-25 18:46:37.606 | INFO     | __main__:train:49 - mae 2.39e-01 loss 5.70e-02 R 0.073 gap 0.015966974198818207 preds 0.014822215773165226\n",
      "2022-04-25 18:46:37.670 | INFO     | __main__:train:47 - Train Step 165\n",
      "2022-04-25 18:46:37.671 | INFO     | __main__:train:49 - mae 2.39e-01 loss 5.68e-02 R 0.074 gap 0.015874389559030533 preds 0.014865941368043423\n",
      "2022-04-25 18:46:37.732 | INFO     | __main__:train:47 - Train Step 166\n",
      "2022-04-25 18:46:37.732 | INFO     | __main__:train:49 - mae 2.38e-01 loss 5.65e-02 R 0.074 gap 0.015755606815218925 preds 0.01489917654544115\n",
      "2022-04-25 18:46:37.796 | INFO     | __main__:train:47 - Train Step 167\n",
      "2022-04-25 18:46:37.797 | INFO     | __main__:train:49 - mae 2.38e-01 loss 5.64e-02 R 0.073 gap 0.015630358830094337 preds 0.014735184609889984\n",
      "2022-04-25 18:46:37.857 | INFO     | __main__:train:47 - Train Step 168\n",
      "2022-04-25 18:46:37.858 | INFO     | __main__:train:49 - mae 2.37e-01 loss 5.61e-02 R 0.074 gap 0.015534880571067333 preds 0.014526554383337498\n",
      "2022-04-25 18:46:37.921 | INFO     | __main__:train:47 - Train Step 169\n",
      "2022-04-25 18:46:37.922 | INFO     | __main__:train:49 - mae 2.37e-01 loss 5.60e-02 R 0.073 gap 0.015385033562779427 preds 0.01428006961941719\n",
      "2022-04-25 18:46:38.205 | INFO     | __main__:train:47 - Train Step 170\n",
      "2022-04-25 18:46:38.206 | INFO     | __main__:train:49 - mae 2.36e-01 loss 5.58e-02 R 0.074 gap 0.015458567999303341 preds 0.013963073492050171\n",
      "2022-04-25 18:46:38.265 | INFO     | __main__:train:47 - Train Step 171\n",
      "2022-04-25 18:46:38.266 | INFO     | __main__:train:49 - mae 2.36e-01 loss 5.56e-02 R 0.074 gap 0.01548558659851551 preds 0.013722680509090424\n",
      "2022-04-25 18:46:38.323 | INFO     | __main__:train:47 - Train Step 172\n",
      "2022-04-25 18:46:38.323 | INFO     | __main__:train:49 - mae 2.35e-01 loss 5.54e-02 R 0.075 gap 0.01535476092249155 preds 0.013514507561922073\n",
      "2022-04-25 18:46:38.380 | INFO     | __main__:train:47 - Train Step 173\n",
      "2022-04-25 18:46:38.381 | INFO     | __main__:train:49 - mae 2.35e-01 loss 5.52e-02 R 0.076 gap 0.015372805297374725 preds 0.013361970894038677\n",
      "2022-04-25 18:46:38.437 | INFO     | __main__:train:47 - Train Step 174\n",
      "2022-04-25 18:46:38.438 | INFO     | __main__:train:49 - mae 2.34e-01 loss 5.49e-02 R 0.076 gap 0.01560627855360508 preds 0.013246795162558556\n",
      "2022-04-25 18:46:38.498 | INFO     | __main__:train:47 - Train Step 175\n",
      "2022-04-25 18:46:38.499 | INFO     | __main__:train:49 - mae 2.34e-01 loss 5.48e-02 R 0.076 gap 0.015596084296703339 preds 0.013306556269526482\n",
      "2022-04-25 18:46:38.560 | INFO     | __main__:train:47 - Train Step 176\n",
      "2022-04-25 18:46:38.561 | INFO     | __main__:train:49 - mae 2.33e-01 loss 5.46e-02 R 0.075 gap 0.015815619379281998 preds 0.013248568400740623\n",
      "2022-04-25 18:46:38.615 | INFO     | __main__:train:47 - Train Step 177\n",
      "2022-04-25 18:46:38.616 | INFO     | __main__:train:49 - mae 2.33e-01 loss 5.44e-02 R 0.075 gap 0.01581350527703762 preds 0.013224887661635876\n",
      "2022-04-25 18:46:38.675 | INFO     | __main__:train:47 - Train Step 178\n",
      "2022-04-25 18:46:38.676 | INFO     | __main__:train:49 - mae 2.32e-01 loss 5.41e-02 R 0.076 gap 0.015838192775845528 preds 0.013222788460552692\n",
      "2022-04-25 18:46:38.734 | INFO     | __main__:train:47 - Train Step 179\n",
      "2022-04-25 18:46:38.735 | INFO     | __main__:train:49 - mae 2.31e-01 loss 5.38e-02 R 0.076 gap 0.015912165865302086 preds 0.013090976513922215\n",
      "2022-04-25 18:46:39.032 | INFO     | __main__:train:47 - Train Step 180\n",
      "2022-04-25 18:46:39.032 | INFO     | __main__:train:49 - mae 2.31e-01 loss 5.36e-02 R 0.075 gap 0.01565968245267868 preds 0.013146108016371727\n",
      "2022-04-25 18:46:39.092 | INFO     | __main__:train:47 - Train Step 181\n",
      "2022-04-25 18:46:39.093 | INFO     | __main__:train:49 - mae 2.30e-01 loss 5.34e-02 R 0.076 gap 0.01595701277256012 preds 0.013227338902652264\n",
      "2022-04-25 18:46:39.153 | INFO     | __main__:train:47 - Train Step 182\n",
      "2022-04-25 18:46:39.154 | INFO     | __main__:train:49 - mae 2.29e-01 loss 5.31e-02 R 0.077 gap 0.015865499153733253 preds 0.01329041551798582\n",
      "2022-04-25 18:46:39.209 | INFO     | __main__:train:47 - Train Step 183\n",
      "2022-04-25 18:46:39.210 | INFO     | __main__:train:49 - mae 2.29e-01 loss 5.29e-02 R 0.078 gap 0.015803033486008644 preds 0.013311916030943394\n",
      "2022-04-25 18:46:39.274 | INFO     | __main__:train:47 - Train Step 184\n",
      "2022-04-25 18:46:39.275 | INFO     | __main__:train:49 - mae 2.28e-01 loss 5.27e-02 R 0.079 gap 0.015754440799355507 preds 0.01314577553421259\n",
      "2022-04-25 18:46:39.337 | INFO     | __main__:train:47 - Train Step 185\n",
      "2022-04-25 18:46:39.338 | INFO     | __main__:train:49 - mae 2.28e-01 loss 5.25e-02 R 0.079 gap 0.015779541805386543 preds 0.013159257359802723\n",
      "2022-04-25 18:46:39.402 | INFO     | __main__:train:47 - Train Step 186\n",
      "2022-04-25 18:46:39.402 | INFO     | __main__:train:49 - mae 2.27e-01 loss 5.23e-02 R 0.081 gap 0.015912791714072227 preds 0.013158989138901234\n",
      "2022-04-25 18:47:00.264 | INFO     | __main__:test:55 - Test epoch 1\n",
      "2022-04-25 18:47:00.265 | INFO     | __main__:test:57 - mae 1.21e-01 loss 1.29e-02 R 0.005 l_test 9.24e-01 l_train 9.40e-01 \n"
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:04:36.352 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_w_bilinear_op.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:07:01.514 | INFO     | __main__:<module>:9 - Test 0.3069 +- 0.0566\n",
      "2022-04-28 11:07:01.515 | INFO     | __main__:<module>:10 - Train 0.1535 +- 0.0122\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 1.0508\n",
      "MSE for Dimension 2: 1.6263\n",
      "MSE for Dimension 3: 0.8530\n",
      "MSE for Dimension 4: 0.8873\n",
      "MSE for Dimension 5: 1.3194\n",
      "MSE for Dimension 6: 0.6710\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.38      0.48      6826\n",
      "         1.0       0.15      0.11      0.13      2121\n",
      "         2.0       0.23      0.61      0.33      1717\n",
      "         3.0       0.01      0.03      0.02       408\n",
      "\n",
      "    accuracy                           0.35     11072\n",
      "   macro avg       0.26      0.28      0.24     11072\n",
      "weighted avg       0.47      0.35      0.38     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.34      0.47      8096\n",
      "         1.0       0.03      0.14      0.05       469\n",
      "         2.0       0.09      0.49      0.15       790\n",
      "         3.0       0.17      0.08      0.11      1717\n",
      "\n",
      "    accuracy                           0.30     11072\n",
      "   macro avg       0.27      0.26      0.20     11072\n",
      "weighted avg       0.60      0.30      0.37     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.38      0.29      2716\n",
      "         1.0       0.29      0.08      0.12      4925\n",
      "         2.0       0.17      0.63      0.27      1293\n",
      "         3.0       0.15      0.05      0.07      2138\n",
      "\n",
      "    accuracy                           0.21     11072\n",
      "   macro avg       0.21      0.28      0.19     11072\n",
      "weighted avg       0.24      0.21      0.17     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.41      0.46      4859\n",
      "         1.0       0.08      0.04      0.06      1442\n",
      "         2.0       0.11      0.06      0.07       561\n",
      "         3.0       0.37      0.57      0.45      4210\n",
      "\n",
      "    accuracy                           0.40     11072\n",
      "   macro avg       0.28      0.27      0.26     11072\n",
      "weighted avg       0.39      0.40      0.39     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

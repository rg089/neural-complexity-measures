{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Importing the libraries\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# %%\n",
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_wo_hpred\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Reading the Dataset\n",
    "\n",
    "# %%\n",
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]\n",
    "\n",
    "# %%\n",
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Combining the dataset\n",
    "\n",
    "# %%\n",
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "# %%\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Adding an accumulator to keep track of the metrics\n",
    "\n",
    "# %%\n",
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Args\n",
    "\n",
    "# %%\n",
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 200\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 6\n",
    "dec_depth = 6\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Memory Bank\n",
    "\n",
    "# %%\n",
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))\n",
    "\n",
    "# %% [markdown]\n",
    "# # Modelling\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Utility Functions\n",
    "\n",
    "# %%\n",
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)\n",
    "\n",
    "# %%\n",
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Multi-Headed Attention (for NC Model)\n",
    "\n",
    "# %%\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs\n",
    "\n",
    "# %%\n",
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Encoder with Attention\n",
    "\n",
    "# %%\n",
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Neural Complexity Model\n",
    "\n",
    "# %%\n",
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task Learner\n",
    "\n",
    "# %%\n",
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training the Task Learner\n",
    "\n",
    "# %%\n",
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())*5.0/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Defining the model\n",
    "\n",
    "# %%\n",
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Adding a logger\n",
    "\n",
    "# %%\n",
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training\n",
    "\n",
    "# %%\n",
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Testing\n",
    "\n",
    "# %%\n",
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    \n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Population\n",
    "\n",
    "# %%\n",
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Running the task learner for a few steps initially\n",
    "\n",
    "# %%\n",
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Main Training Loop\n",
    "\n",
    "# %%\n",
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:32:25.343 | INFO     | __main__:<module>:66 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Evaluation using trained NC Model\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Loading the meta_test dataset\n",
    "\n",
    "# %%\n",
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)\n",
    "\n",
    "# %%\n",
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_wo_hpred.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training the Task Learner with NC \n",
    "\n",
    "# %%\n",
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Testing\n",
    "\n",
    "# %%\n",
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels\n",
    "\n",
    "# %%\n",
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:36:57.640 | INFO     | __main__:<module>:10 - Test 0.1642 +- 0.0291\n",
      "2022-04-28 11:36:57.641 | INFO     | __main__:<module>:11 - Train 0.0479 +- 0.0038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABRnElEQVR4nO2dd5wU9f3/n7Oz9W6vwhW6ICCKCCoIagJ6BlARIYCFiDWamFiSGGNNiBpbTDQ/a4Lla4tirKBiQUCaUqQr9SgHR7le97bPzO+P2Z3bvds7jvOWPdjP8/Hgwe3u7Mx7Zmc+r8+7fD4fSdM0DYFAIBAkLaZEGyAQCASCxCKEQCAQCJIcIQQCgUCQ5AghEAgEgiRHCIFAIBAkOeZEG3CkqKqKorSv0EmWpXZ/N1EcazYLe+OLsDe+HM/2Wixyi58dc0KgKBo1Ne52fTczM6Xd300Ux5rNwt74IuyNL8ezvTk5aS1+JkJDAoFAkOQIIRAIBIIkRwiBQCAQJDnHXI4gFooSpLq6nGDQ3+p2paUSx9qMGk1tNputZGXlIMvHxU8nEAg6AcdFa1JdXY7dnkJqaj6SJLW4nSybUBT1KFr244m0WdM0GhrqqK4up2vXbgm2TCAQHC8cF6GhYNBPamp6qyJwPCBJEqmp6Yf1fAQCgeBIOC6EADjuRSBMspynQCA4ehw3QiAQCARHgqZpfPxDCYFjLFwcD4QQdAD19fV8+OF77fruu+++jdfr7WCLBALB4VhUWMHfvtzBSyv2JtqUhCOEoANwuer56KP2CsFsIQQCQQKo9wYBqGwQObfjomoo0fznP89y4MABrrvuF4wYMZKsrCwWLVpAIOBn9Ojz+eUvf43H42HmzHsoKytDVRWuu+5GqqqqqKgo5/bbf01GRibPPjsr0aciECQNplC+7RirKI8Lx50QzNtcysc/lMT8TJLa96Nfemo+Ewbntfj5zTffxu7du3jttbdZvXolX3+9kJdeeh1N07jnnjvYsGEdNTXVdO2awz/+8TQALpcLp9PJ//73Fs88M4vMzMwjN0wgELSfUN2FyBAch0KQaFavXsl3363k+uuvAsDjcbN//z5OO+10nn/+aV544RnOPfenDB16eoItFQiSG1NICI61Qabx4LgTggmD81rsvR+NAWWapjFjxnVMnjy12WevvPImK1Z8w3/+8xxnnTWK66+/Ka62CASClhGhoUZEsrgDSElJwe3Wp4IdOfJs5s372HhdXl5GdbWeC7DZ7IwffzHTp1/Njh3bIr7bkDDbBYJkRxVKcPx5BIkgIyOTIUOGcvXVlzNq1LmMHXshN998PQAORwozZ/6N/fuLeeGFp5EkE2azmTvvvAeASy/9OXfeeTtdunQVyWKB4ChiEoMzDYQQdBAPPPBI1OvLL58e9bpHj56MHHl2s+9Nm3Yl06ZdGVfbBAJBc8IyoAqHQISGBAJBciIZyeLE2tEZEEIgEAiSkvC8XRpCCYQQCASCpCQcGhIegRACgUCQpISrhUTVkBACgUCQpChCAAyEEAgEgqREDY0tFVVDQgg6hPZOQ33nnbdTX18fB4sEAsHhCHsEYooJIQQdQkvTUCuK0ur3/vnPZ0hLS4uXWQKBoBXUkCsgZEAMKOsQIqehNpvNOBwOunTpys6dO/jvf9/j3nv/SGlpKX6/n8suu5JJk6YAMG3aRF5++U08Hjd33nk7p502jO+/30ROTg6PP/4kNps9wWcmEBy/qIZHkGBDOgFxE4JDhw5x1113UVFRgclk4vLLL+faa6+N2mbVqlX89re/pWfPngCMHTuWW2+99Ucd17btfexb34n5mSRJ7XIDvSdfiW/QtBY/j5yGet26Ndx11+95443/0b17DwDuvXcm6ekZ+HxebrzxGs47r4CMjMyofezfX8wDDzzC3Xf/mb/85R4WL17E+PEXH7GtAoGgbSihpkBUDcVRCGRZ5p577mHw4MG4XC6mTp3KueeeS//+/aO2Gz58OLNmHV9z7Jx88mBDBADee+8dli5dDEBZWSnFxcXNhKBbt+4MGHASACedNIhDhw4eLXMFgqREhIYaiZsQ5ObmkpubC4DT6aRfv36UlpY2E4KOxjdoWou996MxDTWAw+Ew/l63bg1r1qxm1qxXsdvt3Hrrr/D7fc2+Y7FYjL9NJhlFab6NQCDoOESyuJGjkiPYv38/W7duZejQoc0+27BhA5deeim5ubncfffdDBgwoNV9ybJEZmZK1HulpRKy3La8d1u3OxLS0px4PG5k2WTsP/y/x+MmPT2d1NQUior2sGXLD022022XpMZzMJkkTCap2b7CSFLza9BZkGVTp7UtFsLe+NKZ7bXZ9c6XbJYNGzuzvbHoKHvjLgQNDQ3cfvvt3HfffTidzqjPBg8ezKJFi0hNTWXJkiXccsstzJ8/v9X9KYpGTY076j1N09rU04+XR+B0pnPqqUP5xS+mYbPZyc7ONo4zYsQoPvzwfWbMuJxevfpwyimnoiiq8bmi6LZHnoOqaqiq/jqWzZrW/Bp0FjIzUzqtbbEQ9saXzmxvQ2jRen9AMWzszPbG4kjszclpuUJR0uLoFwUCAW6++WZ+8pOfcP311x92+4KCAt5//32ys7Nb2afS7MRLSvaSn9/nsPs/WqGhjiSWzW0930RwPD9InQFhb8fx6qp9vLC8iOG9Mvj35Xq0ojPbG4uOEoK4jSPQNI3777+ffv36tSgC5eXlRnxu06ZNqKpKVlZWvEwSCAQCg8a5hhJsSCcgbqGhtWvXMnfuXAYOHMikSZMAuOOOOzh4UK+GmT59Ol9++SWzZ89GlmXsdjtPPfWUMTWsQCAQxJPwFBNCB+IoBMOHD2f79u2tbjNjxgxmzJjRIcfTNC0pRERUOAgEHUO4akgVLsHxMcWE2WyloaHuuG8kNU2joaEOs9maaFMEgmOecGgoKITg+JhiIisrh+rqclyumla3a+/I4kTS1Gaz2UpWVk4CLRIIjg/CNRiKEILjQwhk2UzXrt0Ou92xVhEAx6bNAsGxgPAIGjkuQkMCgUBwpISFQHgEQggEAkGSEhaAoHpsjS2KB0IIBAJBUhJ2BIRHIIRAIBAkKSJH0IgQAoFAkJQ0hoaEEAghEAgESYlIFjcihEAgECQl4RXKhEcghEAgECQpqggNGQghEAgESYlIFjcihEAgECQlkVNMHGtTz3Q0QggEAkFSokY0/kpy64AQAoFAkJxECUGSh4eEEAgEgqQksvFP9mkmhBAIBIKkJNIJEB6BQCAQJCGKFukRCCEQCASCpCNyiUrhEQgEAkESogqPwEAIgUAgSEoUkSMwEEIgEAiSksjQUDDJBxIIIRAIBElJVGhIjCwWCASC5CPSCQgqYhyBQCAQJB2qqiGbJAD8IjQkEAgEyYeiadjNehMYEB5BfDh06BBXX301F110ERMmTOD1119vto2maTz88MOMHTuWiRMnsnnz5niZIxAIBFGomobdIgNCCMzx2rEsy9xzzz0MHjwYl8vF1KlTOffcc+nfv7+xzdKlSykqKmL+/Pls3LiRBx54gPfeey9eJgkEAoGBqmJ4BCI0FCdyc3MZPHgwAE6nk379+lFaWhq1zcKFC5k8eTKSJDFs2DDq6uooKyuLl0kCgUBgoGgadosIDUEcPYJI9u/fz9atWxk6dGjU+6WlpeTn5xuv8/PzKS0tJTc3t8V9ybJEZmZKu+yQZVO7v5sojjWbhb3xRdjbgUgSqXYLAGabhczMlM5tbww6yt64C0FDQwO333479913H06nM+qzWKsCSZLU6v4URaOmxt0uWzIzU9r93URxrNks7I0vwt6OIxhUMKMLQW2dl5oad6e2NxZHYm9OTlqLn8W1aigQCHD77bczceJExo0b1+zz/Px8SkpKjNclJSWtegMCgUDQUSgaRmjIn+ShobgJgaZp3H///fTr14/rr78+5jYFBQXMmTMHTdPYsGEDaWlpQggEAsFRQdU07Ga9aijZhSBuoaG1a9cyd+5cBg4cyKRJkwC44447OHjwIADTp09nzJgxLFmyhLFjx+JwOHj00UfjZY5AIBBEoaiRyeLkrhqKmxAMHz6c7du3t7qNJEn89a9/jZcJAoFA0CKqBjYxoAwQI4sFAkGSomoaFpMJWRJCIIRAIBAkJYqqYTJJWGSTGFCWaAMEAoEgEaiahkkCi2wSHkGiDRAIBIJEoGogSxIWWUr6qiEhBAKBICkJh4asIjQkhEAgECQnqqYhS2A1mwgEhUcgEAgESYWmaagamEKhoYBYvF4gEAiSi3C7Hw4NiWSxQCAQJBnhhev1ZLEJvwgNCQQCQXKhhFwCvXxUEh5Bog0QCASCo004NCSLAWWAEAJBkvHSir38cY5YGzvZCYeGTFK4fFR4BAJB0lBU6WZ3ZUOizRAkGCM0ZJKwitCQEAJBcqFqjWEBQfLSmCxGhIYQQiBIMjS0mEukCpKL8PoDFtmEVTYRFB6BQJA8CI9AAI0rkllkCbMsCY8g0QYIBEcTTRMegQCCoYbfGvIIRI5AIEgihEcggEaPwCybQjkCIQQCQdKgapqRKBQkL+G5haxyuGoouT1FIQSCpEITHoEAjNlGLSYTVrNYwF4IgSCpUEWOQAAE1JAQmPWRxUBSh4eEEAiSCg3hEQgwqoQsJhOySQIaB5klI0IIBEmFJnIEAjDGDVhlE7KuAyTzbSGEQJBUqFpyP/ACnbBHYJYlJElXApXkvTGEEAiSCuERCABj3IBFNhGKDCV1yFAIgSCpUDWSuN8nCBMwQkONHkEyFxEIIRAkFcIjEEBjqahZNhmNoPAI4sC9997L2WefzSWXXBLz81WrVnHmmWcyadIkJk2axHPPPRcvUwQCAzGyWACNpaJWWcIkPALM8drxlClTmDFjBnfffXeL2wwfPpxZs2bFywSBoBl6sjh5H3iBTjBi9tGQDqAk8X3RJo/A7XajhgZg7Nmzh4ULFxIIBFr9zogRI8jIyPjxFgoEHYiGJjwCQeOAMrlxHEES60DbPIIZM2bw1ltvUVdXx3XXXcepp57KZ599xpNPPvmjDr5hwwYuvfRScnNzufvuuxkwYMBhvyPLEpmZKe06niyb2v3dRHGs2dzZ7TWZ9L5PRoYDSZI6vb1NEfZ2DCazGUmCLlkppKbaAHCm2TutvS3RUfa2SQg0TcPhcPD+++8zY8YMbrrpJiZPnvyjDjx48GAWLVpEamoqS5Ys4ZZbbmH+/PmH/Z6iaNTUuNt1zMzMlHZ/N1EcazZ3dnsDQQWAqmo3sknq9PY2RdjbMbjcPqyyidpaD163H4CaWg9Kl9ROaW9LHMn1zclJa/GzNoWGNE1j/fr1fPLJJ5x33nkAKIrSpoO3hNPpJDU1FYAxY8YQDAapqqr6UfsUCA5HOCwk8gTJjV/RMIdCQpIYWdw2IbjvvvuYNWsWP/vZzxgwYADFxcWMHDnyRx24vLzceBg3bdqEqqpkZWX9qH0KBIcjXDoq8gTJTUBRsYYmmwtXDSVzWXGbQkNnnXUWZ511FoDRYP/5z39u9Tt33HEHq1evprq6mtGjR3PbbbcRDAYBmD59Ol9++SWzZ89GlmXsdjtPPfWUMbBDIIgX4Wc9mR96gS4EltAkQybhEbRNCP74xz/y4IMPYjKZmDJlCi6Xi+uuu44bb7yxxe889dRTre5zxowZzJgx48isFQh+JBrCIxDoA8rC00+HO6CifPQw7Ny5E6fTyYIFCxgzZgxff/01c+fOjbdtAkGHowqPQEBTj0AMKGuTEASDQQKBAAsWLOCCCy7AYrGIMI7gmCT8sCfxMy8g2iMQk861UQiuuOIKCgoK8Hg8jBgxggMHDuB0OuNtm0DQ4QiPQAD6FBOWJsniZPYI2pQjuOaaa7jmmmuM1z169OCNN96Im1ECQbwQHoEA9MXrLU3KR5PZI2iTENTX1/Pcc8/x3XffAXoV0S233EJaWssDFASCzojhEYjJqJOaQFDFYm4aGkree6LN4whSU1N5+umnefrpp3E6ndx7773xtk0g6HA0MY5AQFOPQMw11CaPYN++fTz77LPG61tvvZVJkybFzSiBIF6IkcUCaDqgTH9PlI8eBrvdzpo1a4zXa9euxW63x80ogSBeiJHFAmipfDSRFiWWNnkEDz74IHfddRculwuA9PR0Hn/88bgaJhDEg/CzLjyC5Ca6fFRMMdEmIRg0aBAff/yxIQROp5PXXnuNQYMGxdU4gaCjaSwfTawdgsQippiI5oiWqnQ6ncb4gddeey0e9ggEcaUxWZzET70g5hQTyVxJ1u41i4VrLTgWaUwWJ9YOQWKJHlCmv5fMXmK7hUBMMSE4FhEegQAgGLN8NHnviVZzBKeffnrMBl/TNHw+X9yMEgjiRWOyOKFmCBJMUFExN8kRKGoCDUowrQrB+vXrj5YdAsFRwSgfTeJ4cLKjaRqKhrFCmQnhEbQ7NCQQHIsYC9Mkce8v2VFCyQA5LAShVjCZbwkhBIKkQhU5gqQnGBICsym6akh4BAJBkiCqhgTBph6BqBoSQiBILjSRI0h6Gj0CkSMII4RAkFSIkcWCpjkCsR6BEAJBktG4ME0SP/VJTjOPQMw1JIRAkFwIj0CgNBECSSxMI4RAkDxomiZmHxU0SxaH/0/mzoEQAkHSEPmcJ/NDn+w08whC7ydz50AIgSBpiGz8kzkMkOwEQ6MJm+cIEmZSwhFCIEgaInt8QgeSl8aqoejZR4VHEAfuvfdezj77bC655JKYn2uaxsMPP8zYsWOZOHEimzdvjpcpAgEgPAKBTtOqIUl4BPETgilTpvDyyy+3+PnSpUspKipi/vz5/O1vf+OBBx6IlykCASA8AoFOUGlaPqq/n8ydg7gJwYgRI8jIyGjx84ULFzJ58mQkSWLYsGHU1dVRVlYWL3MEguhksRhZnLQoWtMBZcIjaNOaxfGgtLSU/Px843V+fj6lpaXk5ua2+j1ZlsjMTGnXMWXZ1O7vJopjzebObK/ZFzT+TkmxkZmZ0qntjYWw98djq3ADkJnhIDMzBZMtoL9vt3RKe1ujo+xNmBDESsy0ZdUzRdGoqXG365iZmSnt/m6iONZs7sz2uiKEoN7lpabG3antjYWw98dTW+8FwOv2UVPjNu4Lt9uPoqidzt7WOJLrm5OT1uJnCasays/Pp6SkxHhdUlJyWG9AIPgxRMaAk3k1qmQnnCOQxRQTBgkTgoKCAubMmYOmaWzYsIG0tDQhBIK4EhkDTuZSwWQnnCNomixO5lsibqGhO+64g9WrV1NdXc3o0aO57bbbCAZ1F2z69OmMGTOGJUuWMHbsWBwOB48++mi8TBEIgOjGPxEOQVGVG1mS6JXlSMDRBWGCIXewebI4eZUgbkLw1FNPtfq5JEn89a9/jdfhBYJmJNojuOzVNQB898fRR/3YgkaarlAmFqYRI4sFSUSUR5DED32y0ziyWH8tPAIhBC3yh49+4JWVexNthqADSbRHIOgcNPUIZJEjSFz5aGdnZ3kDqVY50WYIOhBVeAQCYq1HIDwC4RG0gKppxg0jOD6InoZa/LbJStP1CEDPEyRzRbEQghYIqhqKaCuOK9ROMtfQO+sOUFR17AxaOt5o6hGA7hUkc7hQCEELKKrwCI43Ip/zRHoET369i1ve25Sw4yc7TWcfhZBHkMSPuxCCFlBEaCgheAMKheWuuOxb02L/nQj8wt1MGE0XpgF9dLHwCATNEB5BYvjTx1v4xRvr8Ac7PmIbnSxO7G8rChEShxIjRyCR3NOOCCFoAUXVCCZxDyFRrCyqBsAfh6cyKjTU4Xs/MoQQJI6gqiFL0ZNcyiYJLYmnJhdC0AKKhvAIEkgwDqGTyDUIEh0GEEKQOBRVwyxHN32SyBEImqKF8gNCCI4u3oBi/B2O43Yk0UtVdvjuD3Ps6AOm2sQQnkShewTRU96LHIGgGeFGQgjB0SWypDIYh2sfvVTl0f1tm3o4DovwCBKF7hFEC4GE8AgETQgLQKITisnGvmqP8Xc8hCCRHkGgiYeTzL3PRNOSR5DMz7sQghiE5yuPR2MkaJkGf0RoKA45Ai2BVUNNzycgykcTRlCJ4RFIiS8pTiRCCGIQ9ghEaOjo4onKERxfHkHT8xGdjMQR1GJ7BEoSK4EQghjEWwhUTePP87by/cG6uOz/WGTJzkpqPAHjddNQSkeQyBxBoEk5bNPXgqNHUFGbeQQmKbnDdaJ0IQbhnkG8eghuv8KX28rp3zWVId3T43KMY4ldFQ3cOXdz1HtxKR/tRB5BQHgECUNRo0cVQzhHkCCDOgHCI4hBvD2CcCMnGgOdWIPH4hMaSmCOIHQ+U4d2o2em3VguUXD0Capq1KhiEB6BEIIYxF0IQmEP0RjoWEzNb8N4jCNI5FxDYfEf0TuTE7JTRLI4gSiqZixKE0YSHoGgKeHeW7ye1fD+RcJQJ1YILi4eAYn0CBonOjObJPHbJ5CgqsX0CET5qCCKeA8oCzcColeoE8szik/5aOy/jwbh39osm7DIJpEsTiC6R9C0fFQS5aOCaI5ajqCTNwYvflvEFa+tiftxIgXRbtZvyfiMLG78O1E5ArNJwiJLIj+UQIRH0BwhBDGIf47g2AgNvbRiH7sr47+SVuR1SLPrhWzxEMmoZHHE+wt3lDNh1krqvcEOP2aYyNCQxWQS+aEEEmzBI+jkj2NcEUIQA0MI4tRDCDcKoleoEzlmwBmajC3eHkG4QkRRNe75ZCtlLj+lLl+HHzNM2OuxyCbMsiTCgglEieERyGKKCUFTgnGeYsLwCESvEIgODTmt8ROC6GSx/v+mA7XGe/H8PSJDQ2aTFJcBc4K2EdsjEFNMCJqgRoSG4lFb3JgjODbuvHj3lCIb4DS7PitnvKeYCP+uLm/jaOZ4Lh8ZPh+LLGGRTXFJhgvaRqxksZh0Lo4sXbqU8ePHM3bsWF588cVmn69atYozzzyTSZMmMWnSJJ577rl4mtNmInMD8XAKjoUcQaRt8W60onIE4dBQO4+pqBpvr93PvM2l+IMqk19ezZKdlUDTSef0/72BRhGKZ/I+LHZmk0kkixNMUFVjCAFJvD5ZHKeYUBSFhx56iFdffZW8vDymTZtGQUEB/fv3j9pu+PDhzJo1K15mtAulyQhUGamVrY8cI0fQiUNDNW6/8XdAVbHGsc8QFRoycgTtuzZvfFfMC8uLkCU4q08mB2q97K5sYEz/Lk2mmNBfeIONE93FVQgiq4ZMJhRVQ9U0TFLH3luCwxNQYq1QJjyCuLBp0yb69OlDr169sFqtTJgwgYULF8brcB1KZA81HpVDxjiCTtwrrHRHTAAXZ48gsgEOewTtve57Q2sa2MyyMa21O/R/9KRz+v+RHkE8Q0Phc7TIkjHhmQgPJYaAomKNMelcMqdt4iYEpaWl5OfnG6/z8vIoLS1ttt2GDRu49NJLufHGGyksLIyXOUdEZCMUj/BNuAHozMniqgiPIN52BmKEhtorPp5w4x9QqHDp5xCe3jqWR+BLgEcQDkuIhHFi8AVVrE09ApLbI4hbaChWklVq4gYPHjyYRYsWkZqaypIlS7jllluYP39+q/uVZYnMzJR22STLpjZ9157iMv52ptnJTLG263gtYXPo+9Okw59LW23uaLxatfF3itNOZqajTd9rj72WiPV7c7L045itcrvOO3IkQGloXEAwdJ0dEb+jxWomMzMlygsw2yxxu9YWmwWA7KxU0p02IHRdj/DeStT90F46o71BVSMt1Rpll80qI8tSp7S3NTrK3rgJQX5+PiUlJcbr0tJScnNzo7ZxOp3G32PGjOHBBx+kqqqK7OzsFverKBo1Ne0b5JSZmdKm79bXe42/q2rc4O/YgUa1of17/cph7WmrzR3N/opGMaysdpPSxlRae+ytj6jf1wIKsgQut79d513v9iOhJ/62hUpDa10+amrcuCKO4/EGqKlx4/Y1/ra19d64Xeu60LHdLi/B0P1UWe1GOsJ7K1H3Q3vpjPb6gipqUI2yS1FUNFX/v7PZ2xpHcn1zctJa/CxuoaEhQ4ZQVFREcXExfr+fefPmUVBQELVNeXm54Tls2rQJVVXJysqKl0ltRol7jqDzJ4tdEQ1kvEMYkWEgh0XG/CPKK90Bld4hr6Koyh16Lxwaap4j8AWPdtWQZMy22plDg8crqqYRVLXYOYLkjQzFzyMwm83MnDmTG2+8EUVRmDp1KgMGDGD27NkATJ8+nS+//JLZs2cjyzJ2u52nnnqqWfgoEcQ9WRzOEXTiO8/li4ydx7t8tLFBzHCYf9TsnJ6AQu8sB3urPewLCYEnlBDWNOhBOZlSA6qm56+8EctjHp1xBCYjWXysjCM5nogc4R2JJEloSZyziesKZWPGjGHMmDFR702fPt34e8aMGcyYMSOeJrSLyPLRuCSLj4GRxQ0RIYu4J4sVfcj/89OGcHqPjB8lBN6AQnaKBadN5lCdHo4JJ5BVNO6wvM9p0m6e00bpnwUUUiwy7oBCIBi/8wyoGhIgmySjERLJ4qOPP/Qb28zRQmAiuT0CMbI4BkdrQFlnLh89mh5BQNFd9TN7ZSJJkh4aamcj6Q4oOCwyOak2I6sRDg1pGjjxkCa5G6uGAipOmz6aOdZKaR1FUNEMT8CoGhIeQTNK6rx8u6cqbvv3G2W8TYRAjCMQNCWyDYrrOIJO3BBEeQRxFix9pGfjrWg2tX9SNk9A1YXAaY14r1EIbARw4DNmH/UGFVKsMiYp3uWjqpEbsBjjCIRH0JSJL63mdx/+ELf9h3/jpjkCKclzBEIIYhCM6BnEJ0cQWqqyE4cGXD6FjPCU0HG0s6TOS7U7YDSOQLtDQwFFRVE1UqzRQhAeUKZqGjYC2PEbRQregIrNLIcWi4ljjiDCIzCSxcnc8sQg8lmL1/rB/hZyBLJJEmsWC6KJf9XQseERZDj02vd42vm7D39g/vbyqLlfzCapXVVD4QbfbpHJCdXqg+4RaJqGpoFd8mOTgmiqvq0vqGA36/P/xAoNVUcMrPsxRM54KZLFsdlcUm/8HS+RbClHICE8AkETokYWx2P20WMgWezyKWSGhCCedlY26A1t5NwvlnbmCMIhIIfZFOURqJreEwx7BAAWVU8k6x6BCWsMj2BbaT3j/r2Sz7Y0HxF/pASUxonO2pMsTobe6g+H6oy/fXFK3IscQWyEEMTgaHkEitZ5l8dz+YOGEMSz5+oNPfCWph5BO657uExUDw3Zoj/zK6iAjZDwqKFBfQEFm1lfR7ipR7ClVB9Ut2ZfzRHb0pSg2jjRWXuSxVe9uY6Hv9zxo+3ozESOXYmXELSWI+ikj+JRQQhBDOItBJFJyc448Zg/qBJQNLLCQhCnHIGqacYDH9lDM8vtFYLI0FD01A3uUHjIJukegTnkEfiCeo7AKkvNksXhMILV/OMfk4CiGmJ3pMniXRUNFJY3MPeHksNvfAwTWakWrwquI/UIatwBPtp06Lj3yIQQxECJc7I4cp+dsZbcFaoYykwJh4biG68FOiRZHBaClIgcQbjn5w4oqKGqIYgMDSnYLCbMMUJD4f01jSe3h3A1EzQmizcdrD9sAxNUVF5YXgRArrNj57zqbER5BIE4CUFQv95NJ52LtXi9qmlMenk1j35VSHGNl+MZIQQxiPII4pgjgM6ZMGwI9cyM0FCEvTvLGyh47ltK63/8+r6eiFG9zZPFPyJHYDHRJcWCBHRJ1RtPjz/kERD2CEKhoaCKPZQjaNoLDc9e2hFhCk9AwWHVhSCcLH5r7X42Hqhr7Wu8vfYAS3fpC+u0VZC2l7m48D8rqYjjGszxwOVvvB98cfYImnp5sRav33SgzhiD0lFFA50VIQQxiHuOIKLx74wJ47BHYISGImx8e+1+6n1BvumAQT/eiAY2MllsNpkIqho3vbOBP3zU9prycI7AYdXnK5p+Zg8uPFmf6LAlj8AXyhHECg2VhxLZdd62TQzn8gWp8QRifuYJKDgs+jl2z7AzflAOAPtrPa3us7CigW7pNn5+Wj7uNvaS1+2vpbLBf8z1YiM9An/ccwTNPYKm3llZhJDWeDp24snOhhCCGMQ7NBRZEdMZa8nDD2Rj1VCjje6I8EskmqYx9/tD1LXQEMYiclGYKI9A1stHNxyoY/nuqjb/Bo0egW7bH847kQsG6A2ux6+gqqqRI7CoXjRNwxOVLI4+TtgjqG+jEPzls22MfWGFMdldU9vC18wkSfx53EAAyl2t9zRL67zkp9lwWGRjqozDsTd0/IYYM5vWe4OdNt7d4FcMryf+OYJYaxZHbxvZ+NcewX19LCKEIAZHq2oIOntoqPmAsnCtftMe28E6Lw/PL+SzI0hoRi4TGdk4Nc0RRJYVtka4oXSYG0UqnOeocvsxqY2NrkXzEVQ13Uswykejz6myQe8R1vnaJgTLd+te0ssr9ja3LaBijxBPu0UmzWY2xKYlSut95KXbSbHIeAIKahvux7AQuJsIx8YDtRQ8/y2LCisOu49E4PIF6RL6vbxxKx9tOUfQVCBrvY2Nf7UQguQjsj2Ie46gEyaLww9AVmjRlCiPINS41DdpHGtDvac6b9sfGE8LM3+aTaYokVhRVN2m/YWXpgzH4kHPEZgkKHP5MSmNoRKL6jNi//rI4uhpLTRNM0JD9a2c06E6L48vKKTGEzC8mshlPsN4Q3MgRdLVaTWOEQtF1Sh1+clLs5FildGIFs+WKKrSw02uJkLw7NI9AOyp7Jzz7bt8QbJDOZ14hYZaqgSLlSOo9QRIs5mxmU0thvyOF4QQxCDuoSGl7R7Bl5tLuHPO5rjY0RJVoYYsO8WiN5ARxw4LQFMhCIdPXG0Mo0CT9YKj8gUSVQ2ND96WiBGnrVFS7yXLYYlKqppNEtkpVspdPmSlsdG1aj6j1xlrHEGtJ2j8Nq3lCOZvK+eDjYe47q31hsA3xPAg3LGEINXaakK3yu1HUTUjNATNe/lNcfmCVITEJdIOb0Bh48G2eVaJosGvkJ0SXyFoLUfQtGqoxhMgw2Em02ERQpCMxH3N4jbuf2dFA7e+s4Eluyoprm49qdiRVLkDpFpl7BYZiyk6ZBJuZFxNGru6FgSiNSLd/8hG2GySjFyEzWxie5mrTXHtAzVeemTam72f47RS5vIja41CYFa9xjmkWsMeQaMN+2r06z0wJ5V6X7DFgX8ldbqXcaBW/z87xdLs2gQVfVxGOFkcaVdFKx5BSWga7fx03SOARq+nJfZG3CeRohGZi4is1+8MaJrGiqIqfEGV7FBoKN4ji5sPKJOarcFX4wmQ6bAIIUhWoqehjnOyuJWk2Pr9tcbfK/ZWc+AwFSag98yvfH0NK4vaX9VT1eA3HkiL3DjvjyegGL3jpr3kcPikLR5BUNWoaPA3WRQmduJ4RO9MqtyBVhvMMAfrvHRPby4EuU4b5S4fktLY+1b9Hg6GGu/u6Xa9fDSi8dlXrYdPTu2Wjqq13BPfV+2JGrx2cl5as4Y2csRzJF1TbZS7GifAe3bpHj7cdMj4PFyimxfpEfiCSA1lLQ6D3RuRqI4UjcgKGFcHL736Y/l6ZyW3f6BXh4VDQ/ErH21cFyISk9Tc+6/xBENCYBbJ4uOdA7WeZknCzpIsDjcEEvDU17uY/PJ3h933st2V7Kpw8/KKfe22r8rtN1x02SQRUFVUTePBL7Yb2zTt+YeFoS0ewb++3sVF/1nJuuJGoYtshFOtjesljeidCcATC3dGCUdTFFXjUJ0vpkfQ1Wml3OVHVhsbQ5/Xxf5QeWXPTDtWc/SAsn3VHmSTxEl5zqjzA70H+/yyPWwpqWdftYfhvTKNz/p1SWnW0EaOeEbTSFn1D8zlP5DjtBJUNWo8Aeq8Ad74rpjHvio0vrd4ZwU2s4nuGXaj4ihYvZ+ur52BY93zMa9DUZUbWdI9k8iqoUiPIFboKpEsD42TAIxkcTxzBFazqdlKiCZJaqatemhIeATHPQ3+IFe8tpYPNx6Kej+oaUZ5WUcKgT+o4g0oBBXNiGO3Nrlaab2PnpkOYwASHH7ysfDgo40H69jUzphwlTtAluER6A1kZYOfhTsqGD8oh9N7pDcLfxi5g8Mki31BlXc3HASImjIhMlncK6uxMR/ROxOrLLF4ZyWfbm558rcylw9F1Vr0COq8QRR/Y7LYrPr44VAddouJrnIDY6vfRlEbz2lftYceGXayQyW0kUnwg3Ve3lm9k6zPriPTVUjvLAdPTh7ML0f1Js1uJqBoUaGNyBHPUqCB1DVPYyucy4CcVECvNooclxFUVLaXufhyWzkzhvck1Wo2EuBq3QEAHJtebXaemqaxt8pDj0wHGXZLk9CQLoJ9u6R0qtCQomos29147kaOIE4eQUBRm+UHIPYUEzWeAJn2sBAcXjznbDrUIRMUJoKkFoJ91R58QZXdTaooVFUzbpaO7Jg8uqCQO+ZsJqhq2ENCEO6FqprWLExUWu8jP6I3CK0nLlVNY2VRNeMH5ZCdYuG5pbsN4fjHwp28+V1xm+yscgeMEbmW0Lw/4dDMzwbmkOGwNK8aCieLD9PILN+tC1W4EQwT6ZWFF58HyE+zs+CWc8hOsfDNnqqoSqNIDoR69y3lCABc7gbjPTt+Vu2tpldWCs4Vj3Jh2UuMUtcDeoO6p9JN7ywHXUPfLXf5OVjr5dVV+3hpxT6GmXZxmmcVF8mr6J3lYPSJXbj53BNw2syh69B4fSJHPEse/fxNnkrO6JlB/66pvLV2Pwu3N5Z07q50831IxCcP0ddWDt8DqqscANkd3eComsYVr69lUWEFw9IbeCjwDxRvY0egzOUnxSKT57R1aGhI0zRe+nYvd3z0Q7s6HsXVnqjedprNjEQ8y0fVZmMIIFQ+GvHaG1DwBVUyQ8niel+w1YWLNE3jka8K+evn24280bFEUgtBOAF7sMkPp0QIQUeWj248UMuOMpcuBKEHO1yR8/SS3dwwe0PU9qV1Xrpl2Hl22hDOPiEL0MsVW6LC5afBrzCsRwY3nt2H9Qfq2FLqosLl490NB3kmVD7YGkFVo9YTaMwRmEwEFdWod+/qtJJmMzcbZBV+fTiPYNPBOmxmExeFRvyGiewB9spsFIJUm4zDIjP2pByW765i9DPfxHTTw/mT7hnNhaBfV110dpU29jwd+KlyB+iV5UAK6t/tolahaRp/+WwbuyvdnNotjdzQnEVlLh//WryLF5YXMW9zKYOlIgBOlYoY0j3d2G942cv6KCHQz81ukTGFhEDyVCBJEr8bVM9VNf9hya4KzuvfBYBtZS6KazzYzCZy0/TjO6z6/Sg1lBv7lbyNZbXbSl1GWegkz4f8NPAN59Z/YXxe7vKR47TitMnNvLkfw4qial5csZc1xTXc8dEPR1zUsDuU0wiPtE536OWa7QkNeQNKVCNcVu9rPpGgosX0CKQmHkF43ECGw2LkLapilAWH2RORm3lr7YEjtr01Fu4oZ/62sg7dZ1OSSgiaxpj3hW7apgquaJpRZ9xRoSFPQOFAjZdab5A6b4CuoZtrX+gGWrOvhq2lLmN+flXTKHP56Z5h5+S8NH77kxMAjAXZY2EkPzPsFAzoaux3/vbGxqOlHnWYGrcfjUYX3Ryqrw97BF1TraTZzc16/uGqocM1MttKXQzMSW3WYA/McRp/hxs/0F12gHGDGoVjR5mr2X4P1nqRJchLay4Ep+Q5GZiTit/b2Eg5JP069s5OAZMueidIh2jwKyzcUcGEwXlcO6IXXRwmZEn3zraU1BveyimmIgDOMW3mhEPzjOStM5TfaIjhEaRYZExeXYzCglCgfMMN5i8YKJdw1wX9SbXKRu6hV6bDOP+wRyC7G39Ly/5vjL9XhIoDTuyawgl5uqCkBBuFoqzeT47TSqqt+W/3Y3htdTHd0228ftUZAPz2vU08vqCwzc/N7grdS7t/3EBmX3smJ+elHbEQlNX7+OeinUx4cRUTX1ptCMKEF1cx61t9cN9nW0qZ/vpavAEl5myy+lKlmuFBhz2y/l1T6ZISFoKWCxbC+a4eGXbWFte0ye6VRVXc8PaGw57rPZ9s5f5529q0z/aSNEKwpaSesx5bxK6KxvBAcahE8FCdLyr2rqiNMfyOEoI9lW7D9TxU56Nbup2h3dP53/qDHKz1GuGpsHtd1eAnqGp0y9Abnm6h2Hcsj6DBr9eOhz2b7hl2uqRa6dclheeW7eG5ZY2eQKxGNBJjDEFIqMyhZHFYCLqkWnHazLgDSlQoyxhH4Gt5CgNV09he5mJgrpO8iMb+5SuH8v+mnGq8NknNXffTuqfz+a9HAnpZbRSaSteDC+mVJkVVHIWRJIkrzuhhzDOkyTayrXpjOPWMnphCjesA6QAfbjxEUNU4v38XbDXbyZ/Vl4tTtrGt1EWZy8+U07rxm3NP4Pw0Pa/kkPykL/gd5pK1gB7a0K+Dvv/9NR6e/HqXvq1FjgoNAcgufT9PDm8gx2nj1G5pfH+wjuJqD70iQmThqiGzpwLVmoaSkou9cI7x+cqiak7Oc/LOtcPpbtNFrntwv/G57hHYcFrNR1Ti2xJyVSGOjS/jK9/Fuf260LdLCk9OHkxJvY8PNh6K6iG3xp5KN93T9aqo/iHPzWo2xS4f1TRQo20Pqhr3fbqV/60/aIRNa+beQfG7vwNgVWgw4mdbStlZ0cDO8oaYHkGvTAe+oMqhUGdqRVE16XYzp+Sn0SVV7yhUtlC5pmoaX20vJ9dpZcIpeewsb2jmMdfNuZ26D26Oeu/DTSV8f6iOwqb3M7B610Ee/s9/jM6qfvrxG0uUNELQPc1CvlrC3O8bE5RhN9YXVKPcvqDamCwOKCoLtpe3WuapaVrM+WhWFlUZXsiuJj+2WZaYMDiPigY/k15ebVQShWejDDd23UIx73S7GVmCF5YXsafSzUvf7jW2mfHmOi76z0qjlj0sGqeFQhYjemcy+9ozgejlAGMRvvG6pesN9S/873Nr1SPU1rvIsJuxyCbSQ41d+HjQGBJqmiiN5ECNlwa/wqAmQjC0R4Yxr1GYi07O5dy+2cZryVPJCd/NpIc92Oxa2go/5rflD/CM9vcWyyovPiUPmxTythxdGWXdzeorUxmUn4Yp1Bj3Nx3gP98WAfq1s++YA8DPzBuN0c2D8pzcMCKPLE8Rnl5jjP1b9y8HaMwRhOLwH2w8ZFxTh9VkCIDJUwGahqlBvx/7+7cYx91R3sDekEdgnKPZhEkCq68SNTUP34BLsRYtQvLo4azC8gZO7ab/3sb5qHoHwBtQKK330SvLgdMm4wuqxv3sCSi89O1eI3cj1+wm88MpSBGeRyxS1j6Dc/kDvM29nGzWz2FojwzjPjtchyPMnio3fbtE54ussilm+ajp+7fIeHUERJQBL9tVycaDdfxl/EC+vvUcAIaUfsDFvnlIqNR6A3gDilGKvbfaEzNHcFKu7pFuOVSHqgSZsus+Hs74FBmFPEn/7SMHOYapdvuZ+dk21u2v5YZRvRnWMx0N2BQxLYqqaaTu/xrboZXGe/6gyqqianpJpXi3fgboXm2DP0iV24/6zVM8rTzM8sUfG99pLTT1Y0kaIehau4mFlt+zY8saXL4gpfU+CssbyA81SDM/22a48KrWGEf8fGsZ9366lS9aidF9ua2ci2atjJqaeWdFA7d98AP/XaP3ygrLG4i8/cwmiYmD8/jDef2M93KcVlbvq0bTNF5dVUyXVCtn99XdfEmSGJSXhi+octM7G3hxxV6mv76Wm9/daJRBbj1Ux+OON8nc+AKWvV9z06he/PH8E3ly0mD6d02ld5aDTzeXxhzE9tKKvXxdWMGOcheySaJf6OH8hee/nONbzqUHnmC26S9Y98znpyd2IcOqJ8f0tYA16nxBY5BO06kNwoQfxlPy04zQkxN31IMd5qGLB0V5CdaihTi2vMXk9G3srIjubdp2fQrAaYENWEINclPMJonfjNQTr66fPogkmchc/RjSwfXI9XqpbW+pnAHqHnpnOchKsWI5qD+42eZG+07KdSLX7kXSFAInTaXihk0Eup6K5YAepnFaADTUukNkv3E2dUVrje86InMEQS8E3IZHYClZo59DRL6hT4RHIEkSDouM3V+J6uiK95SrQAuSsuZpKt0BGvyKsX1YCHpKZeCuYF+1Bw04ITslIpmt/0b3fbqVF1fs5YVFm0lb8DtSv30Ey6HVzPvgxVYHdZXvL6SUbILITN19L4TyLCdkObDKEoXlzXu5TfEGFMqqqujfJXo1uZZCQ4XrvsLqLcd74HvjvU83l9I11crFp+ThtJkZ3TfD+OyB0wMcqvOxZGelUZU23rSan5uWNtt3/5xUZFS2H6hmx+ZVFLCKS2vfIGPedZz80Rh6UE5lk9CQpmnM/Hw7iworuGZEL6ac1o1Tu6VjM5v4KiIcu3PvXvKlarpRicenP6vr9tfgDii8aXmcC7f8EZ+rikkvr+a8Z79l/L9X4qnWf0Pbvq+N/RysjV8SOmmEQE3vhQmN04MbuOeTLdzzyRYceLm3oC8Aq/fVMC9UnqioGmaTCYnGHvLSXVXNXLNwdcmywhIGKDv5+4JC3l67Xx8pGSoHXLBDvyFW7q3mzFApJGhMqnqF3Ld/yrV5e5k6tBtmk8RVZ/aksLyBL7eVs25/Ldee1csoG0xZ/SRvZb/CqD6ZRoXOtNPyKSxuTExVFa3nSu1znCsfJ/PTq+l14BOuPKOHMcXzb39yAoXlDXzepMRtc0k9L367l7s+3sKOsgb6ZqfoobFgo1t6rnshp6jbSV31BH1LP2e15VcU799HUZWHB77YjsuncHJeGtAYH69w+fhw0yEj3rqwsJzu6TYG5KSGBvRofGz9M87lDx729zPX6OGVkZY9bC+t56+fb2PRjnLwN2Dd+zVvBS/AKzvJ/Hg6zqX3NwshAOSG2tVAz3PxnnwF1oMrMb96AVLQi/uMW/DauvLvtNd4+OKTkDxVmEvWAdBD1a/xiN6ZOG1m5Jrd+n2S2Q/NkU2g57lYDq3FVH+Qkz6byN/NL5Fdvgq5vpipda/xkuVJulAblSMAMHnKMTWUoMk2zNU7kbzVnNotnSyHhcH5aYwOJY/DOG1mHL5K1JQcyu19WJ81AccPb1C6fycZuOiTrZ+g3FDCwVRdROUdHxuzoZ6QrXsE55h+oNcHF7D/0H5jorxr3K9i3/4Btj1fAnBSzWLWtBDrVlQNW8N+FgdP4w+B35Lh3kPKWn1cg1k2cWLX1GYeQazc1MbdB1hg/h3XNvxf1Pu2UGiotN4XNYgyo0G/7mWFukBXNvj5ZnclF5+Sa4QEHx6TaWw/UtV/v7/N30G63Qxo/MXyX35Z8/+M39DY98ElrLLfxrS1V1K24RPjfeu+xUhqkJ/aCqPyd6+vLuahL3ewsqiaP5x3IreN7muI9dSh3fh8SynPLdvD6r3V/LBRt1eWNA7u2wnAsl1V2Mwmepr0arE9G74yjtk93Ua2SRfSm82fstp+K6NMW4QQdASqsxtaVj+uyt3L+v21lFaU803qnxhb8gKvXjaQ7uk25m0pZU+lm6IqDxP8X/BkzueMlLaS4zDxdWEFBc9/y+drt1JXX48vqPLL2Ru5/LU19N39Jp/a/kzNnjX8a/Fupv7fd7y7dA02/OyqcDPlldXsqXTz037Z5KfbGSAdYGz128h1e0n7+i7uHtODpbefy/hBOUySv2X5F2+Sa6rn4kFdQ8YHSf3uX2TunsPve+k38AUDu/Jgxqdssv+KFen3M0P+iocsr6JionrKRygpuaQvugPHhpeMcEnBgK70znLw+VbduzlU5+X11cXM/KwxEfXNnipOytW9AXPVDmRUXglepJuBCXPlNtK/ug2b0sA4eQ1/+Wwbn20p47qzenHtGblMk5fwh/+t4s3vipn8ync89lUhN72zgQffX8JV+//KLfnbkCQJ++a3eMf6MP1MJVj3Ljrs7ydX6w/QcPMuxg7K5ZvdVdz9yVb+/ua7SIqP+epwivPGAeD4/nXSv/g15pJ12Le8TdqXv0Wu2Y0U1Hv2mmzFf8LYqP0rGX0IjLqTPv4dDJF2YznwLRIawS6DyPPrZbfXjOip2xISJSXrRAA8g2cAGtn/PQdr9Q6uMC/GXbgQgDGmjYyV1/Ks5VlyFt6Cfdt7xjHN1buQFB/+3ucBYClZi9NmZv5vz+a1q05vFi4b0i2NlIDuEby7/iCPHjodSQ1w3oJxbLT/in62elB8mDwVVHcbzVa1N+r6NygrK0FCj4OnWWXesDyOvW43u1Z8gEWWePzCPlxpavwN3JqNUaatDPt6Oih+1hbX8PNXVjNh1kreXX+A/63eSa5UQ7GWwzL1NOp7no99y2xQ9VHPA3OcbDxYxz8X7WR7ST2Haj3c9cKrzP+hGMu+xaTPuwFb4cecs2QKOVIdffe+g6lO95xthXO5y/MvfIEg17+9nskvf8cnP5SApnIC+vgT9aDewH+xtQxFg0sG5xu2p/saQ78Dd77Et7bb+Rv/5orBGVzXt4GeUgWyFiT1m4cbL2zQi3PxPUgWB3mBA0yt/y9uOQNf3/HGJmeZd1EZCg1tKannuWV7+HRzKdOGdmPq0G5Rv9O1Z/ViUF4a/12zn1ve/57yPeuNz6oO7UTTNJbvruSs3pm4zZkANGxbQJdUK+9fP5z3rh/BmK71BE02vmEo6emZvGD5f1RWxa9yyHz4TY4ftN7n0Hfjf1lzSh4p+xdj9pTDxpc5f+PL/LPXzfyicDRX/3cdI8y7uTnwHABTbOC15zNPGsZgdjFwxU6qV2bweb+ZVByyIJPJ5fJiAP5k/h9r5KGYGnz8xvYRXnMGb6f9kmVlZk4zuTj3hDPpkeFAXbMQKqH+/H/i/PpPpC+8A9d5j5Gr1vO05TnDXvXtTLRRt2CzNt5ow7c9xjXd/8hV9m9I/e55/L3HkFu9i4f9+gCjuq5nEuw2goaz7yV94R9wfvMgmsWBd/AMTO5ynnDOxlyygYbZ+TxQdiG7gl3p2SWTZd2e4dPyHP4enM7AULzUXKHHrXue/xt+WHUI/wkXMHT/m8hu/YacZlvN22UFDMpN49Z+5SjfvcgUyxesCX7Nom+G8YfMHM4/uSezKodw3d4/cZq8FfatpmZvb1JXPMYoUw0Acv1+THXFqOm99NfVO7Ft/wDV2R3vSdPA4jB6cI7K7/nblP4ENBPPLdvDafsWghesPU5HHTOdmvqfYylZi2Pjy2SFercAct1egrnD9BcmK8G8Yfj6XYQ5/2SkNS8T6DYSNSUH5/K/krr6H0gBD6rFiXfgz3GueIz3flrNiWllKGQj1+xGSclFs+oekJrZF/cZt5Cy4UWCGX0xVxcyRW4MUQUlG+fIW2DXlqj7MXXlYwD4+l2Ide9CLIfW4D/hZ8gVW7Dt/IRg7mmACTWlK8H8M5matoU03BRZurN8ZxXF8knUaSmkS3qP/6RF1yDX6nmBXr368Xr5lfym5p9cv+lyKpw3Y7eMpm/DOsySHnY58eBcpvQbTkFGGRZJ4d30G7jU9Q63+37LuNTdXO75kMX/dwMP1U3BnNWLbul2/rFoFydKB/idDYo1veRTOXU68he/In3e9dj2fc3D+efQK+8iZn/vZv3OvTyf+grvmJeyedk7ODM1zFXbsRXNB2CXZSD9tGIyPr2GhnPux7n4Hs7317O4fCB/0b5is7kPH3z/ey7q7sWBD0WT6F+3gi3L3+HDHSdyTp7K0FW34Tr3L6gZJ2Cq1wWlZvK7pC5/CKecxtTSZXjqnkDKzIVD4D7tl6RsegXHhpfwDppGyroXkF0HkSbOprD8AKesvBMGTSSQ3sfwkM5gK/PqD4F2Est3VyGhMWdqLt16n9hslHJ2ipXXrzodb0Dhi61lnL1hH6rHgSnoQSv5nq+2jeJgnY9fjsgj7aDukZ3p/Za3MhvoUTIRb9aVmOv24RlyDQPOnYm7cisZ/xvPmE1/pKLff+jarS8djaTFMRW9dOlSHnnkEVRV5bLLLuNXv/pV1OeapvHII4+wZMkS7HY7jz/+OIMHD251n4GAQk1N+6bRzarfBB/fhuSvB7MdJaMv1uIlxufrsy6kS90Weqn7UR1dqZn2KebSdTg2v4XlwLcEcoexO204XQrfIUeqjdq3P60P1vrGeeg9GQOQrQ6s5ZuM99zDfo3vxAmkrn4SuXYPVVd/i2PdCzhXPAqAastA8rtY3vM39OmaTl7VKmx79Z6lJpmovXQ2GZ/MQFL1nom3/0TqL3gKkFiw9nuy6zYz4sxzUbIH6InIun2kfX0XlkPf4T7jNzh+eAP8LtYET+RE6QBdpHo0yYSakoPcoIeL9maMxDby19gy8khd9QTmknVU3rgZJN15lCu24Fz+V5TM/jg2v0m1tTu23AGk7NevY2X+T0krX4dVaYwRa2Y7UtBLXcFTOL/9G6aI+vfGbVKomfwuqrMbme9fgqmhFElTUe1ZeE69htQ1TxPMGoi5ege1F72Cv5/eW0v/4teYy7+n6upvo/YneatxLr6XYP4ZKM7uZHypV2wEug6m5opGgcjMTIm6n1JW/p2Utc8hoeHvNQb36b8h8+MrdRuRcI/8E9Y9X6KZ7dT+/IPmN5mmkfHBZKyla/F3H4WlZB01U+eQ/tkNmDyVSGoA1ZKKpPiN37F6yhyc3zyI5Kuj/mdP68laNToe7R0wGUvRV+zwdWGy/yF8WLn1p325rOgv5JUt4c/SbTxheRE1vRdqai71Y/5OnS2fBcuXcs62B+kvHcB1zVIc3/0/pK0f8WlgBJeZl+K3ZhIYegOp3z3FSO9zlJLF+EG5XD28J13mTOHk4FbKHP3xXLUAqxSg/ut/0n/3q5hUP1N8D7BOG8h3vz+bzLlXYDm0mkD+cOTaIkyeCnzWbMp9JnKp5l3lPC6Tl2KTAmxiAGbZwp+81/KXywoYxB7SP7sRk78O1eLEHwxi17y4NRspko8SLYv8UML2s+6/Z+ChD+mv7WO1djInO72kNezBc8ovcJ3/BClrniF11ROU/6oQLHqozLHxZZzLH9Cv48Ap1J//BJkfTcVSttG4vp7BM3Cd9ziZmSnUHdiDak1Drj9AxtwrCfQ426jQqpOz2KT1B7ONn/iX0zDyLtzDbwdVAdWPc/mDKFkD8Ay5FkxmJL+LLv83FO/JV+jPH/CR8hNey7mHf19gp/t7P2O+42LGefSEsWrPxjX6b6TPv4X60Y/gHXItADvnP88JO19mz6l3MGD01S3ev62Rk5PW4mdxEwJFURg/fjyvvvoqeXl5TJs2jaeeeor+/fsb2yxZsoQ333yTl156iY0bN/LII4/w3nvvtbLXHycEzS6apmH//lUCPc4mdeU/sBXNx9/jHALdR+EbcClKVv+IbVWjMVyzYzeB4nWMym4g1a/3jj2n34zJVYKamofkq0VNzQMkrHsXopkd2Hd8hH37+8buwjcugFy+GdveBZjLfyDQfSSeoTc22uwvwrvta9T0XnpvsXI7cu0elIwTULJPghillpFI3mrSv/wN1v3LUdJ7Uzvhdda4c1m28Xt+m7uFdLUWc/VOfH3H6T21HXOQXQeN79f/9CG8p93QfMdqEFvhHGw7P0Wu3om/9/l4ht1Eeu9B1NS4kRrKMPlqkWv3YN/2Hr6+4/ANuoyU1U+S+t2/8A6cgnvE79GQSFt6P+ayjaBpegOoadRMnQMBDynr/42tSI+f1p//T1K+exLZdYhA11NRU/Ow7V2oC+L4f7d6HcxlmzCXrsd30hSjJx/znkCvUErZMAvfCeMI5p+JXLEFuaEE244PsRfOBaDhzNtwj7o79sEUH7Ydc/H3HYtmSQHZhqV4GSbXQTR7Fkp6H9SUHByb38L+/atUT1+IuWwjGfOuQ1KDaCYLVVctQXYdRDOnYN/8X2yFcwnmDuX9Pn9hRamVA7VeHrzoJLqbqthfvIuGrqczMF1Fs6SCKXpyO6l2H13eKUAzp2DyVlHX9xL+L+VGzq94k6Glupiptkz+cepnrNtfw58K+tMnOwXJU4nz20ewb3sX16h7MPlqSVnfeJ3/efJHVJmy+cN5J4Lix1r0Ff4+BaCB9eAKnEv/TLUnyA2u3zDkjDH0835P7t5PeEK7moNuiZkXnsTFp+QZ19xavAx/z3OZN+9dzAdX86IykedHQ+m3r1NvziYtWEXa5a/SLctJw7Kn6XvwY2z1e9EkGUxm3MNvx7HxFZAkKm/YGHUNTDV7MHmrCOadoT8zmoa5bCOW/cvRHF3wDroMTObYDWvQw5ZVn7N1xxZ6uTYwgGIGmg6gpOYjN5SgpPeBoBeTpwJJ03MhmmQi0PMneohRDVL98w9J/exXWH16TqBu0C+wuw9g3beE/Zd+ROr290nxHIxKDtdMfItA7zG0RqcXgvXr1/Pcc8/xyiuvADBr1iwAfv3rXxvbzJw5k7POOotLLrkEgPHjx/Pmm2+Sm5vbfIchOlQIIlEVTPXFqBkntGvfh0XTkKt3ItftAzQC+cPR7JmH/dqR/NCtHdvkOojqyAazo/Vtgx5se+YDJlB9+AZOPazYHJG9ahBzxRaCOUOi9itX7SBt8d0Eu5yCZ8h1ulcT/qx6F5b9y/EOuhx74RwcG15Es6QgBRqQlAANo+7BN2Bim208Insj0TQs+79BCjTgP+ECMHVAZFXTjOtgKV5K2uJ78Qy+Cs8Zv/3x9kZgPrSGlPX/xlKyjrpxzxHoeS4Aqd8+TMr6/+A7cQJ1F85q/sWAmy6vj8Dk0z1g3wnj8A6ahm3nJ9SPe6H1e0PxkZnp5JsdlZzYNdUYTV/Z4KeiwW+UbDal3OXjH4t2MSjXyQ2jevPhpkPM31bG+f27csUZPaL2b9/6HsGcwTgX34OlYjOayYL3lOm4xjx6xNcIDn99a9wBJAkyTR40Scbx/auYy39A0hRUexZK9kmo9kzMpRuw7ZmPkt4LNSWX+nHPYXKVsPqAm5F7XyBr1wdImoImmai8YZPeFmgaKd/9S+9EaireU6Yf9h7r9ELwxRdfsGzZMh555BEA5syZw6ZNm5g5c6axza9//Wtuuukmhg8fDsC1117LnXfeyZAhQ1rcr6qqKO1c3lGWTSidcLH41jjWbBb2xpe42Fu1C+xZkJId+/OKHUhVu5F2L0IddQtk9mnzro/q9fU3gGwF2XL4bVvgqNkb9IK7Evwu6HpSu3dzJPZamiyMFEncksWx9KVpUqUt2zRFUbT4eASdlGPNZmFvfImLvaZu4Af8LezX3BNye0LuaP31ERz/6F5fCQiE/rWPo2tvFpizjuh6NqWjPIK4lY/m5+dTUtJYylVaWtos5NN0m5KSklbDQgKBQCDoeOImBEOGDKGoqIji4mL8fj/z5s2joKAgapuCggLmzJmDpmls2LCBtLQ0IQQCgUBwlIlbaMhsNjNz5kxuvPFGFEVh6tSpDBgwgNmzZwMwffp0xowZw5IlSxg7diwOh4NHH21fgkcgEAgE7Seu4wjiQdyqhjopx5rNwt74IuyNL8ezvQnJEQgEAoHg2EAIgUAgECQ5QggEAoEgyRFCIBAIBEnOMZcsFggEAkHHIjwCgUAgSHKEEAgEAkGSI4RAIBAIkhwhBAKBQJDkCCEQCASCJEcIgUAgECQ5QggEAoEgyUkaIVi6dCnjx49n7NixvPjii4k2JyYFBQVMnDiRSZMmMWXKFABqamq4/vrrGTduHNdffz21tbUJs+/ee+/l7LPPNpYWPZx9s2bNYuzYsYwfP55ly5Z1CnufffZZfvrTnzJp0iQmTZrEkiVLOo29hw4d4uqrr+aiiy5iwoQJvP7660DnvcYt2dtZr7HP52PatGlceumlTJgwgWeeeQbovNe3JXvjcn21JCAYDGoXXHCBtm/fPs3n82kTJ07UCgsLE21WM84//3ytsrIy6r2///3v2qxZszRN07RZs2ZpTzzxRCJM0zRN01avXq398MMP2oQJE4z3WrKvsLBQmzhxoubz+bR9+/ZpF1xwgRYMBhNu7zPPPKO9/PLLzbbtDPaWlpZqP/zwg6ZpmlZfX6+NGzdOKyws7LTXuCV7O+s1VlVVc7lcmqZpmt/v16ZNm6atX7++017fluyNx/VNCo9g06ZN9OnTh169emG1WpkwYQILFy5MtFltYuHChUyePBmAyZMns2DBgoTZMmLECDIyMqLea8m+hQsXMmHCBKxWK7169aJPnz5s2rQp4fa2RGewNzc3l8GDBwPgdDrp168fpaWlnfYat2RvSyTaXkmSSE1NBSAYDBIMBpEkqdNe35bsbYkfY29SCEFpaSn5+fnG67y8vFZv2ETyy1/+kilTpvC///0PgMrKSmPVttzcXKqqqhJpXjNasq8zX/O33nqLiRMncu+99xphgM5m7/79+9m6dStDhw49Jq5xpL3Qea+xoihMmjSJc845h3POOafTX99Y9kLHX9+kEAItxnRKrSlropg9ezYfffQRL730Em+99Rbfffddok1qN531mk+fPp2vvvqKuXPnkpuby+OPPw50LnsbGhq4/fbbue+++3A6nS1u11lsbmpvZ77Gsiwzd+5clixZwqZNm9ixY0eL23ZWe+NxfZNCCPLz8ykpKTFel5aWdsq1kfPy8gDo0qULY8eOZdOmTXTp0oWysjIAysrKyM7OTqSJzWjJvs56zbt27Yosy5hMJi677DK+//57oPPYGwgEuP3225k4cSLjxo0DOvc1jmVvZ7/GAOnp6YwcOZJly5Z16usby954XN+kEIIhQ4ZQVFREcXExfr+fefPmUVBQkGizonC73bhcLuPvb775hgEDBlBQUMCcOXMAmDNnDhdccEECrWxOS/YVFBQwb948/H4/xcXFFBUVcdpppyXQUp3wAw+wYMECBgwYAHQOezVN4/7776dfv35cf/31xvud9Rq3ZG9nvcZVVVXU1dUB4PV6+fbbb+nXr1+nvb4t2RuP6xu3xes7E2azmZkzZ3LjjTeiKApTp041Ll5nobKykltuuQXQ44KXXHIJo0ePZsiQIfz+97/n/fffp1u3bjz99NMJs/GOO+5g9erVVFdXM3r0aG677TZ+9atfxbRvwIABXHTRRVx88cXIsszMmTORZTnh9q5evZpt27YB0KNHDx566KFOY+/atWuZO3cuAwcOZNKkScY5dNZr3JK9n376aae8xmVlZdxzzz0oioKmaVx44YWcf/75DBs2rFNe35bs/dOf/tTh11esRyAQCARJTlKEhgQCgUDQMkIIBAKBIMkRQiAQCARJjhACgUAgSHKEEAgEAkGSkxTlowJBe/j3v//Np59+islkwmQy8dBDD7F+/XquuOIKHA5Hos0TCDoMIQQCQQzWr1/P4sWL+eijj7BarVRVVREIBHjjjTe49NJLhRAIjiuEEAgEMSgvLycrKwur1QpAdnY2b7zxBmVlZVx77bVkZmby5ptvsnz5cp599ln8fj+9evXiscceIzU1lYKCAi666CJWrVoFwJNPPkmfPn34/PPPef755zGZTKSlpfHWW28l8jQFAkAMKBMIYtLQ0MAvfvELvF4vZ599NhdffDFnnXUWBQUFvP/++2RnZ1NVVcVtt93GSy+9REpKCi+++CJ+v59bb72VgoICLrvsMn7zm98wZ84cPv/8c2bNmsXEiRN5+eWXycvLo66ujvT09ESfqkAgPAKBIBapqal8+OGHrFmzhlWrVvGHP/yBP/7xj1HbbNy4kZ07dzJ9+nRAn4Bt2LBhxufhldEmTJjAY489BsDpp5/OPffcw0UXXcTYsWOPzskIBIdBCIFA0AKyLDNy5EhGjhzJwIEDjYnJwmiaxrnnnstTTz3V5n0+9NBDbNy4kcWLFzN58mTmzJlDVlZWB1suEBwZonxUIIjB7t27KSoqMl5v3bqV7t27k5qaSkNDAwDDhg1j3bp17N27FwCPx8OePXuM73z++ecAfPbZZ5x++ukA7Nu3j6FDh/K73/2OrKysqGmDBYJEITwCgSAGbrebhx9+mLq6OmRZpk+fPjz00EPMmzePm266iZycHN58800ee+wx7rjjDvx+PwC///3v6du3LwB+v5/LLrsMVVUNr+GJJ55g7969aJrGqFGjGDRoUMLOUSAII5LFAkEciEwqCwSdHREaEggEgiRHeAQCgUCQ5AiPQCAQCJIcIQQCgUCQ5AghEAgEgiRHCIFAIBAkOUIIBAKBIMn5//IqeDP1eYE8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Plotting\n",
    "\n",
    "# %%\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.6530\n",
      "MSE for Dimension 2: 0.5273\n",
      "MSE for Dimension 3: 0.3047\n",
      "MSE for Dimension 4: 0.5143\n",
      "MSE for Dimension 5: 0.5073\n",
      "MSE for Dimension 6: 0.4384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### MSE (For Regression)\n",
    "\n",
    "# %%\n",
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.36      0.45      6826\n",
      "         1.0       0.17      0.17      0.17      2121\n",
      "         2.0       0.08      0.19      0.11      1717\n",
      "         3.0       0.11      0.19      0.14       408\n",
      "\n",
      "    accuracy                           0.29     11072\n",
      "   macro avg       0.24      0.23      0.22     11072\n",
      "weighted avg       0.42      0.29      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.47      0.55      8096\n",
      "         1.0       0.00      0.00      0.00       469\n",
      "         2.0       0.07      0.11      0.08       790\n",
      "         3.0       0.07      0.11      0.09      1717\n",
      "\n",
      "    accuracy                           0.37     11072\n",
      "   macro avg       0.20      0.17      0.18     11072\n",
      "weighted avg       0.50      0.37      0.42     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.36      0.35      2716\n",
      "         1.0       0.39      0.26      0.31      4925\n",
      "         2.0       0.05      0.05      0.05      1293\n",
      "         3.0       0.20      0.35      0.25      2138\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.25      0.25      0.24     11072\n",
      "weighted avg       0.30      0.27      0.28     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.35      0.41      4859\n",
      "         1.0       0.09      0.09      0.09      1442\n",
      "         2.0       0.02      0.00      0.01       561\n",
      "         3.0       0.38      0.55      0.45      4210\n",
      "\n",
      "    accuracy                           0.38     11072\n",
      "   macro avg       0.24      0.25      0.24     11072\n",
      "weighted avg       0.37      0.38      0.36     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Classification Metrics\n",
    "\n",
    "# %%\n",
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

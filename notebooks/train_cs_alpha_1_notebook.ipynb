{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())*5.0/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:42:34.364 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=0)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:00:48.761 | INFO     | __main__:<module>:9 - Test 0.3233 +- 0.0533\n",
      "2022-04-28 12:00:48.762 | INFO     | __main__:<module>:10 - Train 0.1221 +- 0.0082\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 1.4163\n",
      "MSE for Dimension 2: 1.4395\n",
      "MSE for Dimension 3: 1.0358\n",
      "MSE for Dimension 4: 0.6474\n",
      "MSE for Dimension 5: 0.8471\n",
      "MSE for Dimension 6: 1.1031\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.11      0.18      6826\n",
      "         1.0       0.14      0.29      0.19      2121\n",
      "         2.0       0.05      0.02      0.02      1717\n",
      "         3.0       0.04      0.51      0.08       408\n",
      "\n",
      "    accuracy                           0.14     11072\n",
      "   macro avg       0.19      0.23      0.12     11072\n",
      "weighted avg       0.35      0.14      0.15     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.09      0.16      8096\n",
      "         1.0       0.04      0.45      0.07       469\n",
      "         2.0       0.02      0.04      0.02       790\n",
      "         3.0       0.21      0.34      0.26      1717\n",
      "\n",
      "    accuracy                           0.14     11072\n",
      "   macro avg       0.22      0.23      0.13     11072\n",
      "weighted avg       0.49      0.14      0.16     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.39      0.31      2716\n",
      "         1.0       0.46      0.19      0.27      4925\n",
      "         2.0       0.08      0.19      0.12      1293\n",
      "         3.0       0.18      0.15      0.17      2138\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.24      0.23      0.22     11072\n",
      "weighted avg       0.31      0.23      0.24     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.22      0.30      4859\n",
      "         1.0       0.12      0.44      0.19      1442\n",
      "         2.0       0.05      0.30      0.08       561\n",
      "         3.0       0.46      0.01      0.01      4210\n",
      "\n",
      "    accuracy                           0.17     11072\n",
      "   macro avg       0.27      0.24      0.15     11072\n",
      "weighted avg       0.39      0.17      0.17     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

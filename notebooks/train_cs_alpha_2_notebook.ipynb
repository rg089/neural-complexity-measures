{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:02:24.988 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=1.0)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:24:34.837 | INFO     | __main__:<module>:9 - Test 0.2958 +- 0.0388\n",
      "2022-04-28 12:24:34.838 | INFO     | __main__:<module>:10 - Train 0.1394 +- 0.0134\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.6776\n",
      "MSE for Dimension 2: 0.8195\n",
      "MSE for Dimension 3: 1.1310\n",
      "MSE for Dimension 4: 0.8889\n",
      "MSE for Dimension 5: 1.3460\n",
      "MSE for Dimension 6: 1.0543\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.56      0.59      6826\n",
      "         1.0       0.15      0.16      0.15      2121\n",
      "         2.0       0.30      0.21      0.24      1717\n",
      "         3.0       0.01      0.04      0.02       408\n",
      "\n",
      "    accuracy                           0.41     11072\n",
      "   macro avg       0.27      0.24      0.25     11072\n",
      "weighted avg       0.45      0.41      0.43     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.67      0.73      8096\n",
      "         1.0       0.01      0.01      0.01       469\n",
      "         2.0       0.16      0.25      0.20       790\n",
      "         3.0       0.19      0.29      0.23      1717\n",
      "\n",
      "    accuracy                           0.55     11072\n",
      "   macro avg       0.29      0.30      0.29     11072\n",
      "weighted avg       0.63      0.55      0.58     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.11      0.13      2716\n",
      "         1.0       0.46      0.64      0.53      4925\n",
      "         2.0       0.20      0.26      0.23      1293\n",
      "         3.0       0.23      0.09      0.13      2138\n",
      "\n",
      "    accuracy                           0.36     11072\n",
      "   macro avg       0.26      0.27      0.25     11072\n",
      "weighted avg       0.31      0.36      0.32     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.49      0.48      4859\n",
      "         1.0       0.14      0.27      0.19      1442\n",
      "         2.0       0.04      0.05      0.05       561\n",
      "         3.0       0.38      0.23      0.29      4210\n",
      "\n",
      "    accuracy                           0.34     11072\n",
      "   macro avg       0.26      0.26      0.25     11072\n",
      "weighted avg       0.37      0.34      0.34     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

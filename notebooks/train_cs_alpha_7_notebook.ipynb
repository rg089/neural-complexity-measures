{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:11:56.342 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_test_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=1.0)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:40:24.941 | INFO     | __main__:<module>:9 - Test 0.2560 +- 0.0272\n",
      "2022-04-28 14:40:24.942 | INFO     | __main__:<module>:10 - Train 0.1309 +- 0.0118\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.8646\n",
      "MSE for Dimension 2: 0.7508\n",
      "MSE for Dimension 3: 0.7388\n",
      "MSE for Dimension 4: 0.8425\n",
      "MSE for Dimension 5: 0.4949\n",
      "MSE for Dimension 6: 0.5985\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.32      0.41      6826\n",
      "         1.0       0.18      0.25      0.21      2121\n",
      "         2.0       0.16      0.32      0.21      1717\n",
      "         3.0       0.02      0.06      0.03       408\n",
      "\n",
      "    accuracy                           0.30     11072\n",
      "   macro avg       0.24      0.24      0.22     11072\n",
      "weighted avg       0.43      0.30      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.39      0.50      8096\n",
      "         1.0       0.06      0.03      0.04       469\n",
      "         2.0       0.02      0.09      0.04       790\n",
      "         3.0       0.16      0.27      0.20      1717\n",
      "\n",
      "    accuracy                           0.34     11072\n",
      "   macro avg       0.23      0.20      0.19     11072\n",
      "weighted avg       0.52      0.34      0.40     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.18      0.13      0.15      2716\n",
      "         1.0       0.38      0.47      0.42      4925\n",
      "         2.0       0.09      0.10      0.10      1293\n",
      "         3.0       0.20      0.15      0.17      2138\n",
      "\n",
      "    accuracy                           0.28     11072\n",
      "   macro avg       0.21      0.21      0.21     11072\n",
      "weighted avg       0.26      0.28      0.27     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.21      0.27      4859\n",
      "         1.0       0.16      0.31      0.21      1442\n",
      "         2.0       0.03      0.12      0.05       561\n",
      "         3.0       0.43      0.33      0.38      4210\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.25      0.25      0.23     11072\n",
      "weighted avg       0.35      0.27      0.29     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

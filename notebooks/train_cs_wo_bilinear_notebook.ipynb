{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Importing the libraries\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_wo_bilinear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Reading the Dataset\n",
    "\n",
    "# %%\n",
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]\n",
    "\n",
    "# %%\n",
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Combining the dataset\n",
    "\n",
    "# %%\n",
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "# %%\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Adding an accumulator to keep track of the metrics\n",
    "\n",
    "# %%\n",
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Args\n",
    "\n",
    "# %%\n",
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 200\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 6\n",
    "dec_depth = 6\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Memory Bank\n",
    "\n",
    "# %%\n",
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))\n",
    "\n",
    "# %% [markdown]\n",
    "# # Modelling\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Utility Functions\n",
    "\n",
    "# %%\n",
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)\n",
    "\n",
    "# %%\n",
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Multi-Headed Attention (for NC Model)\n",
    "\n",
    "# %%\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs\n",
    "\n",
    "# %%\n",
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Encoder with Attention\n",
    "\n",
    "# %%\n",
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, xtrain_dim+y_train_dim+train_pred_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        # y_tr = convert_y_ohe(y_tr)\n",
    "        # tr_loss = inputs[\"tr_loss\"]\n",
    "        # bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        # bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(inputs[\"tr_xyp\"])\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Neural Complexity Model\n",
    "\n",
    "# %%\n",
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task Learner\n",
    "\n",
    "# %%\n",
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training the Task Learner\n",
    "\n",
    "# %%\n",
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())*5/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += torch.max(torch.tensor((nc_regularization, -0.1))) *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Defining the model\n",
    "\n",
    "# %%\n",
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Adding a logger\n",
    "\n",
    "# %%\n",
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training\n",
    "\n",
    "# %%\n",
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Testing\n",
    "\n",
    "# %%\n",
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    \n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Population\n",
    "\n",
    "# %%\n",
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Running the task learner for a few steps initially\n",
    "\n",
    "# %%\n",
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Main Training Loop\n",
    "\n",
    "# %%\n",
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:30:55.808 | INFO     | __main__:<module>:66 - Dataset loading took 0.00 seconds\n",
      "2022-04-28 11:31:41.498 | INFO     | __main__:<module>:176 - Test 0.2676 +- 0.0544\n",
      "2022-04-28 11:31:41.499 | INFO     | __main__:<module>:177 - Train 0.0521 +- 0.0041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Evaluation using trained NC Model\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Loading the meta_test dataset\n",
    "\n",
    "# %%\n",
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)\n",
    "\n",
    "# %%\n",
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_wo_bilinear.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training the Task Learner with NC \n",
    "\n",
    "# %%\n",
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Testing\n",
    "\n",
    "# %%\n",
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels\n",
    "\n",
    "# %%\n",
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n",
    "\n",
    "\n",
    "# %%\n",
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Plotting\n",
    "%matplotlib inline\n",
    "# %%\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.7797\n",
      "MSE for Dimension 2: 0.9517\n",
      "MSE for Dimension 3: 0.8449\n",
      "MSE for Dimension 4: 1.3765\n",
      "MSE for Dimension 5: 0.9429\n",
      "MSE for Dimension 6: 1.0798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### MSE (For Regression)\n",
    "\n",
    "# %%\n",
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.33      0.42      6826\n",
      "         1.0       0.27      0.51      0.35      2121\n",
      "         2.0       0.11      0.14      0.12      1717\n",
      "         3.0       0.01      0.03      0.02       408\n",
      "\n",
      "    accuracy                           0.32     11072\n",
      "   macro avg       0.24      0.25      0.23     11072\n",
      "weighted avg       0.43      0.32      0.35     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.42      0.55      8096\n",
      "         1.0       0.06      0.29      0.10       469\n",
      "         2.0       0.05      0.15      0.08       790\n",
      "         3.0       0.27      0.35      0.31      1717\n",
      "\n",
      "    accuracy                           0.39     11072\n",
      "   macro avg       0.29      0.30      0.26     11072\n",
      "weighted avg       0.62      0.39      0.46     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.04      0.07      2716\n",
      "         1.0       0.36      0.31      0.33      4925\n",
      "         2.0       0.14      0.30      0.19      1293\n",
      "         3.0       0.15      0.25      0.19      2138\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.22      0.22      0.19     11072\n",
      "weighted avg       0.27      0.23      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.36      0.43      4859\n",
      "         1.0       0.14      0.26      0.18      1442\n",
      "         2.0       0.02      0.03      0.03       561\n",
      "         3.0       0.42      0.47      0.45      4210\n",
      "\n",
      "    accuracy                           0.37     11072\n",
      "   macro avg       0.29      0.28      0.27     11072\n",
      "weighted avg       0.42      0.37      0.38     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Classification Metrics\n",
    "\n",
    "# %%\n",
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmeta.toy import Sinusoid\n",
    "from torchmeta.transforms import ClassSplitter\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "\n",
    "class ToTensor1D(object):\n",
    "    \"\"\"Convert a `numpy.ndarray` to tensor. Unlike `ToTensor` from torchvision,\n",
    "    this converts numpy arrays regardless of the number of dimensions.\n",
    "\n",
    "    Converts automatically the array to `float32`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, array):\n",
    "        return torch.tensor(array.astype(\"float32\"))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "def get_sine_loader(batch_size, num_steps, shots=10, test_shots=15):\n",
    "    dataset_transform = ClassSplitter(\n",
    "        shuffle=True, num_train_per_class=shots, num_test_per_class=test_shots\n",
    "    )\n",
    "    transform = ToTensor1D()\n",
    "    dataset = Sinusoid(\n",
    "        shots + test_shots,\n",
    "        num_tasks=batch_size * num_steps,\n",
    "        transform=transform,\n",
    "        target_transform=transform,\n",
    "        dataset_transform=dataset_transform,\n",
    "    )\n",
    "    loader = BatchMetaDataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(task, batch_size, num_steps):\n",
    "    if task == \"sine\":\n",
    "        loader = get_sine_loader(batch_size=batch_size, num_steps=num_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"task={task} is not implemented\")\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_task(saved, task, batch_size, num_steps):\n",
    "    if not saved:\n",
    "        return get_loader(task, batch_size, num_steps)\n",
    "\n",
    "    os.makedirs(\"data/saved\", exist_ok=True)\n",
    "    filename = f\"data/saved/{task}_{batch_size}_{num_steps}.pkl\"\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as handle:\n",
    "            tasks = pickle.load(handle)\n",
    "    else:\n",
    "        test_task_gen = get_loader(\n",
    "            task=task, batch_size=batch_size, num_steps=num_steps\n",
    "        )\n",
    "        tasks = [t for t in test_task_gen]\n",
    "        with open(filename, \"wb\") as handle:\n",
    "            pickle.dump(tasks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds):\n",
    "        super().__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mha = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.size(0)\n",
    "        query = self.S.repeat(batch_size, 1, 1)\n",
    "        return self.mha(query, X, X).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim):\n",
    "    if num_layers == 0:\n",
    "        return nn.Identity()\n",
    "    elif num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim =  hid_dim\n",
    "\n",
    "        self.mlp_v = fc_stack(enc_depth, 3, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, 2, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim,  num_heads)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        v = self.mlp_v(inputs[\"tr_xyp\"])\n",
    "        out = self.attn(q, k, v)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MeanPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        return x.mean(1)\n",
    "\n",
    "\n",
    "class NeuralComplexity1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bs =  batch_size\n",
    "        self.encoder = CrossAttEncoder()\n",
    "\n",
    "        if  pool == \"pma\":\n",
    "            self.pool = PMA(dim= hid_dim, num_heads= num_heads, num_seeds=1)\n",
    "        elif  pool == \"mean\":\n",
    "            self.pool = MeanPool()\n",
    "\n",
    "        self.decoder = fc_stack( dec_depth,  hid_dim,  hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.pool(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(batch_size, layers, hidden_size, activation, regularizer=None, task='regression'):\n",
    "    if activation == \"relu\":\n",
    "        activation = nn.ReLU\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation = nn.Sigmoid\n",
    "    elif activation == \"tanh\":\n",
    "        activation = nn.Tanh\n",
    "    elif activation == \"none\":\n",
    "        activation = nn.Identity\n",
    "    else:\n",
    "        raise ValueError(f\"activation={activation} not implemented!\")\n",
    "        \n",
    "    if task == 'regression':\n",
    "        return ParallelNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "        )\n",
    "    elif task == 'classification':\n",
    "        raise NotImplementedError\n",
    "        return ParallelNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "            output_activation=nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "class ParallelLinear(nn.Module):\n",
    "    def __init__(self, bs, input_size, output_size):\n",
    "        super().__init__()\n",
    "        fcs = [nn.Linear(input_size, output_size) for _ in range(bs)]\n",
    "        self.weight = Parameter(torch.stack([m.weight for m in fcs]))\n",
    "        self.bias = Parameter(torch.stack([m.bias for m in fcs]).unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.einsum(\"bnd,bmd->bnm\", x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "class RegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, batch_size, num_layers, init_dim, hidden_size, activation, num_outputs, regularizer=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(init_dim, hidden_size))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(activation())\n",
    "            self.layers.append(\n",
    "                nn.Linear(hidden_size, hidden_size)\n",
    "            )\n",
    "            if regularizer == \"dropout\":\n",
    "                self.layers.append(nn.Dropout())\n",
    "\n",
    "        self.layers.append(activation())\n",
    "        self.layers.append(nn.Linear(hidden_size, num_outputs))\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelNeuralNetwork(nn.Module):\n",
    "    \"\"\" Equivalent to running  batch_size neural networks in parallel. No weight sharing. \"\"\"\n",
    "\n",
    "    def __init__(self, bs, num_layers, hidden_size, activation, regularizer):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        modules = [ParallelLinear(bs, 1, hidden_size)]\n",
    "        for _ in range(num_layers - 1):\n",
    "            modules.append(activation())\n",
    "            modules.append(ParallelLinear(bs, hidden_size, hidden_size))\n",
    "            if regularizer == \"dropout\":\n",
    "                modules.append(nn.Dropout())\n",
    "            if regularizer == \"g_dropout\":\n",
    "                modules.append(GaussianDropout(alpha=1.0))\n",
    "            if regularizer == \"v_dropout\":\n",
    "                modules.append(VariationalDropout(alpha=1.0, dim=hidden_size))\n",
    "            if regularizer == \"alpha_dropout\":\n",
    "                modules.append(nn.AlphaDropout(p=0.5))\n",
    "            if regularizer == \"batchnorm\":\n",
    "                # Parallel batchnorm is equivalent to layernorm in this case\n",
    "                modules.append(nn.LayerNorm(hidden_size, elementwise_affine=False))\n",
    "        modules.append(activation())\n",
    "        modules.append(ParallelLinear(bs, hidden_size, 1))\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[0] != self.bs:\n",
    "            assert x.shape[0] == 1\n",
    "            x = x.repeat(self.bs, 1, 1)\n",
    "        return self.net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1(weight):\n",
    "        return weight.view(weight.shape[0], -1).abs().sum(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2(weight):\n",
    "        return weight.view(weight.shape[0], -1).pow(2).sum(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(weight, p=2, q=2):\n",
    "        return weight.norm(p=p, dim=2).norm(q, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def op_norm(weight, p=float(\"Inf\")):\n",
    "        _, S, _ = weight.svd()\n",
    "        return S.norm(p, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def orthogonal_loss(weight):\n",
    "        bs, n, _ = weight.shape\n",
    "        sym = torch.bmm(weight, weight.transpose(2, 1))\n",
    "        eyes = [torch.eye(n, device=\"cuda\") for _ in range(bs)]\n",
    "        sym -= torch.stack(eyes)\n",
    "        return sym.abs().sum()\n",
    "\n",
    "    def get_measure(self, name):\n",
    "        # https://github.com/bneyshabur/generalization-bounds/blob/master/measures.py\n",
    "        linears = [p for p in self.modules() if isinstance(p, ParallelLinear)]\n",
    "        ws = [p.weight for p in linears]\n",
    "        bs = [p.bias for p in linears]\n",
    "        ps = ws + bs\n",
    "\n",
    "        inf = float(\"Inf\")\n",
    "\n",
    "        if name == \"L1\":\n",
    "            return torch.stack([self.l1(p) for p in ps]).sum(0)\n",
    "        elif name == \"L2\":\n",
    "            return torch.stack([self.l2(p) for p in ps]).sum(0)\n",
    "        elif name == \"L_{1,inf}\":\n",
    "            return torch.stack([self.norm(w, p=1, q=inf) for w in ws]).prod(0)\n",
    "        elif name == \"Frobenius\":\n",
    "            return torch.stack([self.norm(w, p=2, q=2) for w in ws]).prod(0)\n",
    "        elif name == \"L_{3,1.5}\":\n",
    "            return torch.stack([self.norm(w, p=3, q=1.5) for w in ws]).prod(0)\n",
    "        elif name == \"Orthogonal\":\n",
    "            # https://arxiv.org/abs/1609.07093\n",
    "            return torch.stack([self.orthogonal_loss(w) for w in ws]).sum()\n",
    "        elif name == \"Spectral\":\n",
    "            return torch.stack([self.op_norm(w, p=inf) for w in ws]).prod(0)\n",
    "        elif name == \"L_1.5_op\":\n",
    "            return torch.stack([self.op_norm(w, p=1.5) for w in ws]).prod(0)\n",
    "        elif name == \"Trace\":\n",
    "            return torch.stack([self.op_norm(w, p=1) for w in ws]).prod(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Measure {name} is not implemented.\")\n",
    "\n",
    "    def get_measures(self):\n",
    "        measure_names = [\n",
    "            \"L1\",\n",
    "            \"L2\",\n",
    "            \"L_{1,inf}\",\n",
    "            \"Frobenius\",\n",
    "            \"L_{3,1.5}\",\n",
    "            # \"Spectral\",\n",
    "            # \"L_1.5_op\",\n",
    "            # \"Trace\",\n",
    "        ]\n",
    "        return {name: self.get_measure(name) for name in measure_names}\n",
    "\n",
    "\n",
    "class GaussianDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(GaussianDropout, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train():\n",
    "            epsilon = torch.randn(x.size()) * self.alpha + 1\n",
    "            return x * epsilon.cuda()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0, dim=None):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_alpha = alpha\n",
    "        log_alpha = (torch.ones(dim) * alpha).log()\n",
    "        self.log_alpha = nn.Parameter(log_alpha)\n",
    "\n",
    "    def kl(self):\n",
    "        c1 = 1.16145124\n",
    "        c2 = -1.50204118\n",
    "        c3 = 0.58629921\n",
    "\n",
    "        alpha = self.log_alpha.exp()\n",
    "\n",
    "        negative_kl = (\n",
    "            0.5 * self.log_alpha + c1 * alpha + c2 * alpha ** 2 + c3 * alpha ** 3\n",
    "        )\n",
    "\n",
    "        kl = -negative_kl\n",
    "\n",
    "        return kl.mean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise   e ~ N(1, alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            # N(0,1)\n",
    "            epsilon = torch.randn(x.size()).cuda()\n",
    "\n",
    "            # Clip alpha\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_alpha)\n",
    "            alpha = self.log_alpha.exp()\n",
    "\n",
    "            # N(1, alpha)\n",
    "            epsilon = epsilon * alpha\n",
    "\n",
    "            return x * epsilon\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '7'\n",
    "batch_size = 512\n",
    "task_batch_size = 64\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'sine'\n",
    "nc_regularize = True\n",
    "epochs = 1000\n",
    "train_steps = 500\n",
    "log_steps = 500\n",
    "test_steps = 250\n",
    "learn_freq = 10\n",
    "inner_lr = 0.01\n",
    "inner_steps = 16\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 40\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 3\n",
    "dec_depth = 2\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    \"\"\"\n",
    "    Memory bank class. Stores snapshots of task learners.\n",
    "    get_batch() returns a random minibatch of (snapshot, gap) for NC to train on.\n",
    "    \"\"\"\n",
    "\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap):\n",
    "        if not hasattr(self, \"te_xp\"):\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "\n",
    "            MEMORY_LIMIT = 1_000_000\n",
    "            if self.te_xp.shape[0] > MEMORY_LIMIT:\n",
    "                self.te_xp = self.te_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xp = self.tr_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xyp = self.tr_xyp[-MEMORY_LIMIT:]\n",
    "                self.gap = self.gap[-MEMORY_LIMIT:]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        assert N == self.tr_xp.shape[0]\n",
    "        assert N == self.tr_xyp.shape[0]\n",
    "        assert N == self.gap.shape[0]\n",
    "\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    h = get_learner(\n",
    "        batch_size=x_train.shape[0],\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        activation= learner_act,\n",
    "        task='regression'\n",
    "    ).to(device)\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    h_crit = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp}\n",
    "\n",
    "        h_loss = h_crit(preds_train.squeeze(), y_train.squeeze()).mean(-1).sum()\n",
    "        if  nc_regularize and global_step >  train_steps * 2:\n",
    "            model_preds = model(meta_batch)\n",
    "            # We sum NC outputs across tasks because h_loss is also summed.\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "        l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "            )\n",
    "    return h, meta_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    h = get_learner(\n",
    "        batch_size=x_train.shape[0],\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        activation= learner_act,\n",
    "        task='classification',\n",
    "    ).to(device)\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    h_crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp}\n",
    "\n",
    "        h_loss = h_crit(preds_train.squeeze(), y_train.squeeze()).mean(-1).sum()\n",
    "        if  nc_regularize and global_step >  train_steps * 2:\n",
    "            model_preds = model(meta_batch)\n",
    "            # We sum NC outputs across tasks because h_loss is also summed.\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "        l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "            )\n",
    "    return h, meta_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity1D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "\n",
    "test_tasks = get_task(\n",
    "    saved=False,\n",
    "    task= task,\n",
    "    batch_size= task_batch_size,\n",
    "    num_steps= test_steps,\n",
    ")\n",
    "# logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    for batch in test_tasks:\n",
    "        h, meta_batch = run_regression(batch, train=False)\n",
    "\n",
    "        x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "        x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        with torch.no_grad():\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "            l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "            gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "            model_preds = model(meta_batch)\n",
    "            loss = mse_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "        # test_accum.add_dict(\n",
    "        #     {\n",
    "        #         \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "        #         \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "        #         \"mae\": [mae.item()],\n",
    "        #         \"loss\": [loss.item()],\n",
    "        #         \"gap\": [gap.squeeze().detach().cpu()],\n",
    "        #         \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "    # all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    # all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    # R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    # mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    # mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    train_loader = get_task(\n",
    "        saved=False,\n",
    "        task= task,\n",
    "        batch_size= task_batch_size,\n",
    "        num_steps= train_steps,\n",
    "    )\n",
    "    for batch in train_loader:\n",
    "        global_step += 1\n",
    "        if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "            run_regression(batch)\n",
    "\n",
    "        meta_batch, gap = memory_bank.get_batch( batch_size)\n",
    "        model_preds = model(meta_batch)\n",
    "        loss = mse_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mae = mae_criterion(model_preds.squeeze(), gap.squeeze())\n",
    "\n",
    "        if timer() - global_timestamp >  time_budget:\n",
    "            logger.info(f\"Stopping at step {global_step}\")\n",
    "            quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 22:04:04.412 | INFO     | __main__:<cell line: 8>:8 - Populate time: 11.034644416999981\n",
      "2022-04-20 22:04:04.412 | INFO     | __main__:<cell line: 10>:11 - Epoch 0\n",
      "2022-04-20 22:04:04.413 | INFO     | __main__:<cell line: 10>:12 - Bank size: 102400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_original.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000011?line=11'>12</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000011?line=13'>14</a>\u001b[0m test_timestamp \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000011?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m test(epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000011?line=15'>16</a>\u001b[0m test_elapsed \u001b[39m=\u001b[39m timer() \u001b[39m-\u001b[39m test_timestamp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000011?line=17'>18</a>\u001b[0m train_timestamp \u001b[39m=\u001b[39m timer()\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_original.ipynb Cell 12'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000010?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(epoch):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000010?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_tasks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000010?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m run_regression(batch, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000010?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000010?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_original.ipynb Cell 10'\u001b[0m in \u001b[0;36mrun_regression\u001b[0;34m(batch, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000008?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000008?line=30'>31</a>\u001b[0m h_opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000008?line=31'>32</a>\u001b[0m h_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000008?line=32'>33</a>\u001b[0m h_opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_original.ipynb#ch0000008?line=34'>35</a>\u001b[0m l_test \u001b[39m=\u001b[39m mse_criterion(preds_test\u001b[39m.\u001b[39msqueeze(), y_test\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=245'>246</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=246'>247</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=247'>248</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=248'>249</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=252'>253</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=253'>254</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=254'>255</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=144'>145</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=146'>147</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=147'>148</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=148'>149</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory_bank = MemoryBank()\n",
    "populate_timestamp = timer()\n",
    "populate_loader = get_task(\n",
    "    saved=True, task= task, batch_size= task_batch_size, num_steps=100\n",
    ")\n",
    "for batch in populate_loader:\n",
    "    run_regression(batch)\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    test_timestamp = timer()\n",
    "    out = test(epoch)\n",
    "    test_elapsed = timer() - test_timestamp\n",
    "\n",
    "    train_timestamp = timer()\n",
    "    out = train()\n",
    "    train_elapsed = timer() - train_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6387572d3ba60263f2472b530ce49454bee9bd13656fbfcf29efcd586b712758"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

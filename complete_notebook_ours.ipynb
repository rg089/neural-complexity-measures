{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmeta.toy import Sinusoid\n",
    "from torchmeta.transforms import ClassSplitter\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "\n",
    "class ToTensor1D(object):\n",
    "    \"\"\"Convert a `numpy.ndarray` to tensor. Unlike `ToTensor` from torchvision,\n",
    "    this converts numpy arrays regardless of the number of dimensions.\n",
    "\n",
    "    Converts automatically the array to `float32`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, array):\n",
    "        return torch.tensor(array.astype(\"float32\"))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "def get_sine_loader(batch_size, num_steps, shots=10, test_shots=15):\n",
    "    dataset_transform = ClassSplitter(\n",
    "        shuffle=True, num_train_per_class=shots, num_test_per_class=test_shots\n",
    "    )\n",
    "    transform = ToTensor1D()\n",
    "    dataset = Sinusoid(\n",
    "        shots + test_shots,\n",
    "        num_tasks=batch_size * num_steps,\n",
    "        transform=transform,\n",
    "        target_transform=transform,\n",
    "        dataset_transform=dataset_transform,\n",
    "    )\n",
    "    loader = BatchMetaDataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d\n",
    "\n",
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds):\n",
    "        super().__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mha = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.size(0)\n",
    "        query = self.S.repeat(batch_size, 1, 1)\n",
    "        return self.mha(query, X, X).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim):\n",
    "    if num_layers == 0:\n",
    "        return nn.Identity()\n",
    "    elif num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "\n",
    "        self.mlp_v = fc_stack(enc_depth, 43, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, 33, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        v = self.mlp_v(inputs[\"tr_xyp\"])\n",
    "        out = self.attn(q, k, v)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MeanPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assert len(x.shape) == 3\n",
    "        return x.mean(0)\n",
    "\n",
    "\n",
    "class NeuralComplexity1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bs = batch_size\n",
    "        self.encoder = CrossAttEncoder()\n",
    "\n",
    "        if pool == \"pma\":\n",
    "            self.pool = PMA(dim=hid_dim, num_heads=num_heads, num_seeds=1)\n",
    "        elif pool == \"mean\":\n",
    "            self.pool = MeanPool()\n",
    "\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # print(\"input shape:\", inputs[\"te_xp\"].shape)\n",
    "        x = self.encoder(inputs)\n",
    "        # print(\"encoded shape:\", x.shape)\n",
    "        # x = self.pool(x)\n",
    "        # print(\"pool shape:\", x.shape)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(batch_size, layers, hidden_size, activation, regularizer=None, task='regression', init_dim=23, num_outputs=10):\n",
    "    if activation == \"relu\":\n",
    "        activation = nn.ReLU\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation = nn.Sigmoid\n",
    "    elif activation == \"tanh\":\n",
    "        activation = nn.Tanh\n",
    "    elif activation == \"none\":\n",
    "        activation = nn.Identity\n",
    "    else:\n",
    "        raise ValueError(f\"activation={activation} not implemented!\")\n",
    "        \n",
    "    if task == 'regression':\n",
    "        return RegressionNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "            init_dim=init_dim,\n",
    "            num_outputs=num_outputs,\n",
    "        )\n",
    "    elif task == 'classification':\n",
    "        raise NotImplementedError\n",
    "        return ParallelNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "            output_activation=nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ParallelLinear(nn.Module):\n",
    "    def __init__(self, bs, input_size, output_size):\n",
    "        super().__init__()\n",
    "        fcs = [nn.Linear(input_size, output_size) for _ in range(bs)]\n",
    "        self.weight = Parameter(torch.stack([m.weight for m in fcs]))\n",
    "        self.bias = Parameter(torch.stack([m.bias for m in fcs]).unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.einsum(\"bnd,bmd->bnm\", x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "class RegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, batch_size, num_layers, init_dim, hidden_size, activation, num_outputs, regularizer=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(init_dim, hidden_size))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(activation())\n",
    "            self.layers.append(\n",
    "                nn.Linear(hidden_size, hidden_size)\n",
    "            )\n",
    "            if regularizer == \"dropout\":\n",
    "                self.layers.append(nn.Dropout())\n",
    "\n",
    "        self.layers.append(activation())\n",
    "        self.layers.append(nn.Linear(hidden_size, num_outputs))\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # print(f\"In layer {i+1}, Shape={x.shape}\", end=\" \")\n",
    "            x = layer(x)\n",
    "            # print(f\"Ouput Shape = {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelNeuralNetwork(nn.Module):\n",
    "    \"\"\" Equivalent to running  batch_size neural networks in parallel. No weight sharing. \"\"\"\n",
    "\n",
    "    def __init__(self, bs, num_layers, hidden_size, activation, regularizer):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        modules = [ParallelLinear(bs, 1, hidden_size)]\n",
    "        for _ in range(num_layers - 1):\n",
    "            modules.append(activation())\n",
    "            modules.append(ParallelLinear(bs, hidden_size, hidden_size))\n",
    "            if regularizer == \"dropout\":\n",
    "                modules.append(nn.Dropout())\n",
    "            if regularizer == \"g_dropout\":\n",
    "                modules.append(GaussianDropout(alpha=1.0))\n",
    "            if regularizer == \"v_dropout\":\n",
    "                modules.append(VariationalDropout(alpha=1.0, dim=hidden_size))\n",
    "            if regularizer == \"alpha_dropout\":\n",
    "                modules.append(nn.AlphaDropout(p=0.5))\n",
    "            if regularizer == \"batchnorm\":\n",
    "                # Parallel batchnorm is equivalent to layernorm in this case\n",
    "                modules.append(nn.LayerNorm(hidden_size, elementwise_affine=False))\n",
    "        modules.append(activation())\n",
    "        modules.append(ParallelLinear(bs, hidden_size, 1))\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[0] != self.bs:\n",
    "            assert x.shape[0] == 1\n",
    "            x = x.repeat(self.bs, 1, 1)\n",
    "        return self.net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1(weight):\n",
    "        return weight.view(weight.shape[0], -1).abs().sum(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2(weight):\n",
    "        return weight.view(weight.shape[0], -1).pow(2).sum(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(weight, p=2, q=2):\n",
    "        return weight.norm(p=p, dim=2).norm(q, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def op_norm(weight, p=float(\"Inf\")):\n",
    "        _, S, _ = weight.svd()\n",
    "        return S.norm(p, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def orthogonal_loss(weight):\n",
    "        bs, n, _ = weight.shape\n",
    "        sym = torch.bmm(weight, weight.transpose(2, 1))\n",
    "        eyes = [torch.eye(n, device=\"cuda\") for _ in range(bs)]\n",
    "        sym -= torch.stack(eyes)\n",
    "        return sym.abs().sum()\n",
    "\n",
    "    def get_measure(self, name):\n",
    "        # https://github.com/bneyshabur/generalization-bounds/blob/master/measures.py\n",
    "        linears = [p for p in self.modules() if isinstance(p, ParallelLinear)]\n",
    "        ws = [p.weight for p in linears]\n",
    "        bs = [p.bias for p in linears]\n",
    "        ps = ws + bs\n",
    "\n",
    "        inf = float(\"Inf\")\n",
    "\n",
    "        if name == \"L1\":\n",
    "            return torch.stack([self.l1(p) for p in ps]).sum(0)\n",
    "        elif name == \"L2\":\n",
    "            return torch.stack([self.l2(p) for p in ps]).sum(0)\n",
    "        elif name == \"L_{1,inf}\":\n",
    "            return torch.stack([self.norm(w, p=1, q=inf) for w in ws]).prod(0)\n",
    "        elif name == \"Frobenius\":\n",
    "            return torch.stack([self.norm(w, p=2, q=2) for w in ws]).prod(0)\n",
    "        elif name == \"L_{3,1.5}\":\n",
    "            return torch.stack([self.norm(w, p=3, q=1.5) for w in ws]).prod(0)\n",
    "        elif name == \"Orthogonal\":\n",
    "            # https://arxiv.org/abs/1609.07093\n",
    "            return torch.stack([self.orthogonal_loss(w) for w in ws]).sum()\n",
    "        elif name == \"Spectral\":\n",
    "            return torch.stack([self.op_norm(w, p=inf) for w in ws]).prod(0)\n",
    "        elif name == \"L_1.5_op\":\n",
    "            return torch.stack([self.op_norm(w, p=1.5) for w in ws]).prod(0)\n",
    "        elif name == \"Trace\":\n",
    "            return torch.stack([self.op_norm(w, p=1) for w in ws]).prod(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Measure {name} is not implemented.\")\n",
    "\n",
    "    def get_measures(self):\n",
    "        measure_names = [\n",
    "            \"L1\",\n",
    "            \"L2\",\n",
    "            \"L_{1,inf}\",\n",
    "            \"Frobenius\",\n",
    "            \"L_{3,1.5}\",\n",
    "            # \"Spectral\",\n",
    "            # \"L_1.5_op\",\n",
    "            # \"Trace\",\n",
    "        ]\n",
    "        return {name: self.get_measure(name) for name in measure_names}\n",
    "\n",
    "\n",
    "class GaussianDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(GaussianDropout, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train():\n",
    "            epsilon = torch.randn(x.size()) * self.alpha + 1\n",
    "            return x * epsilon.cuda()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0, dim=None):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_alpha = alpha\n",
    "        log_alpha = (torch.ones(dim) * alpha).log()\n",
    "        self.log_alpha = nn.Parameter(log_alpha)\n",
    "\n",
    "    def kl(self):\n",
    "        c1 = 1.16145124\n",
    "        c2 = -1.50204118\n",
    "        c3 = 0.58629921\n",
    "\n",
    "        alpha = self.log_alpha.exp()\n",
    "\n",
    "        negative_kl = (\n",
    "            0.5 * self.log_alpha + c1 * alpha + c2 * alpha ** 2 + c3 * alpha ** 3\n",
    "        )\n",
    "\n",
    "        kl = -negative_kl\n",
    "\n",
    "        return kl.mean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise   e ~ N(1, alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            # N(0,1)\n",
    "            epsilon = torch.randn(x.size()).cuda()\n",
    "\n",
    "            # Clip alpha\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_alpha)\n",
    "            alpha = self.log_alpha.exp()\n",
    "\n",
    "            # N(1, alpha)\n",
    "            epsilon = epsilon * alpha\n",
    "\n",
    "            return x * epsilon\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '7'\n",
    "batch_size = 512\n",
    "task_batch_size = 64\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'sine'\n",
    "nc_regularize = True\n",
    "epochs = 1000\n",
    "train_steps = 500\n",
    "log_steps = 500\n",
    "test_steps = 250\n",
    "learn_freq = 10\n",
    "inner_lr = 0.01\n",
    "inner_steps = 16\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 40\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 3\n",
    "dec_depth = 2\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = f\"results/model.ckpt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    \"\"\"\n",
    "    Memory bank class. Stores snapshots of task learners.\n",
    "    get_batch() returns a random minibatch of (snapshot, gap) for NC to train on.\n",
    "    \"\"\"\n",
    "\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap):\n",
    "        if not hasattr(self, \"te_xp\"):\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "\n",
    "            MEMORY_LIMIT = 1_000_000\n",
    "            if self.te_xp.shape[0] > MEMORY_LIMIT:\n",
    "                self.te_xp = self.te_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xp = self.tr_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xyp = self.tr_xyp[-MEMORY_LIMIT:]\n",
    "                self.gap = self.gap[-MEMORY_LIMIT:]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        assert N == self.tr_xp.shape[0]\n",
    "        assert N == self.tr_xyp.shape[0]\n",
    "        assert N == self.gap.shape[0]\n",
    "\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regression and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    h = get_learner(\n",
    "        batch_size=x_train.shape[0],\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        activation= learner_act,\n",
    "        init_dim=23,\n",
    "        num_outputs=10,\n",
    "        task='regression'\n",
    "    ).to(device)\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    h_crit = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp}\n",
    "\n",
    "        h_loss = h_crit(preds_train.squeeze(), y_train.squeeze()).mean(-1).sum()\n",
    "        if  nc_regularize and global_step >  train_steps * 2:\n",
    "            model_preds = model(meta_batch)\n",
    "            # We sum NC outputs across tasks because h_loss is also summed.\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "        l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "            )\n",
    "    return h, meta_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    h = get_learner(\n",
    "        batch_size=x_train.shape[0],\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        activation= learner_act,\n",
    "        task='classification',\n",
    "    ).to(device)\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    h_crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp}\n",
    "\n",
    "        h_loss = h_crit(preds_train.squeeze(), y_train.squeeze()).mean(-1).sum()\n",
    "        if  nc_regularize and global_step >  train_steps * 2:\n",
    "            model_preds = model(meta_batch)\n",
    "            # We sum NC outputs across tasks because h_loss is also summed.\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "        l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "            )\n",
    "    return h, meta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity1D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    test_accum = Accumulator()\n",
    "    for batch in test_tasks:\n",
    "        h, meta_batch = run_regression(batch, train=False)\n",
    "\n",
    "        x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "        x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        with torch.no_grad():\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "            l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "            gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "            model_preds = model(meta_batch)\n",
    "            loss = mse_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "        test_accum.add_dict(\n",
    "            {\n",
    "                \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                \"mae\": [mae.item()],\n",
    "                \"loss\": [loss.item()],\n",
    "                \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    for batch in train_loader:\n",
    "        global_step += 1\n",
    "        if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "            run_regression(batch)\n",
    "\n",
    "        meta_batch, gap = memory_bank.get_batch( batch_size)\n",
    "        model_preds = model(meta_batch)\n",
    "        loss = mse_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mae = mae_criterion(model_preds.squeeze(), gap.squeeze())\n",
    "        accum.add_dict(\n",
    "            {\n",
    "                \"mae\": [mae.item()],\n",
    "                \"loss\": [loss.item()],\n",
    "                \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if global_step % log_steps == 0:\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)\n",
    "            print(metrics)\n",
    "\n",
    "        if timer() - global_timestamp >  time_budget:\n",
    "            logger.info(f\"Stopping at step {global_step}\")\n",
    "            quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2022-04-21 15:36:40.236 | INFO     | __main__:<cell line: 20>:20 - Populate time: 0.7901519589997861\n"
     ]
    }
   ],
   "source": [
    "memory_bank = MemoryBank()\n",
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 2\n",
    "populate_loader = []\n",
    "for tasks in range(task_count):\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "for i, batch in enumerate(populate_loader):\n",
    "    run_regression(batch)\n",
    "    # print(i+1)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:36:41.718 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-21 15:36:41.719 | INFO     | __main__:<cell line: 3>:5 - Bank size: 20480\n",
      "2022-04-21 15:36:42.424 | INFO     | __main__:test:38 - Test epoch 0\n",
      "2022-04-21 15:36:42.424 | INFO     | __main__:test:39 - mae 9.85e-01 loss 1.89e+00 R -0.107 l_test 7.50e-01 l_train 1.72e-01 \n",
      "2022-04-21 15:36:53.056 | INFO     | __main__:<cell line: 3>:14 - Time: train 10.6 test 0.7\n",
      "2022-04-21 15:36:53.057 | INFO     | __main__:<cell line: 3>:4 - Epoch 1\n",
      "2022-04-21 15:36:53.057 | INFO     | __main__:<cell line: 3>:5 - Bank size: 22528\n",
      "2022-04-21 15:36:53.712 | INFO     | __main__:test:38 - Test epoch 1\n",
      "2022-04-21 15:36:53.712 | INFO     | __main__:test:39 - mae 6.38e-01 loss 8.43e-01 R 0.392 l_test 6.84e-01 l_train 1.07e-01 \n",
      "2022-04-21 15:37:05.041 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.3 test 0.7\n",
      "2022-04-21 15:37:05.041 | INFO     | __main__:<cell line: 3>:4 - Epoch 2\n",
      "2022-04-21 15:37:05.042 | INFO     | __main__:<cell line: 3>:5 - Bank size: 24576\n",
      "2022-04-21 15:37:05.779 | INFO     | __main__:test:38 - Test epoch 2\n",
      "2022-04-21 15:37:05.780 | INFO     | __main__:test:39 - mae 4.26e-01 loss 4.35e-01 R 0.694 l_test 7.18e-01 l_train 1.65e-01 \n",
      "2022-04-21 15:37:18.556 | INFO     | __main__:<cell line: 3>:14 - Time: train 12.8 test 0.7\n",
      "2022-04-21 15:37:18.557 | INFO     | __main__:<cell line: 3>:4 - Epoch 3\n",
      "2022-04-21 15:37:18.557 | INFO     | __main__:<cell line: 3>:5 - Bank size: 26624\n",
      "2022-04-21 15:37:19.189 | INFO     | __main__:test:38 - Test epoch 3\n",
      "2022-04-21 15:37:19.189 | INFO     | __main__:test:39 - mae 4.15e-01 loss 3.96e-01 R 0.607 l_test 7.48e-01 l_train 1.06e-01 \n",
      "2022-04-21 15:37:30.833 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.6 test 0.6\n",
      "2022-04-21 15:37:30.834 | INFO     | __main__:<cell line: 3>:4 - Epoch 4\n",
      "2022-04-21 15:37:30.834 | INFO     | __main__:<cell line: 3>:5 - Bank size: 28672\n",
      "2022-04-21 15:37:31.472 | INFO     | __main__:test:38 - Test epoch 4\n",
      "2022-04-21 15:37:31.473 | INFO     | __main__:test:39 - mae 4.13e-01 loss 4.44e-01 R 0.610 l_test 7.49e-01 l_train 1.47e-01 \n",
      "2022-04-21 15:37:43.097 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.6 test 0.6\n",
      "2022-04-21 15:37:43.098 | INFO     | __main__:<cell line: 3>:4 - Epoch 5\n",
      "2022-04-21 15:37:43.098 | INFO     | __main__:<cell line: 3>:5 - Bank size: 30720\n",
      "2022-04-21 15:37:43.803 | INFO     | __main__:test:38 - Test epoch 5\n",
      "2022-04-21 15:37:43.803 | INFO     | __main__:test:39 - mae 3.74e-01 loss 2.89e-01 R 0.686 l_test 7.78e-01 l_train 1.26e-01 \n",
      "2022-04-21 15:37:55.761 | INFO     | __main__:<cell line: 3>:14 - Time: train 12.0 test 0.7\n",
      "2022-04-21 15:37:55.762 | INFO     | __main__:<cell line: 3>:4 - Epoch 6\n",
      "2022-04-21 15:37:55.762 | INFO     | __main__:<cell line: 3>:5 - Bank size: 32768\n",
      "2022-04-21 15:37:56.509 | INFO     | __main__:test:38 - Test epoch 6\n",
      "2022-04-21 15:37:56.509 | INFO     | __main__:test:39 - mae 6.81e-01 loss 1.80e+00 R 0.335 l_test 7.74e-01 l_train 1.43e-01 \n",
      "2022-04-21 15:38:07.312 | INFO     | __main__:<cell line: 3>:14 - Time: train 10.8 test 0.7\n",
      "2022-04-21 15:38:07.313 | INFO     | __main__:<cell line: 3>:4 - Epoch 7\n",
      "2022-04-21 15:38:07.313 | INFO     | __main__:<cell line: 3>:5 - Bank size: 34816\n",
      "2022-04-21 15:38:07.972 | INFO     | __main__:test:38 - Test epoch 7\n",
      "2022-04-21 15:38:07.973 | INFO     | __main__:test:39 - mae 5.36e-01 loss 2.83e+00 R 0.156 l_test 7.58e-01 l_train 1.52e-01 \n",
      "2022-04-21 15:38:20.159 | INFO     | __main__:<cell line: 3>:14 - Time: train 12.2 test 0.7\n",
      "2022-04-21 15:38:20.160 | INFO     | __main__:<cell line: 3>:4 - Epoch 8\n",
      "2022-04-21 15:38:20.161 | INFO     | __main__:<cell line: 3>:5 - Bank size: 36864\n",
      "2022-04-21 15:38:20.844 | INFO     | __main__:test:38 - Test epoch 8\n",
      "2022-04-21 15:38:20.845 | INFO     | __main__:test:39 - mae 4.90e-01 loss 1.31e+00 R 0.337 l_test 7.45e-01 l_train 1.33e-01 \n",
      "2022-04-21 15:38:32.953 | INFO     | __main__:<cell line: 3>:14 - Time: train 12.1 test 0.7\n",
      "2022-04-21 15:38:32.954 | INFO     | __main__:<cell line: 3>:4 - Epoch 9\n",
      "2022-04-21 15:38:32.954 | INFO     | __main__:<cell line: 3>:5 - Bank size: 38912\n",
      "2022-04-21 15:38:33.679 | INFO     | __main__:test:38 - Test epoch 9\n",
      "2022-04-21 15:38:33.680 | INFO     | __main__:test:39 - mae 3.54e-01 loss 2.74e-01 R 0.705 l_test 7.25e-01 l_train 1.27e-01 \n",
      "2022-04-21 15:38:45.671 | INFO     | __main__:<cell line: 3>:14 - Time: train 12.0 test 0.7\n",
      "2022-04-21 15:38:45.672 | INFO     | __main__:<cell line: 3>:4 - Epoch 10\n",
      "2022-04-21 15:38:45.672 | INFO     | __main__:<cell line: 3>:5 - Bank size: 40960\n",
      "2022-04-21 15:38:46.342 | INFO     | __main__:test:38 - Test epoch 10\n",
      "2022-04-21 15:38:46.343 | INFO     | __main__:test:39 - mae 3.12e-01 loss 2.45e-01 R 0.805 l_test 7.62e-01 l_train 1.48e-01 \n",
      "2022-04-21 15:38:57.934 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.6 test 0.7\n",
      "2022-04-21 15:38:57.935 | INFO     | __main__:<cell line: 3>:4 - Epoch 11\n",
      "2022-04-21 15:38:57.935 | INFO     | __main__:<cell line: 3>:5 - Bank size: 43008\n",
      "2022-04-21 15:38:58.659 | INFO     | __main__:test:38 - Test epoch 11\n",
      "2022-04-21 15:38:58.660 | INFO     | __main__:test:39 - mae 4.41e-01 loss 6.55e-01 R 0.514 l_test 7.00e-01 l_train 1.22e-01 \n",
      "2022-04-21 15:39:10.467 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.8 test 0.7\n",
      "2022-04-21 15:39:10.468 | INFO     | __main__:<cell line: 3>:4 - Epoch 12\n",
      "2022-04-21 15:39:10.468 | INFO     | __main__:<cell line: 3>:5 - Bank size: 45056\n",
      "2022-04-21 15:39:11.092 | INFO     | __main__:test:38 - Test epoch 12\n",
      "2022-04-21 15:39:11.092 | INFO     | __main__:test:39 - mae 3.55e-01 loss 3.18e-01 R 0.732 l_test 7.69e-01 l_train 1.09e-01 \n",
      "2022-04-21 15:39:16.435 | INFO     | __main__:train:33 - Train Step 1000\n",
      "2022-04-21 15:39:16.436 | INFO     | __main__:train:34 - mae 3.66e-01 loss 5.49e-01 R 0.793 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mae': 0.3659355178594828, 'loss': 0.5488774577054925, 'R': 0.793119962149505}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:39:22.960 | INFO     | __main__:<cell line: 3>:14 - Time: train 11.9 test 0.6\n",
      "2022-04-21 15:39:22.961 | INFO     | __main__:<cell line: 3>:4 - Epoch 13\n",
      "2022-04-21 15:39:22.961 | INFO     | __main__:<cell line: 3>:5 - Bank size: 47104\n",
      "2022-04-21 15:39:43.346 | INFO     | __main__:test:38 - Test epoch 13\n",
      "2022-04-21 15:39:43.346 | INFO     | __main__:test:39 - mae nan loss nan R nan l_test nan l_train nan \n",
      "2022-04-21 15:39:56.858 | INFO     | __main__:<cell line: 3>:14 - Time: train 13.5 test 20.4\n",
      "2022-04-21 15:39:56.859 | INFO     | __main__:<cell line: 3>:4 - Epoch 14\n",
      "2022-04-21 15:39:56.860 | INFO     | __main__:<cell line: 3>:5 - Bank size: 49152\n",
      "2022-04-21 15:40:15.931 | INFO     | __main__:test:38 - Test epoch 14\n",
      "2022-04-21 15:40:15.932 | INFO     | __main__:test:39 - mae nan loss nan R nan l_test nan l_train nan \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000018?line=8'>9</a>\u001b[0m test_elapsed \u001b[39m=\u001b[39m timer() \u001b[39m-\u001b[39m test_timestamp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000018?line=10'>11</a>\u001b[0m train_timestamp \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000018?line=11'>12</a>\u001b[0m out \u001b[39m=\u001b[39m train(populate_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000018?line=12'>13</a>\u001b[0m train_elapsed \u001b[39m=\u001b[39m timer() \u001b[39m-\u001b[39m train_timestamp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000018?line=13'>14</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime: train \u001b[39m\u001b[39m{\u001b[39;00mtrain_elapsed\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m test \u001b[39m\u001b[39m{\u001b[39;00mtest_elapsed\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours.ipynb Cell 26'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000014?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m mse_criterion(model_preds\u001b[39m.\u001b[39msqueeze(), gap\u001b[39m.\u001b[39msqueeze())\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000014?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000014?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000014?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours.ipynb#ch0000014?line=16'>17</a>\u001b[0m mae \u001b[39m=\u001b[39m mae_criterion(model_preds\u001b[39m.\u001b[39msqueeze(), gap\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=245'>246</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=246'>247</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=247'>248</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=248'>249</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=252'>253</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=253'>254</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=254'>255</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=144'>145</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=146'>147</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=147'>148</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=148'>149</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    test_timestamp = timer()\n",
    "    out = test(epoch, populate_loader)\n",
    "    test_elapsed = timer() - test_timestamp\n",
    "\n",
    "    train_timestamp = timer()\n",
    "    out = train(populate_loader)\n",
    "    train_elapsed = timer() - train_timestamp\n",
    "    logger.info(f\"Time: train {train_elapsed:.1f} test {test_elapsed:.1f}\")\n",
    "\n",
    "    with open(\"logs.json\", \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6387572d3ba60263f2472b530ce49454bee9bd13656fbfcf29efcd586b712758"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n",
      "\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n",
      "\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:13:42.733 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"result/best_model_train_cs_with_nc_alpha.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=0.9, kappa=1500)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:03:04.261 | INFO     | __main__:<module>:9 - Test 0.4915 +- 0.1441\n",
      "2022-04-28 14:03:04.262 | INFO     | __main__:<module>:10 - Train 0.1612 +- 0.0195\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 1.3090\n",
      "MSE for Dimension 2: 3.1199\n",
      "MSE for Dimension 3: 2.1227\n",
      "MSE for Dimension 4: 1.3085\n",
      "MSE for Dimension 5: 1.6851\n",
      "MSE for Dimension 6: 1.4910\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.30      0.40      6826\n",
      "         1.0       0.14      0.26      0.18      2121\n",
      "         2.0       0.15      0.17      0.16      1717\n",
      "         3.0       0.01      0.06      0.02       408\n",
      "\n",
      "    accuracy                           0.26     11072\n",
      "   macro avg       0.23      0.20      0.19     11072\n",
      "weighted avg       0.42      0.26      0.31     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.60      0.68      8096\n",
      "         1.0       0.00      0.00      0.00       469\n",
      "         2.0       0.10      0.34      0.16       790\n",
      "         3.0       0.15      0.08      0.10      1717\n",
      "\n",
      "    accuracy                           0.48     11072\n",
      "   macro avg       0.25      0.25      0.23     11072\n",
      "weighted avg       0.59      0.48      0.52     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.04      0.08      2716\n",
      "         1.0       0.52      0.46      0.49      4925\n",
      "         2.0       0.09      0.18      0.12      1293\n",
      "         3.0       0.17      0.29      0.21      2138\n",
      "\n",
      "    accuracy                           0.29     11072\n",
      "   macro avg       0.26      0.24      0.22     11072\n",
      "weighted avg       0.34      0.29      0.29     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.44      0.46      4859\n",
      "         1.0       0.12      0.21      0.15      1442\n",
      "         2.0       0.13      0.63      0.22       561\n",
      "         3.0       0.50      0.17      0.25      4210\n",
      "\n",
      "    accuracy                           0.32     11072\n",
      "   macro avg       0.31      0.36      0.27     11072\n",
      "weighted avg       0.42      0.32      0.33     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ec3a1282e4a910a08dbbf6bccb21de56a06709111f3325577682e497c6adc6"
  },
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

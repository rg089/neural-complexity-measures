{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"ds_with_nc_lstm_w_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"ds\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if data_type == 'cs' else 360\n",
    "xtrain_feat_dim = 18 if data_type == \"ds\" else None\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )\n",
    "    elif task == 'seq':\n",
    "        return TimeSeries(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)\n",
    "\n",
    "\n",
    "class TimeSeries(torch.nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size, seq_length=20, batch_size=32):\n",
    "        super(TimeSeries, self).__init__()\n",
    "        self.init_dim = init_dim\n",
    "        self.seq_len = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.lstm = torch.nn.LSTM(input_size = self.init_dim, \n",
    "                                 hidden_size = self.hidden_size,\n",
    "                                 num_layers = self.num_layers, \n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.linear_reg = torch.nn.Linear(self.hidden_size*self.seq_len, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(self.hidden_size*self.seq_len, 4)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        cell_state = torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(device)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, _, _ = x.size()\n",
    "        gru_out, _ = self.lstm(x, self.hidden)\n",
    "        x = gru_out.contiguous().view(batch_size,-1)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:09:40.339 | INFO     | __main__:<module>:19 - Populate time: 0.048970870673656464\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "    if data_type == \"ds\":\n",
    "        h.init_hidden()\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:09:44.847 | INFO     | __main__:<module>:4 - Epoch 0\n",
      "2022-04-26 14:09:44.848 | INFO     | __main__:<module>:5 - Bank size: 14240\n",
      "2022-04-26 14:09:44.926 | INFO     | __main__:train:49 - Train Step 1\n",
      "2022-04-26 14:09:44.926 | INFO     | __main__:train:51 - mae 4.47e-01 loss 1.54e-01 R -0.356 gap 0.010691046714782715 preds 0.00990760326385498\n",
      "2022-04-26 14:09:45.031 | INFO     | __main__:train:49 - Train Step 2\n",
      "2022-04-26 14:09:45.032 | INFO     | __main__:train:51 - mae 5.11e-01 loss 1.95e-01 R -0.164 gap 0.008543393574655056 preds -0.004653267562389374\n",
      "2022-04-26 14:09:45.124 | INFO     | __main__:train:49 - Train Step 3\n",
      "2022-04-26 14:09:45.125 | INFO     | __main__:train:51 - mae 5.63e-01 loss 2.28e-01 R -0.183 gap 0.011010394431650639 preds 0.022747159004211426\n",
      "2022-04-26 14:09:45.217 | INFO     | __main__:train:49 - Train Step 4\n",
      "2022-04-26 14:09:45.219 | INFO     | __main__:train:51 - mae 5.75e-01 loss 2.39e-01 R -0.200 gap 0.010789942927658558 preds 0.02954377420246601\n",
      "2022-04-26 14:09:45.294 | INFO     | __main__:train:49 - Train Step 5\n",
      "2022-04-26 14:09:45.294 | INFO     | __main__:train:51 - mae 5.44e-01 loss 2.20e-01 R -0.189 gap 0.010062215849757195 preds 0.02877225913107395\n",
      "2022-04-26 14:09:45.369 | INFO     | __main__:train:49 - Train Step 6\n",
      "2022-04-26 14:09:45.369 | INFO     | __main__:train:51 - mae 5.39e-01 loss 2.17e-01 R -0.110 gap 0.010304399766027927 preds 0.03344583138823509\n",
      "2022-04-26 14:09:45.449 | INFO     | __main__:train:49 - Train Step 7\n",
      "2022-04-26 14:09:45.450 | INFO     | __main__:train:51 - mae 5.31e-01 loss 2.09e-01 R -0.088 gap 0.009706159122288227 preds 0.031237494200468063\n",
      "2022-04-26 14:09:45.537 | INFO     | __main__:train:49 - Train Step 8\n",
      "2022-04-26 14:09:45.537 | INFO     | __main__:train:51 - mae 5.05e-01 loss 1.95e-01 R -0.075 gap 0.009195903316140175 preds 0.033180978149175644\n",
      "2022-04-26 14:09:45.620 | INFO     | __main__:train:49 - Train Step 9\n",
      "2022-04-26 14:09:45.621 | INFO     | __main__:train:51 - mae 5.03e-01 loss 1.92e-01 R -0.074 gap 0.00903881061822176 preds 0.025895068421959877\n",
      "2022-04-26 14:09:45.967 | INFO     | __main__:train:49 - Train Step 10\n",
      "2022-04-26 14:09:45.968 | INFO     | __main__:train:51 - mae 4.96e-01 loss 1.89e-01 R -0.086 gap 0.00936820823699236 preds 0.02841608226299286\n",
      "2022-04-26 14:09:46.058 | INFO     | __main__:train:49 - Train Step 11\n",
      "2022-04-26 14:09:46.059 | INFO     | __main__:train:51 - mae 4.88e-01 loss 1.83e-01 R -0.083 gap 0.009173455648124218 preds 0.029250813648104668\n",
      "2022-04-26 14:09:46.139 | INFO     | __main__:train:49 - Train Step 12\n",
      "2022-04-26 14:09:46.140 | INFO     | __main__:train:51 - mae 4.76e-01 loss 1.78e-01 R -0.063 gap 0.00879585463553667 preds 0.027743972837924957\n",
      "2022-04-26 14:09:46.216 | INFO     | __main__:train:49 - Train Step 13\n",
      "2022-04-26 14:09:46.217 | INFO     | __main__:train:51 - mae 4.77e-01 loss 1.77e-01 R -0.052 gap 0.008959870785474777 preds 0.027425825595855713\n",
      "2022-04-26 14:09:46.292 | INFO     | __main__:train:49 - Train Step 14\n",
      "2022-04-26 14:09:46.293 | INFO     | __main__:train:51 - mae 4.73e-01 loss 1.73e-01 R -0.052 gap 0.00871039368212223 preds 0.02895832434296608\n",
      "2022-04-26 14:09:46.372 | INFO     | __main__:train:49 - Train Step 15\n",
      "2022-04-26 14:09:46.373 | INFO     | __main__:train:51 - mae 4.71e-01 loss 1.72e-01 R -0.044 gap 0.00872704479843378 preds 0.030938472598791122\n",
      "2022-04-26 14:09:46.450 | INFO     | __main__:train:49 - Train Step 16\n",
      "2022-04-26 14:09:46.451 | INFO     | __main__:train:51 - mae 4.68e-01 loss 1.69e-01 R -0.042 gap 0.009112601168453693 preds 0.029170509427785873\n",
      "2022-04-26 14:09:46.540 | INFO     | __main__:train:49 - Train Step 17\n",
      "2022-04-26 14:09:46.541 | INFO     | __main__:train:51 - mae 4.64e-01 loss 1.68e-01 R -0.040 gap 0.009386197663843632 preds 0.026379704475402832\n",
      "2022-04-26 14:09:46.631 | INFO     | __main__:train:49 - Train Step 18\n",
      "2022-04-26 14:09:46.632 | INFO     | __main__:train:51 - mae 4.64e-01 loss 1.68e-01 R -0.034 gap 0.009508882649242878 preds 0.02617490664124489\n",
      "2022-04-26 14:09:46.717 | INFO     | __main__:train:49 - Train Step 19\n",
      "2022-04-26 14:09:46.718 | INFO     | __main__:train:51 - mae 4.61e-01 loss 1.66e-01 R -0.032 gap 0.009303011000156403 preds 0.024049166589975357\n",
      "2022-04-26 14:09:47.049 | INFO     | __main__:train:49 - Train Step 20\n",
      "2022-04-26 14:09:47.050 | INFO     | __main__:train:51 - mae 4.58e-01 loss 1.64e-01 R -0.034 gap 0.009321214631199837 preds 0.024463247507810593\n",
      "2022-04-26 14:09:47.135 | INFO     | __main__:train:49 - Train Step 21\n",
      "2022-04-26 14:09:47.135 | INFO     | __main__:train:51 - mae 4.56e-01 loss 1.62e-01 R -0.033 gap 0.009464127011597157 preds 0.02391761541366577\n",
      "2022-04-26 14:09:47.220 | INFO     | __main__:train:49 - Train Step 22\n",
      "2022-04-26 14:09:47.220 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.59e-01 R -0.035 gap 0.009539647959172726 preds 0.021072374656796455\n",
      "2022-04-26 14:09:47.298 | INFO     | __main__:train:49 - Train Step 23\n",
      "2022-04-26 14:09:47.298 | INFO     | __main__:train:51 - mae 4.53e-01 loss 1.60e-01 R -0.029 gap 0.009645795449614525 preds 0.01995895244181156\n",
      "2022-04-26 14:09:47.381 | INFO     | __main__:train:49 - Train Step 24\n",
      "2022-04-26 14:09:47.382 | INFO     | __main__:train:51 - mae 4.55e-01 loss 1.62e-01 R -0.029 gap 0.009625536389648914 preds 0.017964573577046394\n",
      "2022-04-26 14:09:47.466 | INFO     | __main__:train:49 - Train Step 25\n",
      "2022-04-26 14:09:47.467 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.60e-01 R -0.032 gap 0.009601805359125137 preds 0.016235677525401115\n",
      "2022-04-26 14:09:47.551 | INFO     | __main__:train:49 - Train Step 26\n",
      "2022-04-26 14:09:47.552 | INFO     | __main__:train:51 - mae 4.52e-01 loss 1.59e-01 R -0.024 gap 0.00953223928809166 preds 0.016479674726724625\n",
      "2022-04-26 14:09:47.638 | INFO     | __main__:train:49 - Train Step 27\n",
      "2022-04-26 14:09:47.639 | INFO     | __main__:train:51 - mae 4.50e-01 loss 1.57e-01 R -0.035 gap 0.009591654874384403 preds 0.015103872865438461\n",
      "2022-04-26 14:09:47.721 | INFO     | __main__:train:49 - Train Step 28\n",
      "2022-04-26 14:09:47.722 | INFO     | __main__:train:51 - mae 4.47e-01 loss 1.55e-01 R -0.025 gap 0.009695478715002537 preds 0.014313817955553532\n",
      "2022-04-26 14:09:47.822 | INFO     | __main__:train:49 - Train Step 29\n",
      "2022-04-26 14:09:47.823 | INFO     | __main__:train:51 - mae 4.45e-01 loss 1.54e-01 R -0.020 gap 0.00965841580182314 preds 0.012795012444257736\n",
      "2022-04-26 14:09:48.147 | INFO     | __main__:train:49 - Train Step 30\n",
      "2022-04-26 14:09:48.148 | INFO     | __main__:train:51 - mae 4.43e-01 loss 1.52e-01 R -0.012 gap 0.009535347111523151 preds 0.012421122752130032\n",
      "2022-04-26 14:09:48.230 | INFO     | __main__:train:49 - Train Step 31\n",
      "2022-04-26 14:09:48.231 | INFO     | __main__:train:51 - mae 4.37e-01 loss 1.49e-01 R -0.005 gap 0.00960548035800457 preds 0.01213536411523819\n",
      "2022-04-26 14:09:48.315 | INFO     | __main__:train:49 - Train Step 32\n",
      "2022-04-26 14:09:48.316 | INFO     | __main__:train:51 - mae 4.37e-01 loss 1.48e-01 R -0.018 gap 0.009633762761950493 preds 0.012612031772732735\n",
      "2022-04-26 14:09:48.399 | INFO     | __main__:train:49 - Train Step 33\n",
      "2022-04-26 14:09:48.399 | INFO     | __main__:train:51 - mae 4.35e-01 loss 1.47e-01 R -0.021 gap 0.009807875379920006 preds 0.012846296653151512\n",
      "2022-04-26 14:09:48.482 | INFO     | __main__:train:49 - Train Step 34\n",
      "2022-04-26 14:09:48.483 | INFO     | __main__:train:51 - mae 4.32e-01 loss 1.45e-01 R -0.017 gap 0.010115692391991615 preds 0.013313055969774723\n",
      "2022-04-26 14:09:48.568 | INFO     | __main__:train:49 - Train Step 35\n",
      "2022-04-26 14:09:48.569 | INFO     | __main__:train:51 - mae 4.26e-01 loss 1.43e-01 R -0.015 gap 0.010060813277959824 preds 0.012380254454910755\n",
      "2022-04-26 14:09:48.651 | INFO     | __main__:train:49 - Train Step 36\n",
      "2022-04-26 14:09:48.652 | INFO     | __main__:train:51 - mae 4.21e-01 loss 1.40e-01 R -0.012 gap 0.009956317022442818 preds 0.013064093887805939\n",
      "2022-04-26 14:09:48.736 | INFO     | __main__:train:49 - Train Step 37\n",
      "2022-04-26 14:09:48.737 | INFO     | __main__:train:51 - mae 4.16e-01 loss 1.38e-01 R -0.008 gap 0.010074797086417675 preds 0.01291937567293644\n",
      "2022-04-26 14:09:48.817 | INFO     | __main__:train:49 - Train Step 38\n",
      "2022-04-26 14:09:48.817 | INFO     | __main__:train:51 - mae 4.12e-01 loss 1.36e-01 R -0.007 gap 0.010006190277636051 preds 0.013730262406170368\n",
      "2022-04-26 14:09:48.899 | INFO     | __main__:train:49 - Train Step 39\n",
      "2022-04-26 14:09:48.900 | INFO     | __main__:train:51 - mae 4.07e-01 loss 1.33e-01 R -0.008 gap 0.010199090465903282 preds 0.013438989408314228\n",
      "2022-04-26 14:09:49.226 | INFO     | __main__:train:49 - Train Step 40\n",
      "2022-04-26 14:09:49.227 | INFO     | __main__:train:51 - mae 4.06e-01 loss 1.32e-01 R -0.013 gap 0.009995566681027412 preds 0.013180620968341827\n",
      "2022-04-26 14:09:49.311 | INFO     | __main__:train:49 - Train Step 41\n",
      "2022-04-26 14:09:49.312 | INFO     | __main__:train:51 - mae 4.03e-01 loss 1.30e-01 R -0.008 gap 0.010075002908706665 preds 0.013499814085662365\n",
      "2022-04-26 14:09:49.395 | INFO     | __main__:train:49 - Train Step 42\n",
      "2022-04-26 14:09:49.396 | INFO     | __main__:train:51 - mae 3.98e-01 loss 1.28e-01 R -0.008 gap 0.010182539001107216 preds 0.012693716213107109\n",
      "2022-04-26 14:09:49.481 | INFO     | __main__:train:49 - Train Step 43\n",
      "2022-04-26 14:09:49.481 | INFO     | __main__:train:51 - mae 3.95e-01 loss 1.26e-01 R -0.002 gap 0.009990985505282879 preds 0.011557983234524727\n",
      "2022-04-26 14:09:49.567 | INFO     | __main__:train:49 - Train Step 44\n",
      "2022-04-26 14:09:49.568 | INFO     | __main__:train:51 - mae 3.92e-01 loss 1.25e-01 R -0.001 gap 0.009867644868791103 preds 0.012679332867264748\n",
      "2022-04-26 14:09:49.656 | INFO     | __main__:train:49 - Train Step 45\n",
      "2022-04-26 14:09:49.656 | INFO     | __main__:train:51 - mae 3.91e-01 loss 1.24e-01 R -0.005 gap 0.009847410954535007 preds 0.012875852175056934\n",
      "2022-04-26 14:09:49.740 | INFO     | __main__:train:49 - Train Step 46\n",
      "2022-04-26 14:09:49.741 | INFO     | __main__:train:51 - mae 3.87e-01 loss 1.22e-01 R -0.001 gap 0.009881227277219296 preds 0.013133466243743896\n",
      "2022-04-26 14:09:49.826 | INFO     | __main__:train:49 - Train Step 47\n",
      "2022-04-26 14:09:49.827 | INFO     | __main__:train:51 - mae 3.84e-01 loss 1.20e-01 R 0.003 gap 0.009830137714743614 preds 0.013966417871415615\n",
      "2022-04-26 14:09:49.905 | INFO     | __main__:train:49 - Train Step 48\n",
      "2022-04-26 14:09:49.906 | INFO     | __main__:train:51 - mae 3.82e-01 loss 1.19e-01 R 0.001 gap 0.009832832030951977 preds 0.014268450438976288\n",
      "2022-04-26 14:09:49.984 | INFO     | __main__:train:49 - Train Step 49\n",
      "2022-04-26 14:09:49.985 | INFO     | __main__:train:51 - mae 3.80e-01 loss 1.18e-01 R 0.001 gap 0.009650932624936104 preds 0.014606147073209286\n",
      "2022-04-26 14:09:50.311 | INFO     | __main__:train:49 - Train Step 50\n",
      "2022-04-26 14:09:50.312 | INFO     | __main__:train:51 - mae 3.77e-01 loss 1.17e-01 R 0.005 gap 0.00978623703122139 preds 0.014768715016543865\n",
      "2022-04-26 14:09:50.396 | INFO     | __main__:train:49 - Train Step 51\n",
      "2022-04-26 14:09:50.397 | INFO     | __main__:train:51 - mae 3.75e-01 loss 1.15e-01 R 0.009 gap 0.009832006879150867 preds 0.014220447279512882\n",
      "2022-04-26 14:09:50.481 | INFO     | __main__:train:49 - Train Step 52\n",
      "2022-04-26 14:09:50.481 | INFO     | __main__:train:51 - mae 3.73e-01 loss 1.14e-01 R 0.007 gap 0.00978753063827753 preds 0.013851453550159931\n",
      "2022-04-26 14:09:50.567 | INFO     | __main__:train:49 - Train Step 53\n",
      "2022-04-26 14:09:50.568 | INFO     | __main__:train:51 - mae 3.70e-01 loss 1.13e-01 R 0.007 gap 0.00963086262345314 preds 0.014222826808691025\n",
      "2022-04-26 14:09:50.653 | INFO     | __main__:train:49 - Train Step 54\n",
      "2022-04-26 14:09:50.653 | INFO     | __main__:train:51 - mae 3.68e-01 loss 1.12e-01 R 0.011 gap 0.00961339846253395 preds 0.015113799832761288\n",
      "2022-04-26 14:09:50.738 | INFO     | __main__:train:49 - Train Step 55\n",
      "2022-04-26 14:09:50.739 | INFO     | __main__:train:51 - mae 3.66e-01 loss 1.11e-01 R 0.007 gap 0.009529389441013336 preds 0.014625848270952702\n",
      "2022-04-26 14:09:50.824 | INFO     | __main__:train:49 - Train Step 56\n",
      "2022-04-26 14:09:50.824 | INFO     | __main__:train:51 - mae 3.63e-01 loss 1.09e-01 R 0.005 gap 0.00950856413692236 preds 0.014218838885426521\n",
      "2022-04-26 14:09:50.905 | INFO     | __main__:train:49 - Train Step 57\n",
      "2022-04-26 14:09:50.906 | INFO     | __main__:train:51 - mae 3.60e-01 loss 1.08e-01 R 0.007 gap 0.009520218707621098 preds 0.01373262144625187\n",
      "2022-04-26 14:09:50.989 | INFO     | __main__:train:49 - Train Step 58\n",
      "2022-04-26 14:09:50.990 | INFO     | __main__:train:51 - mae 3.56e-01 loss 1.06e-01 R 0.010 gap 0.009496393613517284 preds 0.013425331562757492\n",
      "2022-04-26 14:09:51.073 | INFO     | __main__:train:49 - Train Step 59\n",
      "2022-04-26 14:09:51.074 | INFO     | __main__:train:51 - mae 3.54e-01 loss 1.05e-01 R 0.005 gap 0.00950838066637516 preds 0.013215573504567146\n",
      "2022-04-26 14:09:51.419 | INFO     | __main__:train:49 - Train Step 60\n",
      "2022-04-26 14:09:51.420 | INFO     | __main__:train:51 - mae 3.52e-01 loss 1.04e-01 R 0.006 gap 0.009566368535161018 preds 0.013053636066615582\n",
      "2022-04-26 14:09:51.506 | INFO     | __main__:train:49 - Train Step 61\n",
      "2022-04-26 14:09:51.507 | INFO     | __main__:train:51 - mae 3.49e-01 loss 1.03e-01 R 0.010 gap 0.009535671211779118 preds 0.01269760262221098\n",
      "2022-04-26 14:09:51.595 | INFO     | __main__:train:49 - Train Step 62\n",
      "2022-04-26 14:09:51.596 | INFO     | __main__:train:51 - mae 3.46e-01 loss 1.02e-01 R 0.012 gap 0.009559566155076027 preds 0.012621410191059113\n",
      "2022-04-26 14:09:51.679 | INFO     | __main__:train:49 - Train Step 63\n",
      "2022-04-26 14:09:51.680 | INFO     | __main__:train:51 - mae 3.44e-01 loss 1.00e-01 R 0.012 gap 0.009615330025553703 preds 0.012605172581970692\n",
      "2022-04-26 14:09:51.759 | INFO     | __main__:train:49 - Train Step 64\n",
      "2022-04-26 14:09:51.760 | INFO     | __main__:train:51 - mae 3.42e-01 loss 9.94e-02 R 0.010 gap 0.009656557813286781 preds 0.013419980183243752\n",
      "2022-04-26 14:09:51.841 | INFO     | __main__:train:49 - Train Step 65\n",
      "2022-04-26 14:09:51.842 | INFO     | __main__:train:51 - mae 3.40e-01 loss 9.84e-02 R 0.009 gap 0.009610779583454132 preds 0.014330883510410786\n",
      "2022-04-26 14:09:51.921 | INFO     | __main__:train:49 - Train Step 66\n",
      "2022-04-26 14:09:51.922 | INFO     | __main__:train:51 - mae 3.37e-01 loss 9.72e-02 R 0.009 gap 0.009604188613593578 preds 0.014629208482801914\n",
      "2022-04-26 14:09:51.998 | INFO     | __main__:train:49 - Train Step 67\n",
      "2022-04-26 14:09:51.998 | INFO     | __main__:train:51 - mae 3.34e-01 loss 9.60e-02 R 0.012 gap 0.009728124365210533 preds 0.014764221385121346\n",
      "2022-04-26 14:09:52.077 | INFO     | __main__:train:49 - Train Step 68\n",
      "2022-04-26 14:09:52.078 | INFO     | __main__:train:51 - mae 3.32e-01 loss 9.50e-02 R 0.015 gap 0.009660356678068638 preds 0.014338687993586063\n",
      "2022-04-26 14:09:52.164 | INFO     | __main__:train:49 - Train Step 69\n",
      "2022-04-26 14:09:52.164 | INFO     | __main__:train:51 - mae 3.29e-01 loss 9.39e-02 R 0.013 gap 0.009696906432509422 preds 0.014317745342850685\n",
      "2022-04-26 14:09:52.480 | INFO     | __main__:train:49 - Train Step 70\n",
      "2022-04-26 14:09:52.481 | INFO     | __main__:train:51 - mae 3.28e-01 loss 9.31e-02 R 0.013 gap 0.009723171591758728 preds 0.01416220422834158\n",
      "2022-04-26 14:09:52.568 | INFO     | __main__:train:49 - Train Step 71\n",
      "2022-04-26 14:09:52.569 | INFO     | __main__:train:51 - mae 3.25e-01 loss 9.22e-02 R 0.014 gap 0.009844101034104824 preds 0.013509456068277359\n",
      "2022-04-26 14:09:52.648 | INFO     | __main__:train:49 - Train Step 72\n",
      "2022-04-26 14:09:52.648 | INFO     | __main__:train:51 - mae 3.23e-01 loss 9.13e-02 R 0.015 gap 0.009785688482224941 preds 0.013401927426457405\n",
      "2022-04-26 14:09:52.726 | INFO     | __main__:train:49 - Train Step 73\n",
      "2022-04-26 14:09:52.727 | INFO     | __main__:train:51 - mae 3.21e-01 loss 9.04e-02 R 0.017 gap 0.009790904819965363 preds 0.013337558135390282\n",
      "2022-04-26 14:09:52.804 | INFO     | __main__:train:49 - Train Step 74\n",
      "2022-04-26 14:09:52.805 | INFO     | __main__:train:51 - mae 3.19e-01 loss 8.97e-02 R 0.017 gap 0.009751061908900738 preds 0.012590655125677586\n",
      "2022-04-26 14:09:52.881 | INFO     | __main__:train:49 - Train Step 75\n",
      "2022-04-26 14:09:52.882 | INFO     | __main__:train:51 - mae 3.18e-01 loss 8.90e-02 R 0.019 gap 0.009718772955238819 preds 0.011811284348368645\n",
      "2022-04-26 14:09:52.963 | INFO     | __main__:train:49 - Train Step 76\n",
      "2022-04-26 14:09:52.963 | INFO     | __main__:train:51 - mae 3.16e-01 loss 8.82e-02 R 0.020 gap 0.009716865606606007 preds 0.011315525509417057\n",
      "2022-04-26 14:09:53.044 | INFO     | __main__:train:49 - Train Step 77\n",
      "2022-04-26 14:09:53.045 | INFO     | __main__:train:51 - mae 3.14e-01 loss 8.75e-02 R 0.018 gap 0.009789185598492622 preds 0.010933627374470234\n",
      "2022-04-26 14:09:53.124 | INFO     | __main__:train:49 - Train Step 78\n",
      "2022-04-26 14:09:53.125 | INFO     | __main__:train:51 - mae 3.13e-01 loss 8.67e-02 R 0.019 gap 0.009741874411702156 preds 0.01073919702321291\n",
      "2022-04-26 14:09:53.204 | INFO     | __main__:train:49 - Train Step 79\n",
      "2022-04-26 14:09:53.205 | INFO     | __main__:train:51 - mae 3.11e-01 loss 8.60e-02 R 0.019 gap 0.009720311500132084 preds 0.010212235152721405\n",
      "2022-04-26 14:09:53.510 | INFO     | __main__:train:49 - Train Step 80\n",
      "2022-04-26 14:09:53.511 | INFO     | __main__:train:51 - mae 3.10e-01 loss 8.54e-02 R 0.013 gap 0.009746791794896126 preds 0.010089725255966187\n",
      "2022-04-26 14:09:53.593 | INFO     | __main__:train:49 - Train Step 81\n",
      "2022-04-26 14:09:53.594 | INFO     | __main__:train:51 - mae 3.09e-01 loss 8.49e-02 R 0.011 gap 0.009747661650180817 preds 0.009931462816894054\n",
      "2022-04-26 14:09:53.673 | INFO     | __main__:train:49 - Train Step 82\n",
      "2022-04-26 14:09:53.674 | INFO     | __main__:train:51 - mae 3.07e-01 loss 8.41e-02 R 0.011 gap 0.009738363325595856 preds 0.009765234775841236\n",
      "2022-04-26 14:09:53.748 | INFO     | __main__:train:49 - Train Step 83\n",
      "2022-04-26 14:09:53.749 | INFO     | __main__:train:51 - mae 3.06e-01 loss 8.36e-02 R 0.014 gap 0.009874267503619194 preds 0.009977382607758045\n",
      "2022-04-26 14:09:53.823 | INFO     | __main__:train:49 - Train Step 84\n",
      "2022-04-26 14:09:53.824 | INFO     | __main__:train:51 - mae 3.05e-01 loss 8.30e-02 R 0.016 gap 0.009965146891772747 preds 0.009802517481148243\n",
      "2022-04-26 14:09:53.900 | INFO     | __main__:train:49 - Train Step 85\n",
      "2022-04-26 14:09:53.900 | INFO     | __main__:train:51 - mae 3.03e-01 loss 8.22e-02 R 0.016 gap 0.009839922189712524 preds 0.009742122143507004\n",
      "2022-04-26 14:09:53.975 | INFO     | __main__:train:49 - Train Step 86\n",
      "2022-04-26 14:09:53.976 | INFO     | __main__:train:51 - mae 3.01e-01 loss 8.15e-02 R 0.016 gap 0.009795043617486954 preds 0.009930548258125782\n",
      "2022-04-26 14:09:54.055 | INFO     | __main__:train:49 - Train Step 87\n",
      "2022-04-26 14:09:54.055 | INFO     | __main__:train:51 - mae 2.99e-01 loss 8.08e-02 R 0.016 gap 0.009769131429493427 preds 0.009935279376804829\n",
      "2022-04-26 14:09:54.136 | INFO     | __main__:train:49 - Train Step 88\n",
      "2022-04-26 14:09:54.137 | INFO     | __main__:train:51 - mae 2.98e-01 loss 8.01e-02 R 0.016 gap 0.009724400006234646 preds 0.00965889636427164\n",
      "2022-04-26 14:09:54.216 | INFO     | __main__:train:49 - Train Step 89\n",
      "2022-04-26 14:09:54.217 | INFO     | __main__:train:51 - mae 2.96e-01 loss 7.93e-02 R 0.016 gap 0.009822453372180462 preds 0.009583651088178158\n",
      "2022-04-26 14:10:15.860 | INFO     | __main__:test:57 - Test epoch 0\n",
      "2022-04-26 14:10:15.861 | INFO     | __main__:test:59 - mae 1.46e-01 loss 1.83e-02 R -0.020 l_test 9.15e-01 l_train 8.96e-01 \n",
      "2022-04-26 14:10:15.863 | INFO     | __main__:<module>:4 - Epoch 1\n",
      "2022-04-26 14:10:15.863 | INFO     | __main__:<module>:5 - Bank size: 15520\n",
      "2022-04-26 14:10:16.199 | INFO     | __main__:train:49 - Train Step 90\n",
      "2022-04-26 14:10:16.200 | INFO     | __main__:train:51 - mae 2.94e-01 loss 7.87e-02 R 0.014 gap 0.00980132445693016 preds 0.009129748679697514\n",
      "2022-04-26 14:10:16.289 | INFO     | __main__:train:49 - Train Step 91\n",
      "2022-04-26 14:10:16.290 | INFO     | __main__:train:51 - mae 2.93e-01 loss 7.80e-02 R 0.015 gap 0.009785423055291176 preds 0.009424682706594467\n",
      "2022-04-26 14:10:16.373 | INFO     | __main__:train:49 - Train Step 92\n",
      "2022-04-26 14:10:16.374 | INFO     | __main__:train:51 - mae 2.92e-01 loss 7.74e-02 R 0.015 gap 0.0098248440772295 preds 0.009421930648386478\n",
      "2022-04-26 14:10:16.456 | INFO     | __main__:train:49 - Train Step 93\n",
      "2022-04-26 14:10:16.456 | INFO     | __main__:train:51 - mae 2.90e-01 loss 7.67e-02 R 0.015 gap 0.009692821651697159 preds 0.009429804980754852\n",
      "2022-04-26 14:10:16.540 | INFO     | __main__:train:49 - Train Step 94\n",
      "2022-04-26 14:10:16.541 | INFO     | __main__:train:51 - mae 2.89e-01 loss 7.62e-02 R 0.017 gap 0.009649696759879589 preds 0.009272300638258457\n",
      "2022-04-26 14:10:16.626 | INFO     | __main__:train:49 - Train Step 95\n",
      "2022-04-26 14:10:16.626 | INFO     | __main__:train:51 - mae 2.88e-01 loss 7.56e-02 R 0.015 gap 0.009613236412405968 preds 0.009510635398328304\n",
      "2022-04-26 14:10:16.710 | INFO     | __main__:train:49 - Train Step 96\n",
      "2022-04-26 14:10:16.711 | INFO     | __main__:train:51 - mae 2.86e-01 loss 7.50e-02 R 0.014 gap 0.00964098796248436 preds 0.009631244465708733\n",
      "2022-04-26 14:10:16.795 | INFO     | __main__:train:49 - Train Step 97\n",
      "2022-04-26 14:10:16.796 | INFO     | __main__:train:51 - mae 2.85e-01 loss 7.44e-02 R 0.014 gap 0.009675970301032066 preds 0.009850570932030678\n",
      "2022-04-26 14:10:16.878 | INFO     | __main__:train:49 - Train Step 98\n",
      "2022-04-26 14:10:16.879 | INFO     | __main__:train:51 - mae 2.83e-01 loss 7.38e-02 R 0.014 gap 0.009647908620536327 preds 0.00990293174982071\n",
      "2022-04-26 14:10:16.964 | INFO     | __main__:train:49 - Train Step 99\n",
      "2022-04-26 14:10:16.965 | INFO     | __main__:train:51 - mae 2.82e-01 loss 7.32e-02 R 0.017 gap 0.009676225483417511 preds 0.01022128015756607\n",
      "2022-04-26 14:10:17.296 | INFO     | __main__:train:49 - Train Step 100\n",
      "2022-04-26 14:10:17.296 | INFO     | __main__:train:51 - mae 2.80e-01 loss 7.26e-02 R 0.018 gap 0.009640143252909184 preds 0.010635746642947197\n",
      "2022-04-26 14:10:17.380 | INFO     | __main__:train:49 - Train Step 101\n",
      "2022-04-26 14:10:17.381 | INFO     | __main__:train:51 - mae 2.79e-01 loss 7.20e-02 R 0.018 gap 0.009607885964214802 preds 0.011205925606191158\n",
      "2022-04-26 14:10:17.467 | INFO     | __main__:train:49 - Train Step 102\n",
      "2022-04-26 14:10:17.468 | INFO     | __main__:train:51 - mae 2.77e-01 loss 7.14e-02 R 0.019 gap 0.009610768407583237 preds 0.011227156966924667\n",
      "2022-04-26 14:10:17.553 | INFO     | __main__:train:49 - Train Step 103\n",
      "2022-04-26 14:10:17.554 | INFO     | __main__:train:51 - mae 2.76e-01 loss 7.09e-02 R 0.017 gap 0.009596442803740501 preds 0.011291330680251122\n",
      "2022-04-26 14:10:17.636 | INFO     | __main__:train:49 - Train Step 104\n",
      "2022-04-26 14:10:17.637 | INFO     | __main__:train:51 - mae 2.74e-01 loss 7.04e-02 R 0.017 gap 0.009607781656086445 preds 0.011314786039292812\n",
      "2022-04-26 14:10:17.718 | INFO     | __main__:train:49 - Train Step 105\n",
      "2022-04-26 14:10:17.719 | INFO     | __main__:train:51 - mae 2.73e-01 loss 6.99e-02 R 0.017 gap 0.009609920904040337 preds 0.011593939736485481\n",
      "2022-04-26 14:10:17.805 | INFO     | __main__:train:49 - Train Step 106\n",
      "2022-04-26 14:10:17.805 | INFO     | __main__:train:51 - mae 2.72e-01 loss 6.94e-02 R 0.018 gap 0.009632574394345284 preds 0.011647569015622139\n",
      "2022-04-26 14:10:17.884 | INFO     | __main__:train:49 - Train Step 107\n",
      "2022-04-26 14:10:17.885 | INFO     | __main__:train:51 - mae 2.71e-01 loss 6.89e-02 R 0.018 gap 0.009636582806706429 preds 0.011518892832100391\n",
      "2022-04-26 14:10:17.966 | INFO     | __main__:train:49 - Train Step 108\n",
      "2022-04-26 14:10:17.967 | INFO     | __main__:train:51 - mae 2.69e-01 loss 6.84e-02 R 0.018 gap 0.009537285193800926 preds 0.011098450981080532\n",
      "2022-04-26 14:10:18.051 | INFO     | __main__:train:49 - Train Step 109\n",
      "2022-04-26 14:10:18.052 | INFO     | __main__:train:51 - mae 2.68e-01 loss 6.80e-02 R 0.017 gap 0.009587647393345833 preds 0.011131169274449348\n",
      "2022-04-26 14:10:18.383 | INFO     | __main__:train:49 - Train Step 110\n",
      "2022-04-26 14:10:18.384 | INFO     | __main__:train:51 - mae 2.68e-01 loss 6.76e-02 R 0.015 gap 0.009601244702935219 preds 0.011027295142412186\n",
      "2022-04-26 14:10:18.464 | INFO     | __main__:train:49 - Train Step 111\n",
      "2022-04-26 14:10:18.464 | INFO     | __main__:train:51 - mae 2.66e-01 loss 6.71e-02 R 0.014 gap 0.009577344171702862 preds 0.011229349300265312\n",
      "2022-04-26 14:10:18.547 | INFO     | __main__:train:49 - Train Step 112\n",
      "2022-04-26 14:10:18.548 | INFO     | __main__:train:51 - mae 2.65e-01 loss 6.66e-02 R 0.014 gap 0.00960086565464735 preds 0.011019364930689335\n",
      "2022-04-26 14:10:18.631 | INFO     | __main__:train:49 - Train Step 113\n",
      "2022-04-26 14:10:18.632 | INFO     | __main__:train:51 - mae 2.63e-01 loss 6.61e-02 R 0.015 gap 0.009582464583218098 preds 0.01097731851041317\n",
      "2022-04-26 14:10:18.717 | INFO     | __main__:train:49 - Train Step 114\n",
      "2022-04-26 14:10:18.718 | INFO     | __main__:train:51 - mae 2.62e-01 loss 6.56e-02 R 0.015 gap 0.009566936641931534 preds 0.010975656099617481\n",
      "2022-04-26 14:10:18.799 | INFO     | __main__:train:49 - Train Step 115\n",
      "2022-04-26 14:10:18.800 | INFO     | __main__:train:51 - mae 2.61e-01 loss 6.52e-02 R 0.016 gap 0.00958009622991085 preds 0.010784023441374302\n",
      "2022-04-26 14:10:18.888 | INFO     | __main__:train:49 - Train Step 116\n",
      "2022-04-26 14:10:18.889 | INFO     | __main__:train:51 - mae 2.59e-01 loss 6.47e-02 R 0.018 gap 0.009593278169631958 preds 0.01079596858471632\n",
      "2022-04-26 14:10:18.974 | INFO     | __main__:train:49 - Train Step 117\n",
      "2022-04-26 14:10:18.974 | INFO     | __main__:train:51 - mae 2.58e-01 loss 6.43e-02 R 0.018 gap 0.00960321445018053 preds 0.011049509048461914\n",
      "2022-04-26 14:10:19.060 | INFO     | __main__:train:49 - Train Step 118\n",
      "2022-04-26 14:10:19.061 | INFO     | __main__:train:51 - mae 2.57e-01 loss 6.38e-02 R 0.019 gap 0.009586537256836891 preds 0.010664520785212517\n",
      "2022-04-26 14:10:19.142 | INFO     | __main__:train:49 - Train Step 119\n",
      "2022-04-26 14:10:19.143 | INFO     | __main__:train:51 - mae 2.56e-01 loss 6.34e-02 R 0.018 gap 0.009618443436920643 preds 0.010555369779467583\n",
      "2022-04-26 14:10:19.477 | INFO     | __main__:train:49 - Train Step 120\n",
      "2022-04-26 14:10:19.478 | INFO     | __main__:train:51 - mae 2.55e-01 loss 6.29e-02 R 0.017 gap 0.009589490480720997 preds 0.010638768784701824\n",
      "2022-04-26 14:10:19.562 | INFO     | __main__:train:49 - Train Step 121\n",
      "2022-04-26 14:10:19.562 | INFO     | __main__:train:51 - mae 2.54e-01 loss 6.25e-02 R 0.017 gap 0.009589756838977337 preds 0.01027639675885439\n",
      "2022-04-26 14:10:19.645 | INFO     | __main__:train:49 - Train Step 122\n",
      "2022-04-26 14:10:19.646 | INFO     | __main__:train:51 - mae 2.52e-01 loss 6.21e-02 R 0.017 gap 0.009515821933746338 preds 0.010258572176098824\n",
      "2022-04-26 14:10:19.727 | INFO     | __main__:train:49 - Train Step 123\n",
      "2022-04-26 14:10:19.728 | INFO     | __main__:train:51 - mae 2.52e-01 loss 6.18e-02 R 0.017 gap 0.009547124616801739 preds 0.009944731369614601\n",
      "2022-04-26 14:10:19.806 | INFO     | __main__:train:49 - Train Step 124\n",
      "2022-04-26 14:10:19.806 | INFO     | __main__:train:51 - mae 2.50e-01 loss 6.14e-02 R 0.018 gap 0.00959252379834652 preds 0.010276724584400654\n",
      "2022-04-26 14:10:19.889 | INFO     | __main__:train:49 - Train Step 125\n",
      "2022-04-26 14:10:19.890 | INFO     | __main__:train:51 - mae 2.49e-01 loss 6.10e-02 R 0.017 gap 0.009581828489899635 preds 0.009949705563485622\n",
      "2022-04-26 14:10:19.974 | INFO     | __main__:train:49 - Train Step 126\n",
      "2022-04-26 14:10:19.975 | INFO     | __main__:train:51 - mae 2.48e-01 loss 6.06e-02 R 0.017 gap 0.009563826024532318 preds 0.009817786514759064\n",
      "2022-04-26 14:10:20.053 | INFO     | __main__:train:49 - Train Step 127\n",
      "2022-04-26 14:10:20.054 | INFO     | __main__:train:51 - mae 2.47e-01 loss 6.02e-02 R 0.016 gap 0.009600811637938023 preds 0.009904637932777405\n",
      "2022-04-26 14:10:20.135 | INFO     | __main__:train:49 - Train Step 128\n",
      "2022-04-26 14:10:20.136 | INFO     | __main__:train:51 - mae 2.46e-01 loss 5.98e-02 R 0.017 gap 0.009560860693454742 preds 0.009895042516291142\n",
      "2022-04-26 14:10:20.223 | INFO     | __main__:train:49 - Train Step 129\n",
      "2022-04-26 14:10:20.224 | INFO     | __main__:train:51 - mae 2.45e-01 loss 5.94e-02 R 0.017 gap 0.009548060595989227 preds 0.009659970179200172\n",
      "2022-04-26 14:10:20.564 | INFO     | __main__:train:49 - Train Step 130\n",
      "2022-04-26 14:10:20.565 | INFO     | __main__:train:51 - mae 2.44e-01 loss 5.90e-02 R 0.018 gap 0.00959730800241232 preds 0.00971370842307806\n",
      "2022-04-26 14:10:20.650 | INFO     | __main__:train:49 - Train Step 131\n",
      "2022-04-26 14:10:20.651 | INFO     | __main__:train:51 - mae 2.43e-01 loss 5.86e-02 R 0.019 gap 0.00962680671364069 preds 0.009661952964961529\n",
      "2022-04-26 14:10:20.731 | INFO     | __main__:train:49 - Train Step 132\n",
      "2022-04-26 14:10:20.732 | INFO     | __main__:train:51 - mae 2.42e-01 loss 5.83e-02 R 0.020 gap 0.009702910669147968 preds 0.009509450756013393\n",
      "2022-04-26 14:10:20.814 | INFO     | __main__:train:49 - Train Step 133\n",
      "2022-04-26 14:10:20.815 | INFO     | __main__:train:51 - mae 2.41e-01 loss 5.80e-02 R 0.020 gap 0.009677983820438385 preds 0.009633095934987068\n",
      "2022-04-26 14:10:20.899 | INFO     | __main__:train:49 - Train Step 134\n",
      "2022-04-26 14:10:20.900 | INFO     | __main__:train:51 - mae 2.40e-01 loss 5.77e-02 R 0.019 gap 0.009652834385633469 preds 0.00959691684693098\n",
      "2022-04-26 14:10:20.980 | INFO     | __main__:train:49 - Train Step 135\n",
      "2022-04-26 14:10:20.981 | INFO     | __main__:train:51 - mae 2.39e-01 loss 5.73e-02 R 0.020 gap 0.009637216106057167 preds 0.01001318171620369\n",
      "2022-04-26 14:10:21.066 | INFO     | __main__:train:49 - Train Step 136\n",
      "2022-04-26 14:10:21.067 | INFO     | __main__:train:51 - mae 2.39e-01 loss 5.70e-02 R 0.020 gap 0.009621887467801571 preds 0.009970247745513916\n",
      "2022-04-26 14:10:21.150 | INFO     | __main__:train:49 - Train Step 137\n",
      "2022-04-26 14:10:21.151 | INFO     | __main__:train:51 - mae 2.38e-01 loss 5.67e-02 R 0.019 gap 0.009622904472053051 preds 0.009726537391543388\n",
      "2022-04-26 14:10:21.230 | INFO     | __main__:train:49 - Train Step 138\n",
      "2022-04-26 14:10:21.230 | INFO     | __main__:train:51 - mae 2.37e-01 loss 5.64e-02 R 0.019 gap 0.009655148722231388 preds 0.009573211893439293\n",
      "2022-04-26 14:10:21.311 | INFO     | __main__:train:49 - Train Step 139\n",
      "2022-04-26 14:10:21.312 | INFO     | __main__:train:51 - mae 2.36e-01 loss 5.60e-02 R 0.019 gap 0.009639810770750046 preds 0.009674766100943089\n",
      "2022-04-26 14:10:21.652 | INFO     | __main__:train:49 - Train Step 140\n",
      "2022-04-26 14:10:21.653 | INFO     | __main__:train:51 - mae 2.35e-01 loss 5.57e-02 R 0.020 gap 0.00965897087007761 preds 0.009817982092499733\n",
      "2022-04-26 14:10:21.738 | INFO     | __main__:train:49 - Train Step 141\n",
      "2022-04-26 14:10:21.739 | INFO     | __main__:train:51 - mae 2.34e-01 loss 5.55e-02 R 0.020 gap 0.009671556763350964 preds 0.010264662094414234\n",
      "2022-04-26 14:10:21.825 | INFO     | __main__:train:49 - Train Step 142\n",
      "2022-04-26 14:10:21.826 | INFO     | __main__:train:51 - mae 2.34e-01 loss 5.52e-02 R 0.020 gap 0.009724521078169346 preds 0.010321295820176601\n",
      "2022-04-26 14:10:21.910 | INFO     | __main__:train:49 - Train Step 143\n",
      "2022-04-26 14:10:21.911 | INFO     | __main__:train:51 - mae 2.33e-01 loss 5.49e-02 R 0.020 gap 0.009734174236655235 preds 0.010370125994086266\n",
      "2022-04-26 14:10:21.997 | INFO     | __main__:train:49 - Train Step 144\n",
      "2022-04-26 14:10:21.998 | INFO     | __main__:train:51 - mae 2.32e-01 loss 5.46e-02 R 0.019 gap 0.009756321087479591 preds 0.010378766804933548\n",
      "2022-04-26 14:10:22.087 | INFO     | __main__:train:49 - Train Step 145\n",
      "2022-04-26 14:10:22.088 | INFO     | __main__:train:51 - mae 2.31e-01 loss 5.44e-02 R 0.019 gap 0.009722773917019367 preds 0.010313876904547215\n",
      "2022-04-26 14:10:22.174 | INFO     | __main__:train:49 - Train Step 146\n",
      "2022-04-26 14:10:22.175 | INFO     | __main__:train:51 - mae 2.31e-01 loss 5.41e-02 R 0.019 gap 0.009708241559565067 preds 0.010380569845438004\n",
      "2022-04-26 14:10:22.260 | INFO     | __main__:train:49 - Train Step 147\n",
      "2022-04-26 14:10:22.261 | INFO     | __main__:train:51 - mae 2.30e-01 loss 5.39e-02 R 0.019 gap 0.009699338115751743 preds 0.010270942002534866\n",
      "2022-04-26 14:10:22.347 | INFO     | __main__:train:49 - Train Step 148\n",
      "2022-04-26 14:10:22.348 | INFO     | __main__:train:51 - mae 2.29e-01 loss 5.36e-02 R 0.018 gap 0.009682881645858288 preds 0.010121696628630161\n",
      "2022-04-26 14:10:22.433 | INFO     | __main__:train:49 - Train Step 149\n",
      "2022-04-26 14:10:22.434 | INFO     | __main__:train:51 - mae 2.28e-01 loss 5.34e-02 R 0.019 gap 0.009640298783779144 preds 0.010350558906793594\n",
      "2022-04-26 14:10:22.775 | INFO     | __main__:train:49 - Train Step 150\n",
      "2022-04-26 14:10:22.776 | INFO     | __main__:train:51 - mae 2.28e-01 loss 5.31e-02 R 0.019 gap 0.009675467386841774 preds 0.010176105424761772\n",
      "2022-04-26 14:10:22.860 | INFO     | __main__:train:49 - Train Step 151\n",
      "2022-04-26 14:10:22.861 | INFO     | __main__:train:51 - mae 2.27e-01 loss 5.28e-02 R 0.019 gap 0.009714575484395027 preds 0.010424256324768066\n",
      "2022-04-26 14:10:22.946 | INFO     | __main__:train:49 - Train Step 152\n",
      "2022-04-26 14:10:22.947 | INFO     | __main__:train:51 - mae 2.26e-01 loss 5.26e-02 R 0.019 gap 0.009717931970953941 preds 0.010137414559721947\n",
      "2022-04-26 14:10:23.033 | INFO     | __main__:train:49 - Train Step 153\n",
      "2022-04-26 14:10:23.034 | INFO     | __main__:train:51 - mae 2.26e-01 loss 5.24e-02 R 0.019 gap 0.009741765446960926 preds 0.010251044295728207\n",
      "2022-04-26 14:10:23.121 | INFO     | __main__:train:49 - Train Step 154\n",
      "2022-04-26 14:10:23.122 | INFO     | __main__:train:51 - mae 2.25e-01 loss 5.22e-02 R 0.018 gap 0.009750321507453918 preds 0.010261605493724346\n",
      "2022-04-26 14:10:23.210 | INFO     | __main__:train:49 - Train Step 155\n",
      "2022-04-26 14:10:23.211 | INFO     | __main__:train:51 - mae 2.24e-01 loss 5.19e-02 R 0.017 gap 0.00972975604236126 preds 0.010271390900015831\n",
      "2022-04-26 14:10:23.298 | INFO     | __main__:train:49 - Train Step 156\n",
      "2022-04-26 14:10:23.298 | INFO     | __main__:train:51 - mae 2.24e-01 loss 5.17e-02 R 0.016 gap 0.009758283384144306 preds 0.010141866281628609\n",
      "2022-04-26 14:10:23.386 | INFO     | __main__:train:49 - Train Step 157\n",
      "2022-04-26 14:10:23.387 | INFO     | __main__:train:51 - mae 2.23e-01 loss 5.14e-02 R 0.016 gap 0.009754867292940617 preds 0.01008358784019947\n",
      "2022-04-26 14:10:23.473 | INFO     | __main__:train:49 - Train Step 158\n",
      "2022-04-26 14:10:23.474 | INFO     | __main__:train:51 - mae 2.22e-01 loss 5.12e-02 R 0.016 gap 0.009776972234249115 preds 0.009923805482685566\n",
      "2022-04-26 14:10:23.560 | INFO     | __main__:train:49 - Train Step 159\n",
      "2022-04-26 14:10:23.561 | INFO     | __main__:train:51 - mae 2.21e-01 loss 5.09e-02 R 0.017 gap 0.009772001765668392 preds 0.009846129454672337\n",
      "2022-04-26 14:10:23.898 | INFO     | __main__:train:49 - Train Step 160\n",
      "2022-04-26 14:10:23.899 | INFO     | __main__:train:51 - mae 2.20e-01 loss 5.06e-02 R 0.017 gap 0.009782286360859871 preds 0.010086679831147194\n",
      "2022-04-26 14:10:23.983 | INFO     | __main__:train:49 - Train Step 161\n",
      "2022-04-26 14:10:23.984 | INFO     | __main__:train:51 - mae 2.19e-01 loss 5.04e-02 R 0.018 gap 0.009763144887983799 preds 0.010165579617023468\n",
      "2022-04-26 14:10:24.071 | INFO     | __main__:train:49 - Train Step 162\n",
      "2022-04-26 14:10:24.072 | INFO     | __main__:train:51 - mae 2.19e-01 loss 5.01e-02 R 0.017 gap 0.009775874204933643 preds 0.010108229704201221\n",
      "2022-04-26 14:10:24.160 | INFO     | __main__:train:49 - Train Step 163\n",
      "2022-04-26 14:10:24.161 | INFO     | __main__:train:51 - mae 2.18e-01 loss 4.99e-02 R 0.017 gap 0.009788289666175842 preds 0.010020322166383266\n",
      "2022-04-26 14:10:24.250 | INFO     | __main__:train:49 - Train Step 164\n",
      "2022-04-26 14:10:24.251 | INFO     | __main__:train:51 - mae 2.17e-01 loss 4.96e-02 R 0.018 gap 0.009791215881705284 preds 0.010007106699049473\n",
      "2022-04-26 14:10:24.339 | INFO     | __main__:train:49 - Train Step 165\n",
      "2022-04-26 14:10:24.340 | INFO     | __main__:train:51 - mae 2.16e-01 loss 4.94e-02 R 0.018 gap 0.009774457663297653 preds 0.009961274452507496\n",
      "2022-04-26 14:10:24.428 | INFO     | __main__:train:49 - Train Step 166\n",
      "2022-04-26 14:10:24.429 | INFO     | __main__:train:51 - mae 2.16e-01 loss 4.91e-02 R 0.018 gap 0.00977952592074871 preds 0.010008004494011402\n",
      "2022-04-26 14:10:24.516 | INFO     | __main__:train:49 - Train Step 167\n",
      "2022-04-26 14:10:24.517 | INFO     | __main__:train:51 - mae 2.15e-01 loss 4.89e-02 R 0.019 gap 0.009779942221939564 preds 0.010007587261497974\n",
      "2022-04-26 14:10:24.609 | INFO     | __main__:train:49 - Train Step 168\n",
      "2022-04-26 14:10:24.610 | INFO     | __main__:train:51 - mae 2.14e-01 loss 4.87e-02 R 0.019 gap 0.00979638658463955 preds 0.010178283788263798\n",
      "2022-04-26 14:10:24.693 | INFO     | __main__:train:49 - Train Step 169\n",
      "2022-04-26 14:10:24.693 | INFO     | __main__:train:51 - mae 2.14e-01 loss 4.85e-02 R 0.019 gap 0.009815951809287071 preds 0.010378360748291016\n",
      "2022-04-26 14:10:25.036 | INFO     | __main__:train:49 - Train Step 170\n",
      "2022-04-26 14:10:25.037 | INFO     | __main__:train:51 - mae 2.13e-01 loss 4.83e-02 R 0.018 gap 0.009850293397903442 preds 0.010336294770240784\n",
      "2022-04-26 14:10:25.126 | INFO     | __main__:train:49 - Train Step 171\n",
      "2022-04-26 14:10:25.127 | INFO     | __main__:train:51 - mae 2.12e-01 loss 4.81e-02 R 0.018 gap 0.009887916035950184 preds 0.010254980064928532\n",
      "2022-04-26 14:10:25.214 | INFO     | __main__:train:49 - Train Step 172\n",
      "2022-04-26 14:10:25.215 | INFO     | __main__:train:51 - mae 2.12e-01 loss 4.79e-02 R 0.019 gap 0.009924301877617836 preds 0.010168427601456642\n",
      "2022-04-26 14:10:25.307 | INFO     | __main__:train:49 - Train Step 173\n",
      "2022-04-26 14:10:25.308 | INFO     | __main__:train:51 - mae 2.11e-01 loss 4.77e-02 R 0.017 gap 0.009908402338624 preds 0.010462847538292408\n",
      "2022-04-26 14:10:25.398 | INFO     | __main__:train:49 - Train Step 174\n",
      "2022-04-26 14:10:25.399 | INFO     | __main__:train:51 - mae 2.11e-01 loss 4.75e-02 R 0.017 gap 0.009928010404109955 preds 0.010542797856032848\n",
      "2022-04-26 14:10:25.489 | INFO     | __main__:train:49 - Train Step 175\n",
      "2022-04-26 14:10:25.490 | INFO     | __main__:train:51 - mae 2.10e-01 loss 4.73e-02 R 0.016 gap 0.009947075508534908 preds 0.010646476410329342\n",
      "2022-04-26 14:10:25.578 | INFO     | __main__:train:49 - Train Step 176\n",
      "2022-04-26 14:10:25.579 | INFO     | __main__:train:51 - mae 2.10e-01 loss 4.71e-02 R 0.016 gap 0.009925562888383865 preds 0.010752552188932896\n",
      "2022-04-26 14:10:25.671 | INFO     | __main__:train:49 - Train Step 177\n",
      "2022-04-26 14:10:25.672 | INFO     | __main__:train:51 - mae 2.09e-01 loss 4.69e-02 R 0.016 gap 0.009967883117496967 preds 0.010567664168775082\n",
      "2022-04-26 14:10:25.761 | INFO     | __main__:train:49 - Train Step 178\n",
      "2022-04-26 14:10:25.762 | INFO     | __main__:train:51 - mae 2.09e-01 loss 4.68e-02 R 0.015 gap 0.009957835078239441 preds 0.010672615841031075\n",
      "2022-04-26 14:10:47.243 | INFO     | __main__:test:57 - Test epoch 1\n",
      "2022-04-26 14:10:47.244 | INFO     | __main__:test:59 - mae 1.12e-01 loss 1.15e-02 R -0.010 l_test 9.30e-01 l_train 9.22e-01 \n"
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:33:11.731 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"\"#\"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(model_path).to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim if data_type==\"cs\" else xtrain_feat_dim,\n",
    "        task='seq',\n",
    "    ).to(device)\n",
    "if data_type == \"ds\":\n",
    "    h.init_hidden()\n",
    "\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        l_test, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        l_train, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:34:40.625 | INFO     | __main__:<module>:9 - Test 0.0631 +- 0.0052\n",
      "2022-04-26 14:34:40.626 | INFO     | __main__:<module>:10 - Train 0.0628 +- 0.0052\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.1396\n",
      "MSE for Dimension 2: 0.1529\n",
      "MSE for Dimension 3: 0.1442\n",
      "MSE for Dimension 4: 0.1551\n",
      "MSE for Dimension 5: 0.1397\n",
      "MSE for Dimension 6: 0.1332\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.01      0.02      6791\n",
      "         1.0       0.24      0.40      0.30      2227\n",
      "         2.0       0.14      0.27      0.19      1755\n",
      "         3.0       0.02      0.30      0.04       299\n",
      "\n",
      "    accuracy                           0.14     11072\n",
      "   macro avg       0.31      0.24      0.14     11072\n",
      "weighted avg       0.57      0.14      0.11     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.78      0.76      8156\n",
      "         1.0       0.08      0.00      0.00       469\n",
      "         2.0       0.08      0.22      0.12       790\n",
      "         3.0       0.08      0.01      0.02      1657\n",
      "\n",
      "    accuracy                           0.60     11072\n",
      "   macro avg       0.24      0.26      0.23     11072\n",
      "weighted avg       0.57      0.60      0.57     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.00      0.00      2706\n",
      "         1.0       0.46      0.93      0.61      4977\n",
      "         2.0       0.17      0.10      0.13      1293\n",
      "         3.0       0.30      0.03      0.06      2096\n",
      "\n",
      "    accuracy                           0.43     11072\n",
      "   macro avg       0.48      0.26      0.20     11072\n",
      "weighted avg       0.53      0.43      0.30     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.31      0.35      4851\n",
      "         1.0       0.14      0.70      0.23      1442\n",
      "         2.0       0.00      0.00      0.00       561\n",
      "         3.0       1.00      0.00      0.00      4218\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.38      0.25      0.14     11072\n",
      "weighted avg       0.57      0.23      0.18     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

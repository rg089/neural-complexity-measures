{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 10\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, test_x_pred, train_x_pred, train_x_y_pred, gap, train_loss):\n",
    "        if not hasattr(self, \"test_x_pred\"): # if adding the first sample\n",
    "            self.test_x_pred = test_x_pred\n",
    "            self.train_x_pred = train_x_pred\n",
    "            self.train_x_y_pred = train_x_y_pred\n",
    "            self.gap = gap\n",
    "            self.train_loss = train_loss\n",
    "        else:\n",
    "            self.test_x_pred = torch.cat([self.test_x_pred, test_x_pred], dim=0)\n",
    "            self.train_x_pred = torch.cat([self.train_x_pred, train_x_pred], dim=0)\n",
    "            self.train_x_y_pred = torch.cat([self.train_x_y_pred, train_x_y_pred], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.train_loss = torch.cat([self.train_loss, train_loss], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.test_x_pred.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"test_x_pred\": self.test_x_pred[idxs].to(device),\n",
    "            \"train_x_pred\": self.train_x_pred[idxs].to(device),\n",
    "            \"train_x_y_pred\": self.train_x_y_pred[idxs].to(device),\n",
    "            \"tr_loss\": self.train_loss[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_value = mlp(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_query_key = mlp(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"train_x_y_pred\"][:, :xtrain_dim], inputs[\"train_x_y_pred\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"train_x_y_pred\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_query_key(inputs[\"test_x_pred\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_query_key(inputs[\"train_x_pred\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_value(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = mlp(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = mlp(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining train_loss to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())*5.0/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, train_loss = get_task_loss(preds_train, y_train, crit_reg, crit_cls)\n",
    "\n",
    "        test_x_pred = torch.cat([x_test, preds_test], dim=-1)\n",
    "        train_x_pred = torch.cat([x_train, preds_train], dim=-1)\n",
    "        train_x_y_pred = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"test_x_pred\": test_x_pred, \"train_x_pred\": train_x_pred, \"train_x_y_pred\": train_x_y_pred, \"tr_loss\": train_loss}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, test_loss = get_task_loss(preds_test, y_test, crit_reg, crit_cls)\n",
    "\n",
    "        # train_loss and test_loss are used to compute the gap\n",
    "        \n",
    "        gap = test_loss.mean(-1) - train_loss.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                test_x_pred=test_x_pred.cpu().detach(),\n",
    "                train_x_pred=train_x_pred.cpu().detach(),\n",
    "                train_x_y_pred=train_x_y_pred.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                train_loss=train_loss.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, train_loss = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, test_loss = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = test_loss.mean(-1) - train_loss.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"test_loss\": [test_loss.mean(-1).detach().cpu()],\n",
    "                    \"train_loss\": [train_loss.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"test_loss\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"train_loss\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"test_loss {mean_l_test:.2e} train_loss {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"test_loss\": mean_l_test.item(),\n",
    "        \"train_loss\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 16:57:50.686 | INFO     | __main__:<module>:19 - Populate time: 1.0289090853184462\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.test_x_pred.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:45:12.302 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"\"#\"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(\"/home/himanshus/experiments/neural-complexity-measures/result/best_model_test_cs_with_nc_base_1.ckpt\").to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, train_loss = get_task_loss(preds_train, y_train, crit_reg, crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            test_x_pred = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            train_x_pred = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            train_x_y_pred = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"test_x_pred\": test_x_pred, \"train_x_pred\": train_x_pred, \"train_x_y_pred\": train_x_y_pred, \"tr_loss\": train_loss}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step, alpha=1)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        test_loss, _ = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        losses_test.append(test_loss.item())\n",
    "        \n",
    "        train_loss, _ = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "        \n",
    "        losses_train.append(train_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:45:31.255 | INFO     | __main__:<module>:9 - Test 0.0176 +- 0.0015\n",
      "2022-04-28 12:45:31.256 | INFO     | __main__:<module>:10 - Train 0.0018 +- 0.0002\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exit()\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.0532\n",
      "MSE for Dimension 2: 0.0396\n",
      "MSE for Dimension 3: 0.0351\n",
      "MSE for Dimension 4: 0.0759\n",
      "MSE for Dimension 5: 0.0512\n",
      "MSE for Dimension 6: 0.0527\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.45      0.54      6826\n",
      "         1.0       0.30      0.39      0.34      2121\n",
      "         2.0       0.19      0.38      0.25      1717\n",
      "         3.0       0.00      0.00      0.00       408\n",
      "\n",
      "    accuracy                           0.41     11072\n",
      "   macro avg       0.29      0.31      0.28     11072\n",
      "weighted avg       0.51      0.41      0.44     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.67      0.72      8096\n",
      "         1.0       0.24      0.24      0.24       469\n",
      "         2.0       0.00      0.00      0.00       790\n",
      "         3.0       0.15      0.18      0.16      1717\n",
      "\n",
      "    accuracy                           0.52     11072\n",
      "   macro avg       0.29      0.27      0.28     11072\n",
      "weighted avg       0.60      0.52      0.56     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.13      0.05      0.07      2716\n",
      "         1.0       0.40      0.42      0.41      4925\n",
      "         2.0       0.05      0.08      0.06      1293\n",
      "         3.0       0.16      0.20      0.17      2138\n",
      "\n",
      "    accuracy                           0.24     11072\n",
      "   macro avg       0.18      0.19      0.18     11072\n",
      "weighted avg       0.24      0.24      0.24     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.41      0.49      4859\n",
      "         1.0       0.23      0.31      0.27      1442\n",
      "         2.0       0.00      0.00      0.00       561\n",
      "         3.0       0.44      0.53      0.48      4210\n",
      "\n",
      "    accuracy                           0.42     11072\n",
      "   macro avg       0.32      0.31      0.31     11072\n",
      "weighted avg       0.46      0.42      0.43     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6387572d3ba60263f2472b530ce49454bee9bd13656fbfcf29efcd586b712758"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

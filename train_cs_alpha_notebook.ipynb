{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc_alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates for the task learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "t = current step\n",
    "\n",
    "$\\upsilon$ (Uncertainty) = $\\sigma(NC(...))$ \n",
    "\n",
    "[Uncertainty estimated by the standard deviation of the Monte Carlo dropout simulations] \n",
    "\n",
    "$$\\lambda = max(0, min(1, (1 - \\upsilon^{\\lfloor{t/1000 + 1}\\rfloor})))$$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nc_weight(model, meta_batch, step, num_simulations=15, alpha=0.8, kappa=1000):\n",
    "    \"\"\"\n",
    "    Computes the weight of the neural complexity loss by using the standard deviation of the model outputs using MC dropout simulations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for simulation in range(num_simulations):\n",
    "            output = model(meta_batch).squeeze().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs) # Array of shape (num_simulations, batch_size)\n",
    "    uncertainty = np.mean(np.std(outputs, axis=0)) \n",
    "\n",
    "    exponent = 1 + np.floor(step / kappa)\n",
    "    nc_weight = 1 - alpha*np.power(uncertainty, exponent)\n",
    "    nc_weight = np.clip(nc_weight, 0, 1)\n",
    "    return nc_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, global_step)\n",
    "            logger.info(f\"NC Weight: {nc_weight_}\")\n",
    "            h_loss += nc_regularization *  nc_weight_\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2022-04-26 23:28:40.673 | INFO     | __main__:<cell line: 19>:19 - Populate time: 0.038077540999893245\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-26 23:28:45.644 | INFO     | __main__:<cell line: 3>:5 - Bank size: 14880\n",
      "2022-04-26 23:28:46.030 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-26 23:28:46.031 | INFO     | __main__:train:48 - mae 6.85e-01 loss 3.15e-01 R 0.137 gap -0.004025174304842949 preds 0.019302912056446075\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-26 23:28:46.288 | INFO     | __main__:train:48 - mae 6.96e-01 loss 3.22e-01 R 0.075 gap -0.010805429890751839 preds 0.02897205762565136\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-26 23:28:46.588 | INFO     | __main__:train:48 - mae 6.25e-01 loss 2.66e-01 R -0.054 gap -0.012693467549979687 preds 0.030039602890610695\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-26 23:28:46.825 | INFO     | __main__:train:48 - mae 5.75e-01 loss 2.35e-01 R -0.043 gap -0.01496695727109909 preds 0.034109052270650864\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-26 23:28:47.101 | INFO     | __main__:train:48 - mae 5.59e-01 loss 2.23e-01 R -0.080 gap -0.007366997189819813 preds 0.03248163312673569\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-26 23:28:47.434 | INFO     | __main__:train:48 - mae 5.38e-01 loss 2.15e-01 R -0.026 gap -0.0058245365507900715 preds 0.024944690987467766\n",
      "2022-04-26 23:28:47.751 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-26 23:28:47.752 | INFO     | __main__:train:48 - mae 5.31e-01 loss 2.10e-01 R 0.005 gap -0.0011595458490774035 preds 0.02178914286196232\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-26 23:28:48.091 | INFO     | __main__:train:48 - mae 5.19e-01 loss 2.03e-01 R -0.028 gap 0.0004147551953792572 preds 0.017345435917377472\n",
      "2022-04-26 23:28:48.423 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-26 23:28:48.424 | INFO     | __main__:train:48 - mae 5.04e-01 loss 1.94e-01 R 0.003 gap 0.0018580686300992966 preds 0.01597616635262966\n",
      "2022-04-26 23:28:50.201 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6386142015457152\n",
      "2022-04-26 23:28:52.018 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6382613897323608\n",
      "2022-04-26 23:28:53.561 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6459709167480469\n",
      "2022-04-26 23:28:55.260 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6427107334136963\n",
      "2022-04-26 23:28:56.804 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6569726467132568\n",
      "2022-04-26 23:28:57.254 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-26 23:28:57.255 | INFO     | __main__:train:48 - mae 5.00e-01 loss 1.89e-01 R 0.000 gap 0.002617710968479514 preds 0.014508819207549095\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-26 23:28:57.564 | INFO     | __main__:train:48 - mae 4.85e-01 loss 1.81e-01 R 0.015 gap -0.0011261024046689272 preds 0.014864945784211159\n",
      "2022-04-26 23:28:57.831 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-26 23:28:57.832 | INFO     | __main__:train:48 - mae 4.77e-01 loss 1.77e-01 R 0.017 gap -0.0006178456242196262 preds 0.011762782000005245\n",
      "2022-04-26 23:28:58.081 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-26 23:28:58.082 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.75e-01 R 0.013 gap -0.0005259691388346255 preds 0.0026185547467321157\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-26 23:28:58.600 | INFO     | __main__:train:48 - mae 4.73e-01 loss 1.76e-01 R -0.010 gap -1.193636217067251e-05 preds 0.0014807922998443246\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-26 23:28:58.904 | INFO     | __main__:train:48 - mae 4.69e-01 loss 1.74e-01 R 0.004 gap -0.0014606881886720657 preds -0.0007424215436913073\n",
      "2022-04-26 23:28:59.123 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-26 23:28:59.124 | INFO     | __main__:train:48 - mae 4.65e-01 loss 1.72e-01 R 0.018 gap -0.0014548121253028512 preds -0.003664351999759674\n",
      "2022-04-26 23:28:59.404 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-26 23:28:59.405 | INFO     | __main__:train:48 - mae 4.60e-01 loss 1.69e-01 R 0.014 gap -0.000630356720648706 preds -0.004320953041315079\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-26 23:28:59.646 | INFO     | __main__:train:48 - mae 4.61e-01 loss 1.70e-01 R 0.002 gap -0.0007210546755231917 preds -0.0035548703745007515\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-26 23:28:59.913 | INFO     | __main__:train:48 - mae 4.52e-01 loss 1.66e-01 R 0.000 gap -0.0009869002969935536 preds -0.003279823111370206\n",
      "2022-04-26 23:29:01.421 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7162060737609863\n",
      "2022-04-26 23:29:03.004 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.716486644744873\n",
      "2022-04-26 23:29:04.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.711784839630127\n",
      "2022-04-26 23:29:06.106 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.6997286319732665\n",
      "2022-04-26 23:29:07.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7218161582946777\n",
      "2022-04-26 23:29:08.133 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-26 23:29:08.134 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.019 gap -0.00043950200779363513 preds -0.0023840710055083036\n",
      "2022-04-26 23:29:08.349 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-26 23:29:08.350 | INFO     | __main__:train:48 - mae 4.46e-01 loss 1.62e-01 R 0.033 gap -0.0014850901206955314 preds -0.0029290500096976757\n",
      "2022-04-26 23:29:08.621 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-26 23:29:08.622 | INFO     | __main__:train:48 - mae 4.47e-01 loss 1.62e-01 R 0.037 gap -0.0017890621675178409 preds -0.0030872998759150505\n",
      "2022-04-26 23:29:08.885 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-26 23:29:08.886 | INFO     | __main__:train:48 - mae 4.41e-01 loss 1.59e-01 R 0.032 gap -0.0016951110446825624 preds -0.004116981290280819\n",
      "2022-04-26 23:29:09.168 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-26 23:29:09.169 | INFO     | __main__:train:48 - mae 4.37e-01 loss 1.57e-01 R 0.029 gap -0.0022651583421975374 preds -0.004774647299200296\n",
      "2022-04-26 23:29:09.554 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-26 23:29:09.555 | INFO     | __main__:train:48 - mae 4.36e-01 loss 1.57e-01 R 0.021 gap -0.0028953966684639454 preds -0.0020634413231164217\n",
      "2022-04-26 23:29:09.830 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-26 23:29:09.831 | INFO     | __main__:train:48 - mae 4.34e-01 loss 1.55e-01 R 0.022 gap -0.0030436008237302303 preds -0.0009449193021282554\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-26 23:29:10.083 | INFO     | __main__:train:48 - mae 4.32e-01 loss 1.53e-01 R 0.023 gap -0.0020179059356451035 preds -0.0013306555338203907\n",
      "2022-04-26 23:29:10.342 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-26 23:29:10.343 | INFO     | __main__:train:48 - mae 4.30e-01 loss 1.52e-01 R 0.027 gap -0.0026106475852429867 preds -0.0003441407170612365\n",
      "2022-04-26 23:29:10.570 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-26 23:29:10.571 | INFO     | __main__:train:48 - mae 4.28e-01 loss 1.51e-01 R 0.034 gap -0.0023085696157068014 preds 0.0003318673698231578\n",
      "2022-04-26 23:29:11.918 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7684072971343994\n",
      "2022-04-26 23:29:13.530 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7504154920578003\n",
      "2022-04-26 23:29:15.210 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.754740858078003\n",
      "2022-04-26 23:29:16.868 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7520849704742432\n",
      "2022-04-26 23:29:18.521 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7632457494735718\n",
      "2022-04-26 23:29:18.995 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-26 23:29:18.996 | INFO     | __main__:train:48 - mae 4.31e-01 loss 1.52e-01 R 0.024 gap -0.001982692629098892 preds 0.00198011239990592\n",
      "2022-04-26 23:29:19.229 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-26 23:29:19.230 | INFO     | __main__:train:48 - mae 4.29e-01 loss 1.51e-01 R 0.023 gap -0.002047610003501177 preds 0.0009437102708034217\n",
      "2022-04-26 23:29:19.479 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-26 23:29:19.480 | INFO     | __main__:train:48 - mae 4.26e-01 loss 1.48e-01 R 0.020 gap -0.0018432892393320799 preds 0.0012483163736760616\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-26 23:29:19.827 | INFO     | __main__:train:48 - mae 4.22e-01 loss 1.46e-01 R 0.019 gap -0.0009980808245018125 preds 0.0005702159833163023\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-26 23:29:20.097 | INFO     | __main__:train:48 - mae 4.20e-01 loss 1.44e-01 R 0.027 gap -0.0013026399537920952 preds 0.0001504934043623507\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-26 23:29:20.362 | INFO     | __main__:train:48 - mae 4.18e-01 loss 1.43e-01 R 0.028 gap -0.0016899961046874523 preds -0.0003625307872425765\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-26 23:29:20.624 | INFO     | __main__:train:48 - mae 4.14e-01 loss 1.40e-01 R 0.033 gap -0.001719743013381958 preds 9.594899165676907e-05\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-26 23:29:20.917 | INFO     | __main__:train:48 - mae 4.12e-01 loss 1.39e-01 R 0.035 gap -0.0016408524243161082 preds -0.0009625493548810482\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-26 23:29:21.197 | INFO     | __main__:train:48 - mae 4.08e-01 loss 1.36e-01 R 0.035 gap -0.0019434017594903708 preds -0.000601332460064441\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-26 23:29:21.455 | INFO     | __main__:train:48 - mae 4.06e-01 loss 1.35e-01 R 0.036 gap -0.002552586840465665 preds -0.0008047166047617793\n",
      "2022-04-26 23:29:22.900 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.7969206809997559\n",
      "2022-04-26 23:29:24.598 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8025416374206543\n",
      "2022-04-26 23:29:26.198 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8033116340637207\n",
      "2022-04-26 23:29:27.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8072627902030944\n",
      "2022-04-26 23:29:29.805 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.794762372970581\n",
      "2022-04-26 23:29:30.285 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-26 23:29:30.286 | INFO     | __main__:train:48 - mae 4.03e-01 loss 1.33e-01 R 0.041 gap -0.0029393809381872416 preds -0.001004834077320993\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-26 23:29:30.591 | INFO     | __main__:train:48 - mae 4.00e-01 loss 1.31e-01 R 0.040 gap -0.0031229194719344378 preds -0.0006420561694540083\n",
      "2022-04-26 23:29:30.824 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-26 23:29:30.825 | INFO     | __main__:train:48 - mae 3.96e-01 loss 1.29e-01 R 0.042 gap -0.0029769304674118757 preds -0.0015712390886619687\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-26 23:29:31.065 | INFO     | __main__:train:48 - mae 3.93e-01 loss 1.27e-01 R 0.043 gap -0.0030405137222260237 preds -0.00230604549869895\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-26 23:29:31.363 | INFO     | __main__:train:48 - mae 3.90e-01 loss 1.26e-01 R 0.042 gap -0.0030803720001131296 preds -0.0023205080069601536\n",
      "2022-04-26 23:29:31.632 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-26 23:29:31.633 | INFO     | __main__:train:48 - mae 3.89e-01 loss 1.25e-01 R 0.039 gap -0.0035057845525443554 preds -0.0012993252603337169\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-26 23:29:31.927 | INFO     | __main__:train:48 - mae 3.85e-01 loss 1.23e-01 R 0.040 gap -0.0032601093407720327 preds -0.0015362835256382823\n",
      "2022-04-26 23:29:32.155 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-26 23:29:32.156 | INFO     | __main__:train:48 - mae 3.82e-01 loss 1.21e-01 R 0.040 gap -0.0038092739414423704 preds -0.0018552580149844289\n",
      "2022-04-26 23:29:32.416 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-26 23:29:32.417 | INFO     | __main__:train:48 - mae 3.79e-01 loss 1.20e-01 R 0.034 gap -0.003862932324409485 preds -0.002573140896856785\n",
      "2022-04-26 23:29:32.671 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-26 23:29:32.672 | INFO     | __main__:train:48 - mae 3.78e-01 loss 1.19e-01 R 0.035 gap -0.0036917461548000574 preds -0.002876740414649248\n",
      "2022-04-26 23:29:34.104 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8218601465225219\n",
      "2022-04-26 23:29:36.135 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.827106773853302\n",
      "2022-04-26 23:29:37.728 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8279574751853943\n",
      "2022-04-26 23:29:39.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8166125059127808\n",
      "2022-04-26 23:29:41.016 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8080407857894898\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-26 23:29:41.442 | INFO     | __main__:train:48 - mae 3.76e-01 loss 1.18e-01 R 0.037 gap -0.003657860215753317 preds -0.0033888102043420076\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-26 23:29:41.689 | INFO     | __main__:train:48 - mae 3.74e-01 loss 1.17e-01 R 0.042 gap -0.004024251829832792 preds -0.003796732984483242\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-26 23:29:41.932 | INFO     | __main__:train:48 - mae 3.70e-01 loss 1.15e-01 R 0.044 gap -0.0037747970782220364 preds -0.004655835218727589\n",
      "2022-04-26 23:29:42.197 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-26 23:29:42.198 | INFO     | __main__:train:48 - mae 3.68e-01 loss 1.14e-01 R 0.050 gap -0.0038675193209201097 preds -0.004424822051078081\n",
      "2022-04-26 23:29:42.425 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-26 23:29:42.426 | INFO     | __main__:train:48 - mae 3.65e-01 loss 1.12e-01 R 0.052 gap -0.004099351819604635 preds -0.004930342081934214\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-26 23:29:42.662 | INFO     | __main__:train:48 - mae 3.63e-01 loss 1.11e-01 R 0.051 gap -0.004149241838604212 preds -0.005062129348516464\n",
      "2022-04-26 23:29:42.951 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-26 23:29:42.952 | INFO     | __main__:train:48 - mae 3.61e-01 loss 1.10e-01 R 0.051 gap -0.0037791640497744083 preds -0.0045526460744440556\n",
      "2022-04-26 23:29:43.215 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-26 23:29:43.216 | INFO     | __main__:train:48 - mae 3.59e-01 loss 1.09e-01 R 0.049 gap -0.003672423306852579 preds -0.004477016627788544\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-26 23:29:43.505 | INFO     | __main__:train:48 - mae 3.57e-01 loss 1.08e-01 R 0.048 gap -0.003460554638877511 preds -0.0053259399719536304\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-26 23:29:43.783 | INFO     | __main__:train:48 - mae 3.54e-01 loss 1.07e-01 R 0.050 gap -0.0036014181096106768 preds -0.00492506567388773\n",
      "2022-04-26 23:29:45.225 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8435122013092041\n",
      "2022-04-26 23:29:46.797 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8467915534973145\n",
      "2022-04-26 23:29:48.321 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8464855790138245\n",
      "2022-04-26 23:29:49.978 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8472644090652466\n",
      "2022-04-26 23:29:51.511 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8503743171691894\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-26 23:29:51.940 | INFO     | __main__:train:48 - mae 3.51e-01 loss 1.05e-01 R 0.052 gap -0.00354475318454206 preds -0.00423211045563221\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-26 23:29:52.151 | INFO     | __main__:train:48 - mae 3.49e-01 loss 1.04e-01 R 0.050 gap -0.003419148735702038 preds -0.004200519062578678\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-26 23:29:52.433 | INFO     | __main__:train:48 - mae 3.46e-01 loss 1.03e-01 R 0.051 gap -0.003685764269903302 preds -0.0038328603841364384\n",
      "2022-04-26 23:29:52.681 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-26 23:29:52.682 | INFO     | __main__:train:48 - mae 3.44e-01 loss 1.02e-01 R 0.051 gap -0.0035238810814917088 preds -0.004039149731397629\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-26 23:29:52.950 | INFO     | __main__:train:48 - mae 3.42e-01 loss 1.01e-01 R 0.053 gap -0.0032744198106229305 preds -0.004004001151770353\n",
      "2022-04-26 23:29:53.168 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-26 23:29:53.169 | INFO     | __main__:train:48 - mae 3.40e-01 loss 9.99e-02 R 0.055 gap -0.003016298869624734 preds -0.0038254880346357822\n",
      "2022-04-26 23:29:53.408 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-26 23:29:53.409 | INFO     | __main__:train:48 - mae 3.38e-01 loss 9.91e-02 R 0.053 gap -0.00289788655936718 preds -0.004548549652099609\n",
      "2022-04-26 23:29:53.632 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-26 23:29:53.633 | INFO     | __main__:train:48 - mae 3.37e-01 loss 9.84e-02 R 0.053 gap -0.0026992682833224535 preds -0.0048513710498809814\n",
      "2022-04-26 23:29:53.868 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-26 23:29:53.869 | INFO     | __main__:train:48 - mae 3.35e-01 loss 9.74e-02 R 0.054 gap -0.002827448770403862 preds -0.004853168502449989\n",
      "2022-04-26 23:29:54.132 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-26 23:29:54.133 | INFO     | __main__:train:48 - mae 3.34e-01 loss 9.68e-02 R 0.054 gap -0.00285080773755908 preds -0.0041443463414907455\n",
      "2022-04-26 23:29:55.608 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8507198095321655\n",
      "2022-04-26 23:29:57.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8582750678062439\n",
      "2022-04-26 23:29:58.972 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8588191032409668\n",
      "2022-04-26 23:30:00.667 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8563930034637451\n",
      "2022-04-26 23:30:02.181 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8534034252166748\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-26 23:30:02.618 | INFO     | __main__:train:48 - mae 3.32e-01 loss 9.58e-02 R 0.057 gap -0.0029002444352954626 preds -0.004466008394956589\n",
      "2022-04-26 23:30:02.875 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-26 23:30:02.876 | INFO     | __main__:train:48 - mae 3.30e-01 loss 9.50e-02 R 0.059 gap -0.0030128867365419865 preds -0.004153374116867781\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-26 23:30:03.135 | INFO     | __main__:train:48 - mae 3.29e-01 loss 9.42e-02 R 0.056 gap -0.002689918503165245 preds -0.004717262461781502\n",
      "2022-04-26 23:30:03.412 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-26 23:30:03.413 | INFO     | __main__:train:48 - mae 3.27e-01 loss 9.33e-02 R 0.057 gap -0.0028525779489427805 preds -0.0051404680125415325\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-26 23:30:03.682 | INFO     | __main__:train:48 - mae 3.25e-01 loss 9.28e-02 R 0.058 gap -0.002608650829643011 preds -0.005139767192304134\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-26 23:30:03.957 | INFO     | __main__:train:48 - mae 3.24e-01 loss 9.21e-02 R 0.057 gap -0.0026324244681745768 preds -0.0048909662291407585\n",
      "2022-04-26 23:30:04.187 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-26 23:30:04.188 | INFO     | __main__:train:48 - mae 3.21e-01 loss 9.12e-02 R 0.058 gap -0.0026066817808896303 preds -0.004703180864453316\n",
      "2022-04-26 23:30:04.450 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-26 23:30:04.451 | INFO     | __main__:train:48 - mae 3.19e-01 loss 9.04e-02 R 0.061 gap -0.0024984076153486967 preds -0.0044120848178863525\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-26 23:30:04.692 | INFO     | __main__:train:48 - mae 3.17e-01 loss 8.96e-02 R 0.059 gap -0.0025261850096285343 preds -0.004197473172098398\n",
      "2022-04-26 23:30:04.948 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-26 23:30:04.949 | INFO     | __main__:train:48 - mae 3.16e-01 loss 8.87e-02 R 0.061 gap -0.0024690809659659863 preds -0.00396050326526165\n",
      "2022-04-26 23:30:06.505 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8715742945671081\n",
      "2022-04-26 23:30:08.296 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8713485598564148\n",
      "2022-04-26 23:30:09.896 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8727051615715027\n",
      "2022-04-26 23:30:11.664 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.87668879032135\n",
      "2022-04-26 23:30:13.251 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8761638641357422\n",
      "2022-04-26 23:30:13.681 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-26 23:30:13.682 | INFO     | __main__:train:48 - mae 3.14e-01 loss 8.80e-02 R 0.059 gap -0.0024057410191744566 preds -0.003582581877708435\n",
      "2022-04-26 23:30:13.931 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-26 23:30:13.932 | INFO     | __main__:train:48 - mae 3.13e-01 loss 8.74e-02 R 0.058 gap -0.002495011081919074 preds -0.003302337136119604\n",
      "2022-04-26 23:30:14.158 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-26 23:30:14.159 | INFO     | __main__:train:48 - mae 3.12e-01 loss 8.68e-02 R 0.061 gap -0.002204685937613249 preds -0.002990720560774207\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-26 23:30:14.578 | INFO     | __main__:train:48 - mae 3.11e-01 loss 8.63e-02 R 0.060 gap -0.0024156789295375347 preds -0.002953128656372428\n",
      "2022-04-26 23:30:15.249 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-26 23:30:15.250 | INFO     | __main__:train:48 - mae 3.10e-01 loss 8.58e-02 R 0.059 gap -0.0021402682177722454 preds -0.0028124521486461163\n",
      "2022-04-26 23:30:15.806 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-26 23:30:15.807 | INFO     | __main__:train:48 - mae 3.09e-01 loss 8.52e-02 R 0.060 gap -0.002245512092486024 preds -0.0027319802902638912\n",
      "2022-04-26 23:30:16.072 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-26 23:30:16.073 | INFO     | __main__:train:48 - mae 3.08e-01 loss 8.46e-02 R 0.060 gap -0.0025610937736928463 preds -0.0028732260689139366\n",
      "2022-04-26 23:30:16.528 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-26 23:30:16.529 | INFO     | __main__:train:48 - mae 3.06e-01 loss 8.39e-02 R 0.059 gap -0.0026211151853203773 preds -0.002747528487816453\n",
      "2022-04-26 23:30:16.771 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-26 23:30:16.772 | INFO     | __main__:train:48 - mae 3.04e-01 loss 8.32e-02 R 0.058 gap -0.002715489361435175 preds -0.0030197903979569674\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-26 23:30:17.014 | INFO     | __main__:train:48 - mae 3.03e-01 loss 8.26e-02 R 0.058 gap -0.0026285985950380564 preds -0.0028812976088374853\n",
      "2022-04-26 23:30:18.719 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8829477548599243\n",
      "2022-04-26 23:30:20.624 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8846614122390747\n",
      "2022-04-26 23:30:22.316 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8811322808265686\n",
      "2022-04-26 23:30:24.129 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8921282052993774\n",
      "2022-04-26 23:30:25.590 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8870390176773071\n",
      "2022-04-26 23:30:26.024 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-26 23:30:26.025 | INFO     | __main__:train:48 - mae 3.02e-01 loss 8.20e-02 R 0.058 gap -0.0027044308371841908 preds -0.002561690052971244\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-26 23:30:26.297 | INFO     | __main__:train:48 - mae 3.00e-01 loss 8.13e-02 R 0.060 gap -0.002907323883846402 preds -0.00252722785808146\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-26 23:30:26.566 | INFO     | __main__:train:48 - mae 2.99e-01 loss 8.10e-02 R 0.059 gap -0.0030676533933728933 preds -0.002729451283812523\n",
      "2022-04-26 23:30:26.836 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-26 23:30:26.837 | INFO     | __main__:train:48 - mae 2.98e-01 loss 8.04e-02 R 0.059 gap -0.0031530449632555246 preds -0.0027055065147578716\n",
      "2022-04-26 23:30:28.289 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.894663417339325\n",
      "2022-04-26 23:30:30.253 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908208131790161\n",
      "2022-04-26 23:30:32.064 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8955212593078613\n",
      "2022-04-26 23:30:33.895 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8884934425354004\n",
      "2022-04-26 23:30:35.640 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8972101926803588\n",
      "2022-04-26 23:30:37.364 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8990776896476745\n",
      "2022-04-26 23:30:38.908 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8968750953674316\n",
      "2022-04-26 23:30:40.570 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8873031616210938\n",
      "2022-04-26 23:30:42.391 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902988791465759\n",
      "2022-04-26 23:30:44.069 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8912638187408447\n",
      "2022-04-26 23:30:45.788 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8857690095901489\n",
      "2022-04-26 23:30:48.639 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8842708349227906\n",
      "2022-04-26 23:30:50.373 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8951775789260864\n",
      "2022-04-26 23:30:52.095 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8922386169433594\n",
      "2022-04-26 23:30:54.737 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953711986541748\n",
      "2022-04-26 23:30:57.141 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.888823401927948\n",
      "2022-04-26 23:30:59.180 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8910039067268372\n",
      "2022-04-26 23:31:02.219 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891122305393219\n",
      "2022-04-26 23:31:03.940 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8868207931518555\n",
      "2022-04-26 23:31:05.707 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8935174942016602\n",
      "2022-04-26 23:31:07.366 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8929449558258057\n",
      "2022-04-26 23:31:08.955 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8908220529556274\n",
      "2022-04-26 23:31:10.638 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8971814393997193\n",
      "2022-04-26 23:31:12.254 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8974653959274292\n",
      "2022-04-26 23:31:13.790 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8874300003051758\n",
      "2022-04-26 23:31:15.580 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882347583770752\n",
      "2022-04-26 23:31:18.533 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932090640068054\n",
      "2022-04-26 23:31:20.408 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8956380844116211\n",
      "2022-04-26 23:31:22.079 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8853566408157348\n",
      "2022-04-26 23:31:23.867 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949296712875366\n",
      "2022-04-26 23:31:25.758 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902496457099914\n",
      "2022-04-26 23:31:27.337 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.89382404088974\n",
      "2022-04-26 23:31:28.636 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8945414304733277\n",
      "2022-04-26 23:31:29.577 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8920116424560547\n",
      "2022-04-26 23:31:30.645 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8984548807144165\n",
      "2022-04-26 23:31:31.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957023382186889\n",
      "2022-04-26 23:31:32.799 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8958545923233032\n",
      "2022-04-26 23:31:33.854 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8981687307357789\n",
      "2022-04-26 23:31:34.860 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957234621047974\n",
      "2022-04-26 23:31:35.755 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8975642919540405\n",
      "2022-04-26 23:31:36.849 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.897207772731781\n",
      "2022-04-26 23:31:37.985 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8895267009735107\n",
      "2022-04-26 23:31:39.271 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8932228803634643\n",
      "2022-04-26 23:31:40.489 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8914278268814086\n",
      "2022-04-26 23:31:41.456 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8930593252182006\n",
      "2022-04-26 23:31:42.488 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8902605056762696\n",
      "2022-04-26 23:31:43.661 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8860647201538085\n",
      "2022-04-26 23:31:44.887 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8882527470588684\n",
      "2022-04-26 23:31:46.138 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8856009125709534\n",
      "2022-04-26 23:31:47.425 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8969220638275146\n",
      "2022-04-26 23:31:49.237 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8953576803207397\n",
      "2022-04-26 23:31:51.405 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8876024961471558\n",
      "2022-04-26 23:31:52.741 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8927854180335999\n",
      "2022-04-26 23:31:54.305 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960691928863526\n",
      "2022-04-26 23:31:55.774 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.891525387763977\n",
      "2022-04-26 23:31:58.343 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.9003351092338562\n",
      "2022-04-26 23:31:59.700 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8960970759391784\n",
      "2022-04-26 23:32:01.063 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8957788944244385\n",
      "2022-04-26 23:32:02.699 | INFO     | __main__:train_task_learner:28 - NC Weight: 0.8949754476547241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=6'>7</a>\u001b[0m train(task_loader)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=7'>8</a>\u001b[0m test(epoch, task_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(experiment), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000044?line=10'>11</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tracker, f)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=18'>19</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m train_task_learner(batch, h, h_opt, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Running the task learner\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=20'>21</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000038?line=21'>22</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_task_learner\u001b[0;34m(batch, h, h_opt, train, nc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m nc \u001b[39mand\u001b[39;00m nc_regularize \u001b[39mand\u001b[39;00m global_step \u001b[39m>\u001b[39m  train_steps \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=25'>26</a>\u001b[0m     nc_regularization \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=26'>27</a>\u001b[0m     nc_weight_ \u001b[39m=\u001b[39m compute_nc_weight(model, meta_batch, global_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=27'>28</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNC Weight: \u001b[39m\u001b[39m{\u001b[39;00mnc_weight_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000030?line=28'>29</a>\u001b[0m     h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight_\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 31'\u001b[0m in \u001b[0;36mcompute_nc_weight\u001b[0;34m(model, meta_batch, step, num_simulations, alpha, kappa)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m simulation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_simulations):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m model(meta_batch)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=10'>11</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000063?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(outputs) \u001b[39m# Array of shape (num_simulations, batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 26'\u001b[0m in \u001b[0;36mNeuralComplexity.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000025?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb Cell 24'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=19'>20</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=20'>21</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilinear(x_tr, bilinear_input)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_v(bilinear_output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity-measures/train_cs_alpha_notebook.ipynb#ch0000023?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:38.091 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"\"#\"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(model_path).to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, step, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            nc_weight_ = compute_nc_weight(model, meta_batch, step)\n",
    "            h_loss += nc_regularization * nc_weight_\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "step = 0\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        step+=1\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch, step)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        cls1_loss_te = ce_criterion(preds_test[:, 6:10].squeeze(), y_test[:, 6].squeeze().long())\n",
    "        cls2_loss_te = ce_criterion(preds_test[:, 10:14].squeeze(), y_test[:, 7].squeeze().long())\n",
    "        cls3_loss_te = ce_criterion(preds_test[:, 14:18].squeeze(), y_test[:, 8].squeeze().long())\n",
    "        cls4_loss_te = ce_criterion(preds_test[:, 18:22].squeeze(), y_test[:, 9].squeeze().long())\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        l_test = (reg_loss_te.mean(-1).sum() + cls1_loss_te.mean(-1).sum() + cls2_loss_te.mean(-1).sum() + cls3_loss_te.mean(-1).sum() + cls4_loss_te.mean(-1).sum())/160\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        reg_loss_tr = mse_criterion(preds_train[:, :6].squeeze(), y_train[:, :6].squeeze())\n",
    "        cls1_loss_tr = ce_criterion(preds_train[:, 6:10].squeeze(), y_train[:, 6].squeeze().long())\n",
    "        cls2_loss_tr = ce_criterion(preds_train[:, 10:14].squeeze(), y_train[:, 7].squeeze().long())\n",
    "        cls3_loss_tr = ce_criterion(preds_train[:, 14:18].squeeze(), y_train[:, 8].squeeze().long())\n",
    "        cls4_loss_tr = ce_criterion(preds_train[:, 18:22].squeeze(), y_train[:, 9].squeeze().long())\n",
    "        \n",
    "        l_train =  (reg_loss_tr.mean(-1).sum() + cls1_loss_tr.mean(-1).sum() + cls2_loss_tr.mean(-1).sum() + cls3_loss_tr.mean(-1).sum() + cls4_loss_tr.mean(-1).sum())/160\n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:47.543 | INFO     | __main__:<module>:9 - Test 0.0844 +- 0.0053\n",
      "2022-04-25 18:54:47.544 | INFO     | __main__:<module>:10 - Train 0.0779 +- 0.0052\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exit()\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.2027\n",
      "MSE for Dimension 2: 0.2800\n",
      "MSE for Dimension 3: 0.2775\n",
      "MSE for Dimension 4: 0.2494\n",
      "MSE for Dimension 5: 0.2589\n",
      "MSE for Dimension 6: 0.1913\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.26      0.35      6826\n",
      "         1.0       0.24      0.14      0.18      2121\n",
      "         2.0       0.07      0.02      0.03      1717\n",
      "         3.0       0.04      0.66      0.08       408\n",
      "\n",
      "    accuracy                           0.21     11072\n",
      "   macro avg       0.23      0.27      0.16     11072\n",
      "weighted avg       0.41      0.21      0.26     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.00      0.00      8096\n",
      "         1.0       0.06      0.02      0.03       469\n",
      "         2.0       0.07      1.00      0.14       790\n",
      "         3.0       0.68      0.02      0.04      1717\n",
      "\n",
      "    accuracy                           0.08     11072\n",
      "   macro avg       0.45      0.26      0.05     11072\n",
      "weighted avg       0.84      0.08      0.02     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.01      2716\n",
      "         1.0       0.46      0.27      0.34      4925\n",
      "         2.0       0.03      0.01      0.01      1293\n",
      "         3.0       0.21      0.79      0.34      2138\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.30      0.27      0.17     11072\n",
      "weighted avg       0.37      0.27      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.08      0.13      4859\n",
      "         1.0       0.11      0.20      0.14      1442\n",
      "         2.0       0.05      0.23      0.08       561\n",
      "         3.0       0.35      0.41      0.38      4210\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.22      0.23      0.18     11072\n",
      "weighted avg       0.32      0.23      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6387572d3ba60263f2472b530ce49454bee9bd13656fbfcf29efcd586b712758"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

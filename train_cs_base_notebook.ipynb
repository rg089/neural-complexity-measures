{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshus/miniconda3/envs/mtl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}\n",
    "experiment = \"cs_with_nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        self.fit(kind)\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"cs\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "kind = \"seq\" if data_type == \"ds\" else \"flat\"\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    \"\"\"\n",
    "    Returns a task\n",
    "    \"\"\"\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an accumulator to keep track of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = True\n",
    "gpu = '0'\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'cs'\n",
    "nc_regularize = True\n",
    "epochs = 2 if demo else 20\n",
    "train_steps = 2 if demo else 20\n",
    "log_steps = 1\n",
    "test_steps = 5\n",
    "learn_freq = 10\n",
    "inner_lr = 0.005\n",
    "inner_steps = 5 if demo else 10\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 30\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 5\n",
    "dec_depth = 5\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = \"result/model_{}.ckpt\".format(experiment)\n",
    "\n",
    "xtrain_dim = 23 if task == 'cs' else 360\n",
    "train_pred_dim = 22\n",
    "y_train_dim = 10\n",
    "y_train_ohe_dim = 22\n",
    "train_loss_dim = 10\n",
    "bilinear_output_dim = 256\n",
    "\n",
    "best_loss_train = 10000\n",
    "best_loss_test = 10000\n",
    "model_path_best_train = \"result/best_model_train_{}.ckpt\".format(experiment)\n",
    "model_path_best_test = \"result/best_model_test_{}.ckpt\".format(experiment)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"): # if adding the first sample\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    \"\"\"\n",
    "    converts y_tr having shape of (N,10) to (N,22) by converting the classification labels\n",
    "    to one-hot encoding\n",
    "    \"\"\"\n",
    "    output = y_tr[:, :6] # Keeping the regression labels\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16)).to(device)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(layers, hidden_size, task='flat', init_dim=23):\n",
    "    if task == 'flat':\n",
    "        return FlatNeuralNetwork(\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            init_dim=init_dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (for NC Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim, dropout=True):\n",
    "    \"\"\"\n",
    "    stacks a given number of fc layers\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.BatchNorm1d(hidden_dim)])\n",
    "            if dropout:\n",
    "                modules.append(nn.Dropout(0.1))\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(xtrain_dim, y_train_ohe_dim+train_loss_dim+train_pred_dim+1, bilinear_output_dim)\n",
    "        self.mlp_v = fc_stack(enc_depth, bilinear_output_dim, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, xtrain_dim+train_pred_dim, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :xtrain_dim], inputs[\"tr_xyp\"][:, xtrain_dim:xtrain_dim+y_train_dim], inputs[\"tr_xyp\"][:, xtrain_dim+y_train_dim:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        q = self.dropout(q)\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        k = self.dropout(k)\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)).to(device), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Complexity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralComplexity(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention-based MLP model to compute the complexity and generalization \n",
    "    of a task learner\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttEncoder()\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, init_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        A simple MLP based architecture to perform multi-output multi-task learning (regression and classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = fc_stack(num_layers, init_dim, hidden_size, hidden_size, dropout=True)\n",
    "        self.linear_reg = torch.nn.Linear(hidden_size, 6)\n",
    "        self.linear_cls1 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls3 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.linear_cls4 = torch.nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        reg = self.linear_reg(x)\n",
    "        cls1 = self.linear_cls1(x)\n",
    "        cls2 = self.linear_cls2(x)\n",
    "        cls3 = self.linear_cls3(x)\n",
    "        cls4 = self.linear_cls4(x)\n",
    "        return torch.cat([reg, cls1, cls2, cls3, cls4], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_loss(pred, target, reg_crit, cls_crit):\n",
    "    reg_loss = 0.2 * reg_crit(pred[:, :6].squeeze(), target[:, :6].squeeze())\n",
    "\n",
    "    cls1_loss = 0.2 * cls_crit(pred[:, 6:10].squeeze(), target[:, 6].squeeze().long())\n",
    "    cls2_loss = 0.2 * cls_crit(pred[:, 10:14].squeeze(), target[:, 7].squeeze().long())\n",
    "    cls3_loss = 0.2 * cls_crit(pred[:, 14:18].squeeze(), target[:, 8].squeeze().long())\n",
    "    cls4_loss = 0.2 * cls_crit(pred[:, 18:22].squeeze(), target[:, 9].squeeze().long())\n",
    "\n",
    "    concat_loss = torch.cat((reg_loss, cls1_loss.unsqueeze(1), cls2_loss.unsqueeze(1), \n",
    "                cls3_loss.unsqueeze(1), cls4_loss.unsqueeze(1)), dim=-1)\n",
    "    # Defining l_train to send into the NC Model\n",
    "\n",
    "    loss = (reg_loss.mean(-1).sum() + cls1_loss.mean(-1).sum() + cls2_loss.mean(-1).sum() + \n",
    "                cls3_loss.mean(-1).sum() + cls4_loss.mean(-1).sum())/ (batch_size * 5)\n",
    "\n",
    "    return loss, concat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner(batch, h, h_opt, train=True, nc=True):\n",
    "    \"\"\"\n",
    "    training the task learner for a batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "\n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        te_xp = torch.cat([x_test, preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train, preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train, y_train, preds_train], dim=-1)\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "\n",
    "        if  nc and nc_regularize and global_step >  train_steps * 2:\n",
    "            nc_regularization = model(meta_batch).sum()\n",
    "            h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        _, l_test = get_task_loss(preds_test, y_test, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        # l_train and l_test are used to compute the gap\n",
    "        \n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=l_train.cpu().detach()\n",
    "            )\n",
    "    return h, meta_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()\n",
    "memory_bank = MemoryBank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    global best_loss_train\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "\n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "    for task in train_loader: # Iterating over each task\n",
    "        for batch in task: # iterating over each batch in a task\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                train_task_learner(batch, h, h_opt, train=True)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch(batch_size)\n",
    "            model_preds = model(meta_batch) # Getting the predictions of the NC Model\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean() # Computing the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping the gradients for a more stable training\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze()) # Computing the MAE\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            ) # Adding the metrics to the accumulator for logging\n",
    "\n",
    "            # LOGGING: \n",
    "            if accum.mean(\"loss\") < best_loss_train:\n",
    "                best_loss_train = accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_train)\n",
    "\n",
    "            torch.save(model, model_path) # Saving the model\n",
    "\n",
    "            all_gaps = torch.cat(accum[\"gap\"])\n",
    "            all_preds = torch.cat(accum[\"pred\"])\n",
    "            R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "            logger.info(f\"Train Step {global_step}\")\n",
    "            logger.info(\n",
    "                f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} gap {all_gaps.mean()} preds {all_preds.mean()}\"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"mae\": accum.mean(\"mae\"),\n",
    "                \"loss\": accum.mean(\"loss\"),\n",
    "                \"R\": R,\n",
    "            }\n",
    "            log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    \"\"\"\n",
    "    A function to compute the metrics for the NC model\n",
    "    \"\"\"\n",
    "    global best_loss_test\n",
    "    test_accum = Accumulator()\n",
    "\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = train_task_learner(batch, h, h_opt, train=False) # Running the task learner\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                loss, l_train = get_task_loss(preds_train, y_train, mse_criterion, ce_criterion)\n",
    "                _, l_test = get_task_loss(preds_test, y_test, mse_criterion, ce_criterion)\n",
    "\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if accum.mean(\"loss\") < best_loss_test:\n",
    "                best_loss_test = test_accum.mean(\"loss\")\n",
    "                torch.save(model, model_path_best_test)\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:45:57.695 | INFO     | __main__:<module>:19 - Populate time: 0.047833170741796494\n"
     ]
    }
   ],
   "source": [
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 5 if demo else len(data)\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task learner for a few steps initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(task_loader):\n",
    "    h = get_learner(\n",
    "        layers= learner_layers,\n",
    "        hidden_size= learner_hidden,\n",
    "        init_dim=xtrain_dim,\n",
    "        task='flat',\n",
    "    ).to(device)\n",
    "    \n",
    "    h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "    \n",
    "    for j, batch in enumerate(task):\n",
    "        train_task_learner(batch, h, h_opt, train=True, nc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:46:03.120 | INFO     | __main__:<module>:4 - Epoch 0\n",
      "2022-04-25 18:46:03.120 | INFO     | __main__:<module>:5 - Bank size: 14880\n",
      "2022-04-25 18:46:03.252 | INFO     | __main__:train:47 - Train Step 1\n",
      "2022-04-25 18:46:03.253 | INFO     | __main__:train:49 - mae 5.11e-01 loss 1.93e-01 R 0.330 gap -0.006546972319483757 preds -0.027155369520187378\n",
      "2022-04-25 18:46:03.330 | INFO     | __main__:train:47 - Train Step 2\n",
      "2022-04-25 18:46:03.331 | INFO     | __main__:train:49 - mae 5.90e-01 loss 2.46e-01 R 0.091 gap -0.004185333847999573 preds -0.04200897365808487\n",
      "2022-04-25 18:46:03.398 | INFO     | __main__:train:47 - Train Step 3\n",
      "2022-04-25 18:46:03.399 | INFO     | __main__:train:49 - mae 5.51e-01 loss 2.18e-01 R 0.145 gap 0.0009324792772531509 preds -0.014336398802697659\n",
      "2022-04-25 18:46:03.468 | INFO     | __main__:train:47 - Train Step 4\n",
      "2022-04-25 18:46:03.469 | INFO     | __main__:train:49 - mae 5.96e-01 loss 2.45e-01 R 0.029 gap 0.012885267846286297 preds -0.021728243678808212\n",
      "2022-04-25 18:46:03.534 | INFO     | __main__:train:47 - Train Step 5\n",
      "2022-04-25 18:46:03.535 | INFO     | __main__:train:49 - mae 5.88e-01 loss 2.46e-01 R 0.010 gap 0.016330037266016006 preds -0.019997818395495415\n",
      "2022-04-25 18:46:03.597 | INFO     | __main__:train:47 - Train Step 6\n",
      "2022-04-25 18:46:03.598 | INFO     | __main__:train:49 - mae 5.75e-01 loss 2.39e-01 R 0.062 gap 0.020957479253411293 preds -0.011363725177943707\n",
      "2022-04-25 18:46:03.659 | INFO     | __main__:train:47 - Train Step 7\n",
      "2022-04-25 18:46:03.659 | INFO     | __main__:train:49 - mae 5.74e-01 loss 2.40e-01 R 0.044 gap 0.019055286422371864 preds -0.006276439409703016\n",
      "2022-04-25 18:46:03.720 | INFO     | __main__:train:47 - Train Step 8\n",
      "2022-04-25 18:46:03.721 | INFO     | __main__:train:49 - mae 5.63e-01 loss 2.32e-01 R 0.025 gap 0.020903123542666435 preds -0.010051131248474121\n",
      "2022-04-25 18:46:03.784 | INFO     | __main__:train:47 - Train Step 9\n",
      "2022-04-25 18:46:03.785 | INFO     | __main__:train:49 - mae 5.57e-01 loss 2.26e-01 R -0.003 gap 0.01788203977048397 preds -0.01507425494492054\n",
      "2022-04-25 18:46:04.072 | INFO     | __main__:train:47 - Train Step 10\n",
      "2022-04-25 18:46:04.072 | INFO     | __main__:train:49 - mae 5.50e-01 loss 2.21e-01 R 0.009 gap 0.020825928077101707 preds -0.01099025085568428\n",
      "2022-04-25 18:46:04.136 | INFO     | __main__:train:47 - Train Step 11\n",
      "2022-04-25 18:46:04.137 | INFO     | __main__:train:49 - mae 5.40e-01 loss 2.15e-01 R 0.029 gap 0.019430609419941902 preds -0.011181415989995003\n",
      "2022-04-25 18:46:04.200 | INFO     | __main__:train:47 - Train Step 12\n",
      "2022-04-25 18:46:04.201 | INFO     | __main__:train:49 - mae 5.35e-01 loss 2.10e-01 R 0.022 gap 0.019430913031101227 preds -0.015862220898270607\n",
      "2022-04-25 18:46:04.264 | INFO     | __main__:train:47 - Train Step 13\n",
      "2022-04-25 18:46:04.265 | INFO     | __main__:train:49 - mae 5.29e-01 loss 2.04e-01 R 0.020 gap 0.021726273000240326 preds -0.012630214914679527\n",
      "2022-04-25 18:46:04.325 | INFO     | __main__:train:47 - Train Step 14\n",
      "2022-04-25 18:46:04.326 | INFO     | __main__:train:49 - mae 5.22e-01 loss 1.99e-01 R 0.012 gap 0.022746291011571884 preds -0.013697822578251362\n",
      "2022-04-25 18:46:04.389 | INFO     | __main__:train:47 - Train Step 15\n",
      "2022-04-25 18:46:04.390 | INFO     | __main__:train:49 - mae 5.18e-01 loss 1.97e-01 R -0.004 gap 0.022237880155444145 preds -0.012149875983595848\n",
      "2022-04-25 18:46:04.451 | INFO     | __main__:train:47 - Train Step 16\n",
      "2022-04-25 18:46:04.452 | INFO     | __main__:train:49 - mae 5.14e-01 loss 1.94e-01 R 0.006 gap 0.02272442728281021 preds -0.013476644642651081\n",
      "2022-04-25 18:46:04.512 | INFO     | __main__:train:47 - Train Step 17\n",
      "2022-04-25 18:46:04.513 | INFO     | __main__:train:49 - mae 5.03e-01 loss 1.88e-01 R 0.003 gap 0.02248752862215042 preds -0.009005004540085793\n",
      "2022-04-25 18:46:04.572 | INFO     | __main__:train:47 - Train Step 18\n",
      "2022-04-25 18:46:04.573 | INFO     | __main__:train:49 - mae 4.96e-01 loss 1.85e-01 R 0.005 gap 0.022417927160859108 preds -0.007820391096174717\n",
      "2022-04-25 18:46:04.635 | INFO     | __main__:train:47 - Train Step 19\n",
      "2022-04-25 18:46:04.636 | INFO     | __main__:train:49 - mae 4.95e-01 loss 1.85e-01 R 0.002 gap 0.023372754454612732 preds -0.006072927266359329\n",
      "2022-04-25 18:46:04.925 | INFO     | __main__:train:47 - Train Step 20\n",
      "2022-04-25 18:46:04.926 | INFO     | __main__:train:49 - mae 4.94e-01 loss 1.84e-01 R -0.005 gap 0.02335342764854431 preds -0.0012804928701370955\n",
      "2022-04-25 18:46:04.985 | INFO     | __main__:train:47 - Train Step 21\n",
      "2022-04-25 18:46:04.986 | INFO     | __main__:train:49 - mae 4.90e-01 loss 1.82e-01 R 0.013 gap 0.0245254747569561 preds 0.0008987092296592891\n",
      "2022-04-25 18:46:05.045 | INFO     | __main__:train:47 - Train Step 22\n",
      "2022-04-25 18:46:05.046 | INFO     | __main__:train:49 - mae 4.90e-01 loss 1.82e-01 R 0.008 gap 0.02391302026808262 preds -0.0039057256653904915\n",
      "2022-04-25 18:46:05.103 | INFO     | __main__:train:47 - Train Step 23\n",
      "2022-04-25 18:46:05.104 | INFO     | __main__:train:49 - mae 4.85e-01 loss 1.78e-01 R 0.020 gap 0.024192361161112785 preds -0.0028154244646430016\n",
      "2022-04-25 18:46:05.164 | INFO     | __main__:train:47 - Train Step 24\n",
      "2022-04-25 18:46:05.164 | INFO     | __main__:train:49 - mae 4.78e-01 loss 1.74e-01 R 0.019 gap 0.023447751998901367 preds -0.00026856735348701477\n",
      "2022-04-25 18:46:05.223 | INFO     | __main__:train:47 - Train Step 25\n",
      "2022-04-25 18:46:05.223 | INFO     | __main__:train:49 - mae 4.71e-01 loss 1.70e-01 R 0.022 gap 0.02378670498728752 preds 8.438855729764327e-05\n",
      "2022-04-25 18:46:05.280 | INFO     | __main__:train:47 - Train Step 26\n",
      "2022-04-25 18:46:05.281 | INFO     | __main__:train:49 - mae 4.67e-01 loss 1.68e-01 R 0.018 gap 0.023640768602490425 preds 0.002337711863219738\n",
      "2022-04-25 18:46:05.341 | INFO     | __main__:train:47 - Train Step 27\n",
      "2022-04-25 18:46:05.342 | INFO     | __main__:train:49 - mae 4.64e-01 loss 1.66e-01 R 0.013 gap 0.024098487570881844 preds 0.004529865458607674\n",
      "2022-04-25 18:46:05.405 | INFO     | __main__:train:47 - Train Step 28\n",
      "2022-04-25 18:46:05.406 | INFO     | __main__:train:49 - mae 4.59e-01 loss 1.63e-01 R 0.006 gap 0.02319319359958172 preds 0.0066358596086502075\n",
      "2022-04-25 18:46:05.465 | INFO     | __main__:train:47 - Train Step 29\n",
      "2022-04-25 18:46:05.466 | INFO     | __main__:train:49 - mae 4.56e-01 loss 1.61e-01 R 0.006 gap 0.023742351680994034 preds 0.005429270677268505\n",
      "2022-04-25 18:46:05.740 | INFO     | __main__:train:47 - Train Step 30\n",
      "2022-04-25 18:46:05.741 | INFO     | __main__:train:49 - mae 4.53e-01 loss 1.59e-01 R 0.011 gap 0.02339117042720318 preds 0.004706201143562794\n",
      "2022-04-25 18:46:05.801 | INFO     | __main__:train:47 - Train Step 31\n",
      "2022-04-25 18:46:05.802 | INFO     | __main__:train:49 - mae 4.46e-01 loss 1.55e-01 R 0.014 gap 0.023432057350873947 preds 0.00529055017977953\n",
      "2022-04-25 18:46:05.861 | INFO     | __main__:train:47 - Train Step 32\n",
      "2022-04-25 18:46:05.862 | INFO     | __main__:train:49 - mae 4.42e-01 loss 1.53e-01 R 0.025 gap 0.02342154085636139 preds 0.005791566334664822\n",
      "2022-04-25 18:46:05.923 | INFO     | __main__:train:47 - Train Step 33\n",
      "2022-04-25 18:46:05.924 | INFO     | __main__:train:49 - mae 4.40e-01 loss 1.51e-01 R 0.022 gap 0.02256717160344124 preds 0.005736146587878466\n",
      "2022-04-25 18:46:05.988 | INFO     | __main__:train:47 - Train Step 34\n",
      "2022-04-25 18:46:05.989 | INFO     | __main__:train:49 - mae 4.39e-01 loss 1.51e-01 R 0.030 gap 0.02175883948802948 preds 0.006337704136967659\n",
      "2022-04-25 18:46:06.052 | INFO     | __main__:train:47 - Train Step 35\n",
      "2022-04-25 18:46:06.052 | INFO     | __main__:train:49 - mae 4.37e-01 loss 1.50e-01 R 0.029 gap 0.021703070029616356 preds 0.007544808555394411\n",
      "2022-04-25 18:46:06.113 | INFO     | __main__:train:47 - Train Step 36\n",
      "2022-04-25 18:46:06.114 | INFO     | __main__:train:49 - mae 4.36e-01 loss 1.49e-01 R 0.032 gap 0.020897088572382927 preds 0.009254815056920052\n",
      "2022-04-25 18:46:06.172 | INFO     | __main__:train:47 - Train Step 37\n",
      "2022-04-25 18:46:06.173 | INFO     | __main__:train:49 - mae 4.33e-01 loss 1.47e-01 R 0.035 gap 0.01998855546116829 preds 0.01010412909090519\n",
      "2022-04-25 18:46:06.233 | INFO     | __main__:train:47 - Train Step 38\n",
      "2022-04-25 18:46:06.233 | INFO     | __main__:train:49 - mae 4.30e-01 loss 1.46e-01 R 0.037 gap 0.019664570689201355 preds 0.011096124537289143\n",
      "2022-04-25 18:46:06.297 | INFO     | __main__:train:47 - Train Step 39\n",
      "2022-04-25 18:46:06.298 | INFO     | __main__:train:49 - mae 4.25e-01 loss 1.43e-01 R 0.039 gap 0.02062087319791317 preds 0.012546878308057785\n",
      "2022-04-25 18:46:06.588 | INFO     | __main__:train:47 - Train Step 40\n",
      "2022-04-25 18:46:06.588 | INFO     | __main__:train:49 - mae 4.21e-01 loss 1.41e-01 R 0.040 gap 0.020335575565695763 preds 0.012290021404623985\n",
      "2022-04-25 18:46:06.650 | INFO     | __main__:train:47 - Train Step 41\n",
      "2022-04-25 18:46:06.651 | INFO     | __main__:train:49 - mae 4.18e-01 loss 1.40e-01 R 0.042 gap 0.02051592245697975 preds 0.010952058248221874\n",
      "2022-04-25 18:46:06.712 | INFO     | __main__:train:47 - Train Step 42\n",
      "2022-04-25 18:46:06.713 | INFO     | __main__:train:49 - mae 4.15e-01 loss 1.38e-01 R 0.042 gap 0.020362934097647667 preds 0.01080743595957756\n",
      "2022-04-25 18:46:06.773 | INFO     | __main__:train:47 - Train Step 43\n",
      "2022-04-25 18:46:06.774 | INFO     | __main__:train:49 - mae 4.11e-01 loss 1.36e-01 R 0.043 gap 0.020381001755595207 preds 0.010594530031085014\n",
      "2022-04-25 18:46:06.837 | INFO     | __main__:train:47 - Train Step 44\n",
      "2022-04-25 18:46:06.838 | INFO     | __main__:train:49 - mae 4.07e-01 loss 1.33e-01 R 0.043 gap 0.02072213962674141 preds 0.010194221511483192\n",
      "2022-04-25 18:46:06.899 | INFO     | __main__:train:47 - Train Step 45\n",
      "2022-04-25 18:46:06.899 | INFO     | __main__:train:49 - mae 4.04e-01 loss 1.32e-01 R 0.041 gap 0.02060067094862461 preds 0.010048993863165379\n",
      "2022-04-25 18:46:06.962 | INFO     | __main__:train:47 - Train Step 46\n",
      "2022-04-25 18:46:06.963 | INFO     | __main__:train:49 - mae 4.02e-01 loss 1.30e-01 R 0.044 gap 0.02107788622379303 preds 0.009699052199721336\n",
      "2022-04-25 18:46:07.026 | INFO     | __main__:train:47 - Train Step 47\n",
      "2022-04-25 18:46:07.027 | INFO     | __main__:train:49 - mae 3.97e-01 loss 1.28e-01 R 0.043 gap 0.02114422805607319 preds 0.009844240732491016\n",
      "2022-04-25 18:46:07.088 | INFO     | __main__:train:47 - Train Step 48\n",
      "2022-04-25 18:46:07.089 | INFO     | __main__:train:49 - mae 3.95e-01 loss 1.27e-01 R 0.042 gap 0.02106129191815853 preds 0.009630086831748486\n",
      "2022-04-25 18:46:07.152 | INFO     | __main__:train:47 - Train Step 49\n",
      "2022-04-25 18:46:07.153 | INFO     | __main__:train:49 - mae 3.92e-01 loss 1.25e-01 R 0.044 gap 0.02172810770571232 preds 0.009847413748502731\n",
      "2022-04-25 18:46:07.439 | INFO     | __main__:train:47 - Train Step 50\n",
      "2022-04-25 18:46:07.439 | INFO     | __main__:train:49 - mae 3.89e-01 loss 1.23e-01 R 0.047 gap 0.020664632320404053 preds 0.01004046481102705\n",
      "2022-04-25 18:46:07.502 | INFO     | __main__:train:47 - Train Step 51\n",
      "2022-04-25 18:46:07.502 | INFO     | __main__:train:49 - mae 3.86e-01 loss 1.22e-01 R 0.046 gap 0.020714623853564262 preds 0.010345188900828362\n",
      "2022-04-25 18:46:07.563 | INFO     | __main__:train:47 - Train Step 52\n",
      "2022-04-25 18:46:07.564 | INFO     | __main__:train:49 - mae 3.83e-01 loss 1.20e-01 R 0.046 gap 0.020645877346396446 preds 0.01060873456299305\n",
      "2022-04-25 18:46:07.624 | INFO     | __main__:train:47 - Train Step 53\n",
      "2022-04-25 18:46:07.625 | INFO     | __main__:train:49 - mae 3.81e-01 loss 1.19e-01 R 0.049 gap 0.020697420462965965 preds 0.011448048986494541\n",
      "2022-04-25 18:46:07.684 | INFO     | __main__:train:47 - Train Step 54\n",
      "2022-04-25 18:46:07.684 | INFO     | __main__:train:49 - mae 3.78e-01 loss 1.17e-01 R 0.050 gap 0.020684652030467987 preds 0.012213238514959812\n",
      "2022-04-25 18:46:07.745 | INFO     | __main__:train:47 - Train Step 55\n",
      "2022-04-25 18:46:07.746 | INFO     | __main__:train:49 - mae 3.76e-01 loss 1.16e-01 R 0.048 gap 0.02022785320878029 preds 0.01222827099263668\n",
      "2022-04-25 18:46:07.807 | INFO     | __main__:train:47 - Train Step 56\n",
      "2022-04-25 18:46:07.808 | INFO     | __main__:train:49 - mae 3.74e-01 loss 1.15e-01 R 0.048 gap 0.020052244886755943 preds 0.012400004081428051\n",
      "2022-04-25 18:46:07.868 | INFO     | __main__:train:47 - Train Step 57\n",
      "2022-04-25 18:46:07.869 | INFO     | __main__:train:49 - mae 3.72e-01 loss 1.14e-01 R 0.048 gap 0.019977571442723274 preds 0.013088865205645561\n",
      "2022-04-25 18:46:07.932 | INFO     | __main__:train:47 - Train Step 58\n",
      "2022-04-25 18:46:07.933 | INFO     | __main__:train:49 - mae 3.69e-01 loss 1.13e-01 R 0.047 gap 0.019812243059277534 preds 0.012766174040734768\n",
      "2022-04-25 18:46:08.006 | INFO     | __main__:train:47 - Train Step 59\n",
      "2022-04-25 18:46:08.007 | INFO     | __main__:train:49 - mae 3.67e-01 loss 1.12e-01 R 0.050 gap 0.0201601330190897 preds 0.012878523208200932\n",
      "2022-04-25 18:46:08.288 | INFO     | __main__:train:47 - Train Step 60\n",
      "2022-04-25 18:46:08.289 | INFO     | __main__:train:49 - mae 3.64e-01 loss 1.10e-01 R 0.049 gap 0.02043718472123146 preds 0.012788618914783001\n",
      "2022-04-25 18:46:08.352 | INFO     | __main__:train:47 - Train Step 61\n",
      "2022-04-25 18:46:08.353 | INFO     | __main__:train:49 - mae 3.62e-01 loss 1.09e-01 R 0.055 gap 0.020442405715584755 preds 0.012916439212858677\n",
      "2022-04-25 18:46:08.414 | INFO     | __main__:train:47 - Train Step 62\n",
      "2022-04-25 18:46:08.414 | INFO     | __main__:train:49 - mae 3.59e-01 loss 1.08e-01 R 0.056 gap 0.02064584568142891 preds 0.012840958312153816\n",
      "2022-04-25 18:46:08.468 | INFO     | __main__:train:47 - Train Step 63\n",
      "2022-04-25 18:46:08.469 | INFO     | __main__:train:49 - mae 3.56e-01 loss 1.07e-01 R 0.061 gap 0.02109553851187229 preds 0.013158312067389488\n",
      "2022-04-25 18:46:08.533 | INFO     | __main__:train:47 - Train Step 64\n",
      "2022-04-25 18:46:08.534 | INFO     | __main__:train:49 - mae 3.54e-01 loss 1.06e-01 R 0.060 gap 0.021064385771751404 preds 0.013570377603173256\n",
      "2022-04-25 18:46:08.592 | INFO     | __main__:train:47 - Train Step 65\n",
      "2022-04-25 18:46:08.593 | INFO     | __main__:train:49 - mae 3.52e-01 loss 1.05e-01 R 0.059 gap 0.021137263625860214 preds 0.013631387613713741\n",
      "2022-04-25 18:46:08.650 | INFO     | __main__:train:47 - Train Step 66\n",
      "2022-04-25 18:46:08.651 | INFO     | __main__:train:49 - mae 3.49e-01 loss 1.04e-01 R 0.060 gap 0.02112521417438984 preds 0.013987302780151367\n",
      "2022-04-25 18:46:08.711 | INFO     | __main__:train:47 - Train Step 67\n",
      "2022-04-25 18:46:08.712 | INFO     | __main__:train:49 - mae 3.47e-01 loss 1.03e-01 R 0.058 gap 0.02172817476093769 preds 0.01372525840997696\n",
      "2022-04-25 18:46:08.771 | INFO     | __main__:train:47 - Train Step 68\n",
      "2022-04-25 18:46:08.772 | INFO     | __main__:train:49 - mae 3.45e-01 loss 1.01e-01 R 0.060 gap 0.02181960828602314 preds 0.014501411467790604\n",
      "2022-04-25 18:46:08.832 | INFO     | __main__:train:47 - Train Step 69\n",
      "2022-04-25 18:46:08.832 | INFO     | __main__:train:49 - mae 3.43e-01 loss 1.00e-01 R 0.060 gap 0.021377453580498695 preds 0.015085569582879543\n",
      "2022-04-25 18:46:09.108 | INFO     | __main__:train:47 - Train Step 70\n",
      "2022-04-25 18:46:09.109 | INFO     | __main__:train:49 - mae 3.40e-01 loss 9.93e-02 R 0.063 gap 0.021369507536292076 preds 0.01545215304940939\n",
      "2022-04-25 18:46:09.173 | INFO     | __main__:train:47 - Train Step 71\n",
      "2022-04-25 18:46:09.174 | INFO     | __main__:train:49 - mae 3.38e-01 loss 9.83e-02 R 0.063 gap 0.021338216960430145 preds 0.015461070463061333\n",
      "2022-04-25 18:46:09.236 | INFO     | __main__:train:47 - Train Step 72\n",
      "2022-04-25 18:46:09.237 | INFO     | __main__:train:49 - mae 3.36e-01 loss 9.75e-02 R 0.063 gap 0.02106383815407753 preds 0.01547740027308464\n",
      "2022-04-25 18:46:09.298 | INFO     | __main__:train:47 - Train Step 73\n",
      "2022-04-25 18:46:09.298 | INFO     | __main__:train:49 - mae 3.34e-01 loss 9.68e-02 R 0.062 gap 0.020935462787747383 preds 0.015362230129539967\n",
      "2022-04-25 18:46:09.359 | INFO     | __main__:train:47 - Train Step 74\n",
      "2022-04-25 18:46:09.360 | INFO     | __main__:train:49 - mae 3.33e-01 loss 9.62e-02 R 0.062 gap 0.020908966660499573 preds 0.015261477790772915\n",
      "2022-04-25 18:46:09.423 | INFO     | __main__:train:47 - Train Step 75\n",
      "2022-04-25 18:46:09.424 | INFO     | __main__:train:49 - mae 3.31e-01 loss 9.54e-02 R 0.063 gap 0.020984264090657234 preds 0.014992599375545979\n",
      "2022-04-25 18:46:09.485 | INFO     | __main__:train:47 - Train Step 76\n",
      "2022-04-25 18:46:09.486 | INFO     | __main__:train:49 - mae 3.29e-01 loss 9.45e-02 R 0.063 gap 0.02084917388856411 preds 0.014815178699791431\n",
      "2022-04-25 18:46:09.546 | INFO     | __main__:train:47 - Train Step 77\n",
      "2022-04-25 18:46:09.547 | INFO     | __main__:train:49 - mae 3.27e-01 loss 9.36e-02 R 0.064 gap 0.020541496574878693 preds 0.014713633805513382\n",
      "2022-04-25 18:46:09.608 | INFO     | __main__:train:47 - Train Step 78\n",
      "2022-04-25 18:46:09.609 | INFO     | __main__:train:49 - mae 3.26e-01 loss 9.28e-02 R 0.065 gap 0.02030080184340477 preds 0.014677333645522594\n",
      "2022-04-25 18:46:09.676 | INFO     | __main__:train:47 - Train Step 79\n",
      "2022-04-25 18:46:09.677 | INFO     | __main__:train:49 - mae 3.24e-01 loss 9.20e-02 R 0.068 gap 0.020653720945119858 preds 0.015127391554415226\n",
      "2022-04-25 18:46:09.959 | INFO     | __main__:train:47 - Train Step 80\n",
      "2022-04-25 18:46:09.959 | INFO     | __main__:train:49 - mae 3.23e-01 loss 9.13e-02 R 0.069 gap 0.02074701525270939 preds 0.014964893460273743\n",
      "2022-04-25 18:46:10.018 | INFO     | __main__:train:47 - Train Step 81\n",
      "2022-04-25 18:46:10.018 | INFO     | __main__:train:49 - mae 3.22e-01 loss 9.08e-02 R 0.068 gap 0.020766718313097954 preds 0.014602062292397022\n",
      "2022-04-25 18:46:10.075 | INFO     | __main__:train:47 - Train Step 82\n",
      "2022-04-25 18:46:10.076 | INFO     | __main__:train:49 - mae 3.20e-01 loss 8.99e-02 R 0.067 gap 0.020932523533701897 preds 0.01453801803290844\n",
      "2022-04-25 18:46:10.132 | INFO     | __main__:train:47 - Train Step 83\n",
      "2022-04-25 18:46:10.132 | INFO     | __main__:train:49 - mae 3.18e-01 loss 8.91e-02 R 0.067 gap 0.020897917449474335 preds 0.01473234687000513\n",
      "2022-04-25 18:46:10.191 | INFO     | __main__:train:47 - Train Step 84\n",
      "2022-04-25 18:46:10.192 | INFO     | __main__:train:49 - mae 3.17e-01 loss 8.85e-02 R 0.064 gap 0.02043875865638256 preds 0.014396938495337963\n",
      "2022-04-25 18:46:10.252 | INFO     | __main__:train:47 - Train Step 85\n",
      "2022-04-25 18:46:10.253 | INFO     | __main__:train:49 - mae 3.16e-01 loss 8.80e-02 R 0.066 gap 0.02001195214688778 preds 0.014373443089425564\n",
      "2022-04-25 18:46:10.313 | INFO     | __main__:train:47 - Train Step 86\n",
      "2022-04-25 18:46:10.314 | INFO     | __main__:train:49 - mae 3.14e-01 loss 8.73e-02 R 0.065 gap 0.019806288182735443 preds 0.013891107402741909\n",
      "2022-04-25 18:46:10.377 | INFO     | __main__:train:47 - Train Step 87\n",
      "2022-04-25 18:46:10.377 | INFO     | __main__:train:49 - mae 3.12e-01 loss 8.65e-02 R 0.068 gap 0.01974695920944214 preds 0.014062374830245972\n",
      "2022-04-25 18:46:10.440 | INFO     | __main__:train:47 - Train Step 88\n",
      "2022-04-25 18:46:10.441 | INFO     | __main__:train:49 - mae 3.11e-01 loss 8.58e-02 R 0.070 gap 0.019951369613409042 preds 0.013840227387845516\n",
      "2022-04-25 18:46:10.499 | INFO     | __main__:train:47 - Train Step 89\n",
      "2022-04-25 18:46:10.499 | INFO     | __main__:train:49 - mae 3.09e-01 loss 8.51e-02 R 0.071 gap 0.020257923752069473 preds 0.01348432432860136\n",
      "2022-04-25 18:46:10.790 | INFO     | __main__:train:47 - Train Step 90\n",
      "2022-04-25 18:46:10.791 | INFO     | __main__:train:49 - mae 3.07e-01 loss 8.44e-02 R 0.070 gap 0.02001023478806019 preds 0.013043416664004326\n",
      "2022-04-25 18:46:10.850 | INFO     | __main__:train:47 - Train Step 91\n",
      "2022-04-25 18:46:10.851 | INFO     | __main__:train:49 - mae 3.06e-01 loss 8.39e-02 R 0.071 gap 0.020075134932994843 preds 0.012763631530106068\n",
      "2022-04-25 18:46:10.908 | INFO     | __main__:train:47 - Train Step 92\n",
      "2022-04-25 18:46:10.909 | INFO     | __main__:train:49 - mae 3.04e-01 loss 8.31e-02 R 0.072 gap 0.01968812383711338 preds 0.012761272490024567\n",
      "2022-04-25 18:46:10.967 | INFO     | __main__:train:47 - Train Step 93\n",
      "2022-04-25 18:46:10.968 | INFO     | __main__:train:49 - mae 3.03e-01 loss 8.26e-02 R 0.070 gap 0.019711220636963844 preds 0.013116979040205479\n",
      "2022-04-25 18:46:31.460 | INFO     | __main__:test:55 - Test epoch 0\n",
      "2022-04-25 18:46:31.461 | INFO     | __main__:test:57 - mae 1.67e-01 loss 2.48e-02 R 0.007 l_test 1.02e+00 l_train 1.00e+00 \n",
      "2022-04-25 18:46:31.462 | INFO     | __main__:<module>:4 - Epoch 1\n",
      "2022-04-25 18:46:31.463 | INFO     | __main__:<module>:5 - Bank size: 16320\n",
      "2022-04-25 18:46:31.555 | INFO     | __main__:train:47 - Train Step 94\n",
      "2022-04-25 18:46:31.556 | INFO     | __main__:train:49 - mae 3.02e-01 loss 8.20e-02 R 0.068 gap 0.01945924386382103 preds 0.013343680649995804\n",
      "2022-04-25 18:46:31.636 | INFO     | __main__:train:47 - Train Step 95\n",
      "2022-04-25 18:46:31.637 | INFO     | __main__:train:49 - mae 3.00e-01 loss 8.14e-02 R 0.067 gap 0.01930660754442215 preds 0.013832107186317444\n",
      "2022-04-25 18:46:31.703 | INFO     | __main__:train:47 - Train Step 96\n",
      "2022-04-25 18:46:31.704 | INFO     | __main__:train:49 - mae 2.99e-01 loss 8.08e-02 R 0.066 gap 0.01918913424015045 preds 0.014147733338177204\n",
      "2022-04-25 18:46:31.777 | INFO     | __main__:train:47 - Train Step 97\n",
      "2022-04-25 18:46:31.778 | INFO     | __main__:train:49 - mae 2.98e-01 loss 8.02e-02 R 0.068 gap 0.019161291420459747 preds 0.014561149291694164\n",
      "2022-04-25 18:46:31.845 | INFO     | __main__:train:47 - Train Step 98\n",
      "2022-04-25 18:46:31.846 | INFO     | __main__:train:49 - mae 2.96e-01 loss 7.99e-02 R 0.068 gap 0.01927216723561287 preds 0.015165391378104687\n",
      "2022-04-25 18:46:31.903 | INFO     | __main__:train:47 - Train Step 99\n",
      "2022-04-25 18:46:31.904 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.94e-02 R 0.067 gap 0.019347265362739563 preds 0.015505064278841019\n",
      "2022-04-25 18:46:32.164 | INFO     | __main__:train:47 - Train Step 100\n",
      "2022-04-25 18:46:32.165 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.93e-02 R 0.064 gap 0.018952418118715286 preds 0.015406173653900623\n",
      "2022-04-25 18:46:32.224 | INFO     | __main__:train:47 - Train Step 101\n",
      "2022-04-25 18:46:32.225 | INFO     | __main__:train:49 - mae 2.95e-01 loss 7.93e-02 R 0.062 gap 0.018557878211140633 preds 0.01542012207210064\n",
      "2022-04-25 18:46:32.285 | INFO     | __main__:train:47 - Train Step 102\n",
      "2022-04-25 18:46:32.285 | INFO     | __main__:train:49 - mae 2.94e-01 loss 7.90e-02 R 0.062 gap 0.01856388710439205 preds 0.015579369850456715\n",
      "2022-04-25 18:46:32.343 | INFO     | __main__:train:47 - Train Step 103\n",
      "2022-04-25 18:46:32.343 | INFO     | __main__:train:49 - mae 2.93e-01 loss 7.85e-02 R 0.061 gap 0.018374798819422722 preds 0.015807798132300377\n",
      "2022-04-25 18:46:32.405 | INFO     | __main__:train:47 - Train Step 104\n",
      "2022-04-25 18:46:32.406 | INFO     | __main__:train:49 - mae 2.92e-01 loss 7.80e-02 R 0.062 gap 0.018042175099253654 preds 0.01537636574357748\n",
      "2022-04-25 18:46:32.469 | INFO     | __main__:train:47 - Train Step 105\n",
      "2022-04-25 18:46:32.470 | INFO     | __main__:train:49 - mae 2.91e-01 loss 7.74e-02 R 0.063 gap 0.0179001335054636 preds 0.015088628977537155\n",
      "2022-04-25 18:46:32.534 | INFO     | __main__:train:47 - Train Step 106\n",
      "2022-04-25 18:46:32.535 | INFO     | __main__:train:49 - mae 2.89e-01 loss 7.69e-02 R 0.066 gap 0.017435675486922264 preds 0.014842847362160683\n",
      "2022-04-25 18:46:32.597 | INFO     | __main__:train:47 - Train Step 107\n",
      "2022-04-25 18:46:32.598 | INFO     | __main__:train:49 - mae 2.89e-01 loss 7.66e-02 R 0.064 gap 0.01731480471789837 preds 0.014796802774071693\n",
      "2022-04-25 18:46:32.661 | INFO     | __main__:train:47 - Train Step 108\n",
      "2022-04-25 18:46:32.662 | INFO     | __main__:train:49 - mae 2.88e-01 loss 7.61e-02 R 0.064 gap 0.017461324110627174 preds 0.01467945147305727\n",
      "2022-04-25 18:46:32.726 | INFO     | __main__:train:47 - Train Step 109\n",
      "2022-04-25 18:46:32.727 | INFO     | __main__:train:49 - mae 2.87e-01 loss 7.58e-02 R 0.064 gap 0.017456717789173126 preds 0.014737553894519806\n",
      "2022-04-25 18:46:33.022 | INFO     | __main__:train:47 - Train Step 110\n",
      "2022-04-25 18:46:33.023 | INFO     | __main__:train:49 - mae 2.86e-01 loss 7.54e-02 R 0.064 gap 0.017375413328409195 preds 0.01478674914687872\n",
      "2022-04-25 18:46:33.082 | INFO     | __main__:train:47 - Train Step 111\n",
      "2022-04-25 18:46:33.083 | INFO     | __main__:train:49 - mae 2.85e-01 loss 7.49e-02 R 0.063 gap 0.017244970425963402 preds 0.014456511475145817\n",
      "2022-04-25 18:46:33.146 | INFO     | __main__:train:47 - Train Step 112\n",
      "2022-04-25 18:46:33.147 | INFO     | __main__:train:49 - mae 2.84e-01 loss 7.45e-02 R 0.063 gap 0.01724710874259472 preds 0.014274945482611656\n",
      "2022-04-25 18:46:33.209 | INFO     | __main__:train:47 - Train Step 113\n",
      "2022-04-25 18:46:33.210 | INFO     | __main__:train:49 - mae 2.84e-01 loss 7.41e-02 R 0.063 gap 0.01685686968266964 preds 0.014090200886130333\n",
      "2022-04-25 18:46:33.275 | INFO     | __main__:train:47 - Train Step 114\n",
      "2022-04-25 18:46:33.276 | INFO     | __main__:train:49 - mae 2.83e-01 loss 7.36e-02 R 0.064 gap 0.016650276258587837 preds 0.01387648843228817\n",
      "2022-04-25 18:46:33.338 | INFO     | __main__:train:47 - Train Step 115\n",
      "2022-04-25 18:46:33.339 | INFO     | __main__:train:49 - mae 2.81e-01 loss 7.32e-02 R 0.063 gap 0.016467951238155365 preds 0.013785761781036854\n",
      "2022-04-25 18:46:33.400 | INFO     | __main__:train:47 - Train Step 116\n",
      "2022-04-25 18:46:33.401 | INFO     | __main__:train:49 - mae 2.81e-01 loss 7.29e-02 R 0.062 gap 0.016538502648472786 preds 0.014155583456158638\n",
      "2022-04-25 18:46:33.460 | INFO     | __main__:train:47 - Train Step 117\n",
      "2022-04-25 18:46:33.461 | INFO     | __main__:train:49 - mae 2.80e-01 loss 7.24e-02 R 0.062 gap 0.016464760527014732 preds 0.014065781608223915\n",
      "2022-04-25 18:46:33.522 | INFO     | __main__:train:47 - Train Step 118\n",
      "2022-04-25 18:46:33.523 | INFO     | __main__:train:49 - mae 2.78e-01 loss 7.20e-02 R 0.061 gap 0.016499945893883705 preds 0.013986397534608841\n",
      "2022-04-25 18:46:33.585 | INFO     | __main__:train:47 - Train Step 119\n",
      "2022-04-25 18:46:33.586 | INFO     | __main__:train:49 - mae 2.77e-01 loss 7.16e-02 R 0.059 gap 0.016516556963324547 preds 0.013928821310400963\n",
      "2022-04-25 18:46:33.871 | INFO     | __main__:train:47 - Train Step 120\n",
      "2022-04-25 18:46:33.871 | INFO     | __main__:train:49 - mae 2.76e-01 loss 7.11e-02 R 0.060 gap 0.01650412008166313 preds 0.013676728121936321\n",
      "2022-04-25 18:46:33.930 | INFO     | __main__:train:47 - Train Step 121\n",
      "2022-04-25 18:46:33.931 | INFO     | __main__:train:49 - mae 2.75e-01 loss 7.07e-02 R 0.060 gap 0.016711629927158356 preds 0.0133744515478611\n",
      "2022-04-25 18:46:33.995 | INFO     | __main__:train:47 - Train Step 122\n",
      "2022-04-25 18:46:33.996 | INFO     | __main__:train:49 - mae 2.74e-01 loss 7.03e-02 R 0.059 gap 0.01663872040808201 preds 0.01324339397251606\n",
      "2022-04-25 18:46:34.055 | INFO     | __main__:train:47 - Train Step 123\n",
      "2022-04-25 18:46:34.056 | INFO     | __main__:train:49 - mae 2.73e-01 loss 6.98e-02 R 0.060 gap 0.016886617988348007 preds 0.01329066976904869\n",
      "2022-04-25 18:46:34.118 | INFO     | __main__:train:47 - Train Step 124\n",
      "2022-04-25 18:46:34.119 | INFO     | __main__:train:49 - mae 2.72e-01 loss 6.94e-02 R 0.061 gap 0.016744663938879967 preds 0.013532279059290886\n",
      "2022-04-25 18:46:34.185 | INFO     | __main__:train:47 - Train Step 125\n",
      "2022-04-25 18:46:34.185 | INFO     | __main__:train:49 - mae 2.71e-01 loss 6.90e-02 R 0.061 gap 0.016631972044706345 preds 0.013422956690192223\n",
      "2022-04-25 18:46:34.250 | INFO     | __main__:train:47 - Train Step 126\n",
      "2022-04-25 18:46:34.251 | INFO     | __main__:train:49 - mae 2.70e-01 loss 6.87e-02 R 0.063 gap 0.016504919156432152 preds 0.013156179338693619\n",
      "2022-04-25 18:46:34.317 | INFO     | __main__:train:47 - Train Step 127\n",
      "2022-04-25 18:46:34.318 | INFO     | __main__:train:49 - mae 2.69e-01 loss 6.83e-02 R 0.064 gap 0.01653626747429371 preds 0.013087878935039043\n",
      "2022-04-25 18:46:34.381 | INFO     | __main__:train:47 - Train Step 128\n",
      "2022-04-25 18:46:34.382 | INFO     | __main__:train:49 - mae 2.68e-01 loss 6.79e-02 R 0.065 gap 0.016763456165790558 preds 0.013248217292129993\n",
      "2022-04-25 18:46:34.447 | INFO     | __main__:train:47 - Train Step 129\n",
      "2022-04-25 18:46:34.448 | INFO     | __main__:train:49 - mae 2.67e-01 loss 6.75e-02 R 0.064 gap 0.016751747578382492 preds 0.013282835483551025\n",
      "2022-04-25 18:46:34.751 | INFO     | __main__:train:47 - Train Step 130\n",
      "2022-04-25 18:46:34.752 | INFO     | __main__:train:49 - mae 2.66e-01 loss 6.72e-02 R 0.065 gap 0.016949648037552834 preds 0.013270361348986626\n",
      "2022-04-25 18:46:34.816 | INFO     | __main__:train:47 - Train Step 131\n",
      "2022-04-25 18:46:34.816 | INFO     | __main__:train:49 - mae 2.65e-01 loss 6.68e-02 R 0.065 gap 0.016903432086110115 preds 0.013380986638367176\n",
      "2022-04-25 18:46:34.877 | INFO     | __main__:train:47 - Train Step 132\n",
      "2022-04-25 18:46:34.877 | INFO     | __main__:train:49 - mae 2.64e-01 loss 6.64e-02 R 0.065 gap 0.016928313300013542 preds 0.013484465889632702\n",
      "2022-04-25 18:46:34.940 | INFO     | __main__:train:47 - Train Step 133\n",
      "2022-04-25 18:46:34.941 | INFO     | __main__:train:49 - mae 2.63e-01 loss 6.60e-02 R 0.065 gap 0.016794610768556595 preds 0.013185963965952396\n",
      "2022-04-25 18:46:35.003 | INFO     | __main__:train:47 - Train Step 134\n",
      "2022-04-25 18:46:35.004 | INFO     | __main__:train:49 - mae 2.62e-01 loss 6.56e-02 R 0.065 gap 0.01674964465200901 preds 0.012773009948432446\n",
      "2022-04-25 18:46:35.070 | INFO     | __main__:train:47 - Train Step 135\n",
      "2022-04-25 18:46:35.070 | INFO     | __main__:train:49 - mae 2.62e-01 loss 6.53e-02 R 0.064 gap 0.01679118163883686 preds 0.012763652950525284\n",
      "2022-04-25 18:46:35.132 | INFO     | __main__:train:47 - Train Step 136\n",
      "2022-04-25 18:46:35.132 | INFO     | __main__:train:49 - mae 2.60e-01 loss 6.49e-02 R 0.066 gap 0.01678684912621975 preds 0.012888862751424313\n",
      "2022-04-25 18:46:35.193 | INFO     | __main__:train:47 - Train Step 137\n",
      "2022-04-25 18:46:35.194 | INFO     | __main__:train:49 - mae 2.60e-01 loss 6.47e-02 R 0.066 gap 0.01667865924537182 preds 0.012977281585335732\n",
      "2022-04-25 18:46:35.260 | INFO     | __main__:train:47 - Train Step 138\n",
      "2022-04-25 18:46:35.261 | INFO     | __main__:train:49 - mae 2.59e-01 loss 6.43e-02 R 0.067 gap 0.016712404787540436 preds 0.012942316010594368\n",
      "2022-04-25 18:46:35.330 | INFO     | __main__:train:47 - Train Step 139\n",
      "2022-04-25 18:46:35.331 | INFO     | __main__:train:49 - mae 2.58e-01 loss 6.39e-02 R 0.067 gap 0.016610583290457726 preds 0.013222308829426765\n",
      "2022-04-25 18:46:35.609 | INFO     | __main__:train:47 - Train Step 140\n",
      "2022-04-25 18:46:35.610 | INFO     | __main__:train:49 - mae 2.57e-01 loss 6.36e-02 R 0.068 gap 0.016775403171777725 preds 0.013306062668561935\n",
      "2022-04-25 18:46:35.673 | INFO     | __main__:train:47 - Train Step 141\n",
      "2022-04-25 18:46:35.673 | INFO     | __main__:train:49 - mae 2.56e-01 loss 6.33e-02 R 0.067 gap 0.016528859734535217 preds 0.013418583199381828\n",
      "2022-04-25 18:46:35.738 | INFO     | __main__:train:47 - Train Step 142\n",
      "2022-04-25 18:46:35.738 | INFO     | __main__:train:49 - mae 2.56e-01 loss 6.31e-02 R 0.068 gap 0.01639971137046814 preds 0.013519964180886745\n",
      "2022-04-25 18:46:35.801 | INFO     | __main__:train:47 - Train Step 143\n",
      "2022-04-25 18:46:35.802 | INFO     | __main__:train:49 - mae 2.55e-01 loss 6.28e-02 R 0.068 gap 0.01644117757678032 preds 0.01357389148324728\n",
      "2022-04-25 18:46:35.873 | INFO     | __main__:train:47 - Train Step 144\n",
      "2022-04-25 18:46:35.874 | INFO     | __main__:train:49 - mae 2.54e-01 loss 6.24e-02 R 0.069 gap 0.016461960971355438 preds 0.013662455603480339\n",
      "2022-04-25 18:46:35.941 | INFO     | __main__:train:47 - Train Step 145\n",
      "2022-04-25 18:46:35.942 | INFO     | __main__:train:49 - mae 2.53e-01 loss 6.21e-02 R 0.070 gap 0.016534946858882904 preds 0.013726109638810158\n",
      "2022-04-25 18:46:36.010 | INFO     | __main__:train:47 - Train Step 146\n",
      "2022-04-25 18:46:36.011 | INFO     | __main__:train:49 - mae 2.52e-01 loss 6.18e-02 R 0.069 gap 0.016612783074378967 preds 0.013790094293653965\n",
      "2022-04-25 18:46:36.073 | INFO     | __main__:train:47 - Train Step 147\n",
      "2022-04-25 18:46:36.074 | INFO     | __main__:train:49 - mae 2.52e-01 loss 6.15e-02 R 0.069 gap 0.01651226170361042 preds 0.013791987672448158\n",
      "2022-04-25 18:46:36.137 | INFO     | __main__:train:47 - Train Step 148\n",
      "2022-04-25 18:46:36.138 | INFO     | __main__:train:49 - mae 2.51e-01 loss 6.12e-02 R 0.070 gap 0.016527943313121796 preds 0.013983044773340225\n",
      "2022-04-25 18:46:36.201 | INFO     | __main__:train:47 - Train Step 149\n",
      "2022-04-25 18:46:36.202 | INFO     | __main__:train:49 - mae 2.50e-01 loss 6.10e-02 R 0.069 gap 0.016271712258458138 preds 0.014259389601647854\n",
      "2022-04-25 18:46:36.494 | INFO     | __main__:train:47 - Train Step 150\n",
      "2022-04-25 18:46:36.495 | INFO     | __main__:train:49 - mae 2.49e-01 loss 6.08e-02 R 0.069 gap 0.015971751883625984 preds 0.014279073104262352\n",
      "2022-04-25 18:46:36.555 | INFO     | __main__:train:47 - Train Step 151\n",
      "2022-04-25 18:46:36.556 | INFO     | __main__:train:49 - mae 2.49e-01 loss 6.04e-02 R 0.070 gap 0.01597633771598339 preds 0.014395847916603088\n",
      "2022-04-25 18:46:36.619 | INFO     | __main__:train:47 - Train Step 152\n",
      "2022-04-25 18:46:36.619 | INFO     | __main__:train:49 - mae 2.48e-01 loss 6.01e-02 R 0.071 gap 0.015891563147306442 preds 0.014626608230173588\n",
      "2022-04-25 18:46:36.677 | INFO     | __main__:train:47 - Train Step 153\n",
      "2022-04-25 18:46:36.678 | INFO     | __main__:train:49 - mae 2.47e-01 loss 5.99e-02 R 0.071 gap 0.01602254807949066 preds 0.014755147509276867\n",
      "2022-04-25 18:46:36.741 | INFO     | __main__:train:47 - Train Step 154\n",
      "2022-04-25 18:46:36.742 | INFO     | __main__:train:49 - mae 2.46e-01 loss 5.96e-02 R 0.071 gap 0.01605181209743023 preds 0.014841100201010704\n",
      "2022-04-25 18:46:36.807 | INFO     | __main__:train:47 - Train Step 155\n",
      "2022-04-25 18:46:36.808 | INFO     | __main__:train:49 - mae 2.46e-01 loss 5.93e-02 R 0.071 gap 0.016066255047917366 preds 0.014759805053472519\n",
      "2022-04-25 18:46:36.871 | INFO     | __main__:train:47 - Train Step 156\n",
      "2022-04-25 18:46:36.871 | INFO     | __main__:train:49 - mae 2.45e-01 loss 5.90e-02 R 0.071 gap 0.01601598598062992 preds 0.014724061824381351\n",
      "2022-04-25 18:46:36.936 | INFO     | __main__:train:47 - Train Step 157\n",
      "2022-04-25 18:46:36.937 | INFO     | __main__:train:49 - mae 2.44e-01 loss 5.87e-02 R 0.073 gap 0.01604919508099556 preds 0.014659267850220203\n",
      "2022-04-25 18:46:36.996 | INFO     | __main__:train:47 - Train Step 158\n",
      "2022-04-25 18:46:36.997 | INFO     | __main__:train:49 - mae 2.43e-01 loss 5.84e-02 R 0.072 gap 0.01605760119855404 preds 0.014490415342152119\n",
      "2022-04-25 18:46:37.066 | INFO     | __main__:train:47 - Train Step 159\n",
      "2022-04-25 18:46:37.067 | INFO     | __main__:train:49 - mae 2.42e-01 loss 5.81e-02 R 0.073 gap 0.015808425843715668 preds 0.014249720610678196\n",
      "2022-04-25 18:46:37.361 | INFO     | __main__:train:47 - Train Step 160\n",
      "2022-04-25 18:46:37.362 | INFO     | __main__:train:49 - mae 2.42e-01 loss 5.79e-02 R 0.073 gap 0.015925999730825424 preds 0.014363879337906837\n",
      "2022-04-25 18:46:37.424 | INFO     | __main__:train:47 - Train Step 161\n",
      "2022-04-25 18:46:37.425 | INFO     | __main__:train:49 - mae 2.41e-01 loss 5.76e-02 R 0.073 gap 0.015878716483712196 preds 0.014309991151094437\n",
      "2022-04-25 18:46:37.483 | INFO     | __main__:train:47 - Train Step 162\n",
      "2022-04-25 18:46:37.484 | INFO     | __main__:train:49 - mae 2.40e-01 loss 5.74e-02 R 0.074 gap 0.01588575355708599 preds 0.014573434367775917\n",
      "2022-04-25 18:46:37.545 | INFO     | __main__:train:47 - Train Step 163\n",
      "2022-04-25 18:46:37.546 | INFO     | __main__:train:49 - mae 2.40e-01 loss 5.72e-02 R 0.073 gap 0.015877284109592438 preds 0.014787886291742325\n",
      "2022-04-25 18:46:37.605 | INFO     | __main__:train:47 - Train Step 164\n",
      "2022-04-25 18:46:37.606 | INFO     | __main__:train:49 - mae 2.39e-01 loss 5.70e-02 R 0.073 gap 0.015966974198818207 preds 0.014822215773165226\n",
      "2022-04-25 18:46:37.670 | INFO     | __main__:train:47 - Train Step 165\n",
      "2022-04-25 18:46:37.671 | INFO     | __main__:train:49 - mae 2.39e-01 loss 5.68e-02 R 0.074 gap 0.015874389559030533 preds 0.014865941368043423\n",
      "2022-04-25 18:46:37.732 | INFO     | __main__:train:47 - Train Step 166\n",
      "2022-04-25 18:46:37.732 | INFO     | __main__:train:49 - mae 2.38e-01 loss 5.65e-02 R 0.074 gap 0.015755606815218925 preds 0.01489917654544115\n",
      "2022-04-25 18:46:37.796 | INFO     | __main__:train:47 - Train Step 167\n",
      "2022-04-25 18:46:37.797 | INFO     | __main__:train:49 - mae 2.38e-01 loss 5.64e-02 R 0.073 gap 0.015630358830094337 preds 0.014735184609889984\n",
      "2022-04-25 18:46:37.857 | INFO     | __main__:train:47 - Train Step 168\n",
      "2022-04-25 18:46:37.858 | INFO     | __main__:train:49 - mae 2.37e-01 loss 5.61e-02 R 0.074 gap 0.015534880571067333 preds 0.014526554383337498\n",
      "2022-04-25 18:46:37.921 | INFO     | __main__:train:47 - Train Step 169\n",
      "2022-04-25 18:46:37.922 | INFO     | __main__:train:49 - mae 2.37e-01 loss 5.60e-02 R 0.073 gap 0.015385033562779427 preds 0.01428006961941719\n",
      "2022-04-25 18:46:38.205 | INFO     | __main__:train:47 - Train Step 170\n",
      "2022-04-25 18:46:38.206 | INFO     | __main__:train:49 - mae 2.36e-01 loss 5.58e-02 R 0.074 gap 0.015458567999303341 preds 0.013963073492050171\n",
      "2022-04-25 18:46:38.265 | INFO     | __main__:train:47 - Train Step 171\n",
      "2022-04-25 18:46:38.266 | INFO     | __main__:train:49 - mae 2.36e-01 loss 5.56e-02 R 0.074 gap 0.01548558659851551 preds 0.013722680509090424\n",
      "2022-04-25 18:46:38.323 | INFO     | __main__:train:47 - Train Step 172\n",
      "2022-04-25 18:46:38.323 | INFO     | __main__:train:49 - mae 2.35e-01 loss 5.54e-02 R 0.075 gap 0.01535476092249155 preds 0.013514507561922073\n",
      "2022-04-25 18:46:38.380 | INFO     | __main__:train:47 - Train Step 173\n",
      "2022-04-25 18:46:38.381 | INFO     | __main__:train:49 - mae 2.35e-01 loss 5.52e-02 R 0.076 gap 0.015372805297374725 preds 0.013361970894038677\n",
      "2022-04-25 18:46:38.437 | INFO     | __main__:train:47 - Train Step 174\n",
      "2022-04-25 18:46:38.438 | INFO     | __main__:train:49 - mae 2.34e-01 loss 5.49e-02 R 0.076 gap 0.01560627855360508 preds 0.013246795162558556\n",
      "2022-04-25 18:46:38.498 | INFO     | __main__:train:47 - Train Step 175\n",
      "2022-04-25 18:46:38.499 | INFO     | __main__:train:49 - mae 2.34e-01 loss 5.48e-02 R 0.076 gap 0.015596084296703339 preds 0.013306556269526482\n",
      "2022-04-25 18:46:38.560 | INFO     | __main__:train:47 - Train Step 176\n",
      "2022-04-25 18:46:38.561 | INFO     | __main__:train:49 - mae 2.33e-01 loss 5.46e-02 R 0.075 gap 0.015815619379281998 preds 0.013248568400740623\n",
      "2022-04-25 18:46:38.615 | INFO     | __main__:train:47 - Train Step 177\n",
      "2022-04-25 18:46:38.616 | INFO     | __main__:train:49 - mae 2.33e-01 loss 5.44e-02 R 0.075 gap 0.01581350527703762 preds 0.013224887661635876\n",
      "2022-04-25 18:46:38.675 | INFO     | __main__:train:47 - Train Step 178\n",
      "2022-04-25 18:46:38.676 | INFO     | __main__:train:49 - mae 2.32e-01 loss 5.41e-02 R 0.076 gap 0.015838192775845528 preds 0.013222788460552692\n",
      "2022-04-25 18:46:38.734 | INFO     | __main__:train:47 - Train Step 179\n",
      "2022-04-25 18:46:38.735 | INFO     | __main__:train:49 - mae 2.31e-01 loss 5.38e-02 R 0.076 gap 0.015912165865302086 preds 0.013090976513922215\n",
      "2022-04-25 18:46:39.032 | INFO     | __main__:train:47 - Train Step 180\n",
      "2022-04-25 18:46:39.032 | INFO     | __main__:train:49 - mae 2.31e-01 loss 5.36e-02 R 0.075 gap 0.01565968245267868 preds 0.013146108016371727\n",
      "2022-04-25 18:46:39.092 | INFO     | __main__:train:47 - Train Step 181\n",
      "2022-04-25 18:46:39.093 | INFO     | __main__:train:49 - mae 2.30e-01 loss 5.34e-02 R 0.076 gap 0.01595701277256012 preds 0.013227338902652264\n",
      "2022-04-25 18:46:39.153 | INFO     | __main__:train:47 - Train Step 182\n",
      "2022-04-25 18:46:39.154 | INFO     | __main__:train:49 - mae 2.29e-01 loss 5.31e-02 R 0.077 gap 0.015865499153733253 preds 0.01329041551798582\n",
      "2022-04-25 18:46:39.209 | INFO     | __main__:train:47 - Train Step 183\n",
      "2022-04-25 18:46:39.210 | INFO     | __main__:train:49 - mae 2.29e-01 loss 5.29e-02 R 0.078 gap 0.015803033486008644 preds 0.013311916030943394\n",
      "2022-04-25 18:46:39.274 | INFO     | __main__:train:47 - Train Step 184\n",
      "2022-04-25 18:46:39.275 | INFO     | __main__:train:49 - mae 2.28e-01 loss 5.27e-02 R 0.079 gap 0.015754440799355507 preds 0.01314577553421259\n",
      "2022-04-25 18:46:39.337 | INFO     | __main__:train:47 - Train Step 185\n",
      "2022-04-25 18:46:39.338 | INFO     | __main__:train:49 - mae 2.28e-01 loss 5.25e-02 R 0.079 gap 0.015779541805386543 preds 0.013159257359802723\n",
      "2022-04-25 18:46:39.402 | INFO     | __main__:train:47 - Train Step 186\n",
      "2022-04-25 18:46:39.402 | INFO     | __main__:train:49 - mae 2.27e-01 loss 5.23e-02 R 0.081 gap 0.015912791714072227 preds 0.013158989138901234\n",
      "2022-04-25 18:47:00.264 | INFO     | __main__:test:55 - Test epoch 1\n",
      "2022-04-25 18:47:00.265 | INFO     | __main__:test:57 - mae 1.21e-01 loss 1.29e-02 R 0.005 l_test 9.24e-01 l_train 9.40e-01 \n"
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    train(task_loader)\n",
    "    test(epoch, task_loader)\n",
    "\n",
    "    with open(\"logs_{}.json\".format(experiment), \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using trained NC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the meta_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(meta_test[\"train\"], meta_test[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "task_count = len(data)\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    # print(train_file)\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    train_data.scale(kind)\n",
    "    test_data.scale(kind)\n",
    "    # print(train_data, test_data)\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:38.091 | INFO     | __main__:<module>:29 - Dataset loading took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "## Populating the dataset\n",
    "regularizer = \"\"#\"NC\"\n",
    "\n",
    "\n",
    "model = torch.load(model_path).to(device)\n",
    "# model = NeuralComplexity1D().to(device)\n",
    "# model.load_state_dict(saved_state_dict)\n",
    "\n",
    "task_loader = []\n",
    "\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "ce_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "global_timestamp = timer()\n",
    "\n",
    "logger.info(f\"Dataset loading took {timer() - global_timestamp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Task Learner with NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_learner_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "        \n",
    "    h_crit_reg = nn.MSELoss(reduction=\"none\")\n",
    "    h_crit_cls = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    h.train()\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "\n",
    "        h_loss, l_train = get_task_loss(preds_train, y_train, h_crit_reg, h_crit_cls)\n",
    "\n",
    "        if regularizer == \"NC\":\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "            te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "            tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "            tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "            meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": l_train}\n",
    "            model_preds = model(meta_batch)\n",
    "\n",
    "            nc_regularization = model_preds.sum()\n",
    "            h_loss += nc_regularization\n",
    "        \n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reg(metric_reg, mse):\n",
    "    for i in range(6):\n",
    "        mse[i].append(metric_reg[i])\n",
    "    return mse\n",
    "\n",
    "def combine_cls_preds(preds, preds_test):\n",
    "    for cls_num in range(4):\n",
    "        current_preds = preds_test[:, cls_num*4:cls_num*4+4]\n",
    "        current_preds = current_preds.argmax(dim=-1).squeeze().cpu().tolist()\n",
    "        preds[cls_num].extend(current_preds)\n",
    "    return preds\n",
    "\n",
    "def combine_cls_labels(labels, labels_test):\n",
    "    for cls_num in range(4):\n",
    "        current_labels = labels_test[:, cls_num].squeeze().cpu().tolist()\n",
    "        labels[cls_num].extend(current_labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test, losses_train = [], []\n",
    "preds = [[], [], [], []]\n",
    "labels = [[], [], [], []]\n",
    "mse = [[] for i in range(6)]\n",
    "\n",
    "h = get_learner(\n",
    "    layers= learner_layers,\n",
    "    hidden_size= learner_hidden,\n",
    "    init_dim=xtrain_dim,\n",
    "    task='flat',\n",
    ").to(device)\n",
    "\n",
    "h_opt = torch.optim.Adam(h.parameters(), lr= inner_lr)\n",
    "\n",
    "for tasks in task_loader:\n",
    "    for batch in tasks:\n",
    "        x_train, y_train = batch[\"train\"][0].cuda(), batch[\"train\"][1].cuda()\n",
    "        x_test, y_test = batch[\"test\"][0].cuda(), batch[\"test\"][1].cuda()\n",
    "        h = train_task_learner_timeseries(batch)\n",
    "        with torch.no_grad():\n",
    "            h.eval()\n",
    "            preds_train = h(x_train)\n",
    "            preds_test = h(x_test)\n",
    "\n",
    "        reg_loss_te = mse_criterion(preds_test[:, :6].squeeze(), y_test[:, :6].squeeze())\n",
    "        metric_reg = reg_loss_te.mean(0).squeeze()\n",
    "        mse = combine_reg(metric_reg, mse)\n",
    "\n",
    "        cls1_loss_te = ce_criterion(preds_test[:, 6:10].squeeze(), y_test[:, 6].squeeze().long())\n",
    "        cls2_loss_te = ce_criterion(preds_test[:, 10:14].squeeze(), y_test[:, 7].squeeze().long())\n",
    "        cls3_loss_te = ce_criterion(preds_test[:, 14:18].squeeze(), y_test[:, 8].squeeze().long())\n",
    "        cls4_loss_te = ce_criterion(preds_test[:, 18:22].squeeze(), y_test[:, 9].squeeze().long())\n",
    "\n",
    "        preds = combine_cls_preds(preds, preds_test[:, 6:])\n",
    "        labels = combine_cls_labels(labels, y_test[:, 6:])\n",
    "\n",
    "        l_test = (reg_loss_te.mean(-1).sum() + cls1_loss_te.mean(-1).sum() + cls2_loss_te.mean(-1).sum() + cls3_loss_te.mean(-1).sum() + cls4_loss_te.mean(-1).sum())/160\n",
    "        losses_test.append(l_test.item())\n",
    "        \n",
    "        reg_loss_tr = mse_criterion(preds_train[:, :6].squeeze(), y_train[:, :6].squeeze())\n",
    "        cls1_loss_tr = ce_criterion(preds_train[:, 6:10].squeeze(), y_train[:, 6].squeeze().long())\n",
    "        cls2_loss_tr = ce_criterion(preds_train[:, 10:14].squeeze(), y_train[:, 7].squeeze().long())\n",
    "        cls3_loss_tr = ce_criterion(preds_train[:, 14:18].squeeze(), y_train[:, 8].squeeze().long())\n",
    "        cls4_loss_tr = ce_criterion(preds_train[:, 18:22].squeeze(), y_train[:, 9].squeeze().long())\n",
    "        \n",
    "        l_train =  (reg_loss_tr.mean(-1).sum() + cls1_loss_tr.mean(-1).sum() + cls2_loss_tr.mean(-1).sum() + cls3_loss_tr.mean(-1).sum() + cls4_loss_tr.mean(-1).sum())/160\n",
    "        losses_train.append(l_train.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:54:47.543 | INFO     | __main__:<module>:9 - Test 0.0844 +- 0.0053\n",
      "2022-04-25 18:54:47.544 | INFO     | __main__:<module>:10 - Train 0.0779 +- 0.0052\n"
     ]
    }
   ],
   "source": [
    "losses_test = np.array(losses_test)\n",
    "losses_train = np.array(losses_train)\n",
    "\n",
    "\n",
    "t_mean = losses_test.mean()\n",
    "t_conf = losses_test.std() * 1.96 / np.sqrt(len(losses_test))\n",
    "c_mean = losses_train.mean()\n",
    "c_conf = losses_train.std() * 1.96 / np.sqrt(len(losses_train))\n",
    "logger.info(f\"Test {t_mean:.4f} +- {t_conf:.4f}\")\n",
    "logger.info(f\"Train {c_mean:.4f} +- {c_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.plot(range(losses_test.shape[0]), losses_test, label=\"test\");\n",
    "plt.plot(range(losses_train.shape[0]), losses_train, label=\"train\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Steps\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (For Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Dimension 1: 0.2027\n",
      "MSE for Dimension 2: 0.2800\n",
      "MSE for Dimension 3: 0.2775\n",
      "MSE for Dimension 4: 0.2494\n",
      "MSE for Dimension 5: 0.2589\n",
      "MSE for Dimension 6: 0.1913\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"MSE for Dimension {i+1}: {torch.mean(torch.tensor(mse[i])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Dimension 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.26      0.35      6826\n",
      "         1.0       0.24      0.14      0.18      2121\n",
      "         2.0       0.07      0.02      0.03      1717\n",
      "         3.0       0.04      0.66      0.08       408\n",
      "\n",
      "    accuracy                           0.21     11072\n",
      "   macro avg       0.23      0.27      0.16     11072\n",
      "weighted avg       0.41      0.21      0.26     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.00      0.00      8096\n",
      "         1.0       0.06      0.02      0.03       469\n",
      "         2.0       0.07      1.00      0.14       790\n",
      "         3.0       0.68      0.02      0.04      1717\n",
      "\n",
      "    accuracy                           0.08     11072\n",
      "   macro avg       0.45      0.26      0.05     11072\n",
      "weighted avg       0.84      0.08      0.02     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.01      0.01      2716\n",
      "         1.0       0.46      0.27      0.34      4925\n",
      "         2.0       0.03      0.01      0.01      1293\n",
      "         3.0       0.21      0.79      0.34      2138\n",
      "\n",
      "    accuracy                           0.27     11072\n",
      "   macro avg       0.30      0.27      0.17     11072\n",
      "weighted avg       0.37      0.27      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Dimension 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.08      0.13      4859\n",
      "         1.0       0.11      0.20      0.14      1442\n",
      "         2.0       0.05      0.23      0.08       561\n",
      "         3.0       0.35      0.41      0.38      4210\n",
      "\n",
      "    accuracy                           0.23     11072\n",
      "   macro avg       0.22      0.23      0.18     11072\n",
      "weighted avg       0.32      0.23      0.22     11072\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Classification Report for Dimension {6+i}:\")\n",
    "    print(classification_report(labels[i], preds[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6387572d3ba60263f2472b530ce49454bee9bd13656fbfcf29efcd586b712758"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mtl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

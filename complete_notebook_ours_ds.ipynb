{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import os, pickle\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {\"train\":{}, \"test\":{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDS(Dataset):\n",
    "    def __init__(self, XL,yL,flatten=False,lno=None,long=True):\n",
    "        self.samples=[]\n",
    "        self.labels=[]\n",
    "        self.flatten=flatten\n",
    "        self.lno=lno\n",
    "        self.long=long\n",
    "        self.scaler = StandardScaler()\n",
    "        for X,Y in zip(XL,yL):\n",
    "            self.samples += [torch.tensor(X).float()]\n",
    "            self.labels += [torch.tensor(Y)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return sum([s.shape[0] for s in self.samples])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.flatten: sample=self.samples[idx].flatten(start_dim=1)\n",
    "        else: sample=self.samples[idx]\n",
    "        if self.lno==None: label=self.labels[idx]\n",
    "        elif self.long: label=self.labels[idx][:,self.lno].long()\n",
    "        else: label=self.labels[idx][:,self.lno].float()\n",
    "        return (sample,label)\n",
    "\n",
    "    def fit(self,kind='seq'):\n",
    "        if kind=='seq':\n",
    "            self.lastelems=[torch.cat([s[:,-1,:] for s in self.samples],dim=0)]\n",
    "            self.scaler.fit(torch.cat([le for le in self.lastelems],dim=0))            \n",
    "        elif kind=='flat': self.scaler.fit(torch.cat([s for s in self.samples],dim=0))\n",
    "    def scale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.transform(s)).float() for s in self.samples]\n",
    "    def unscale(self,kind='flat',scaler=None):\n",
    "        def cs(s):\n",
    "            return (s.shape[0]*s.shape[1],s.shape[2])\n",
    "        if scaler==None: scaler=self.scaler\n",
    "        if kind=='seq':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s.reshape(cs(s))).reshape(s.shape)).float() for s in self.samples]\n",
    "            pass\n",
    "        elif kind=='flat':\n",
    "            self.samples=[torch.tensor(scaler.inverse_transform(s)).float() for s in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.metrics = defaultdict(lambda: [])\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self.metrics[key] += value\n",
    "\n",
    "    def add_dict(self, dict):\n",
    "        for key, value in dict.items():\n",
    "            self.add(key, value)\n",
    "\n",
    "    def mean(self, key):\n",
    "        return np.mean(self.metrics[key])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.metrics[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def get_dict(self):\n",
    "        return copy.deepcopy(dict(self.metrics))\n",
    "\n",
    "    def items(self):\n",
    "        return self.metrics.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(dict(self.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(name):\n",
    "    splitted = name.split('_')\n",
    "    g, d = (splitted[2]), int(splitted[3])\n",
    "    return g, d\n",
    "\n",
    "folder_path = os.path.join(\"marketdata\")\n",
    "l = os.listdir(folder_path)\n",
    "\n",
    "data_type = \"ds\"\n",
    "meta_train = {\"train\": [], \"test\": []}\n",
    "meta_test = {\"train\": [], \"test\": []}\n",
    "\n",
    "for file in l:\n",
    "    if data_type in file:\n",
    "        type_ = \"train\" if \"train\" in file else \"test\"\n",
    "        g, d = get_numbers(file)\n",
    "        if d < 20: # for meta-training\n",
    "            meta_train[type_].append(file)\n",
    "        else: # for meta-testing\n",
    "            meta_test[type_].append(file)\n",
    "\n",
    "\n",
    "meta_train[\"train\"] = sorted(meta_train[\"train\"])\n",
    "meta_train[\"test\"] = sorted(meta_train[\"test\"])\n",
    "\n",
    "data = list(zip(meta_train[\"train\"], meta_train[\"test\"]))\n",
    "data = sorted(data, key=lambda x: get_numbers(x[0])[1])\n",
    "idx = 0\n",
    "\n",
    "def load_task(task):\n",
    "    \"\"\"\n",
    "    task is a tuple of strings of the form (train_cs_g_d_2.pkl, test_cs_g_d_2.pkl)\n",
    "    returns X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    train_file, test_file = task\n",
    "    train_data = pickle.load(open(os.path.join(folder_path, train_file), \"rb\"))\n",
    "    test_data = pickle.load(open(os.path.join(folder_path, test_file), \"rb\"))\n",
    "    return train_data.samples, train_data.labels, test_data.samples, test_data.labels\n",
    "\n",
    "def sample_task():\n",
    "    global idx\n",
    "    if idx >= len(data):\n",
    "        idx = 0\n",
    "    task = data[idx]\n",
    "    idx += 1\n",
    "    \n",
    "    return load_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_query, dim_key, dim_value, dim_output, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_query, dim_output, bias=False)\n",
    "        self.fc_k = nn.Linear(dim_key, dim_output, bias=False)\n",
    "        self.fc_v = nn.Linear(dim_value, dim_output, bias=False)\n",
    "        self.fc_o = nn.Linear(dim_output, dim_output)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        query_ = torch.cat(query.chunk(self.num_heads, -1), 0)\n",
    "        key_ = torch.cat(key.chunk(self.num_heads, -1), 0)\n",
    "        value_ = torch.cat(value.chunk(self.num_heads, -1), 0)\n",
    "\n",
    "        A_logits = (query_ @ key_.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
    "        if mask is not None:\n",
    "            mask = torch.stack([mask.squeeze(-1)] * query.shape[-2], -2)\n",
    "            mask = torch.cat([mask] * self.num_heads, 0)\n",
    "            A_logits.masked_fill(mask, -float(\"inf\"))\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "        else:\n",
    "            A = torch.softmax(A_logits, -1)\n",
    "\n",
    "        outs = torch.cat((A @ value_).chunk(self.num_heads, 0), -1)\n",
    "        outs = query + outs\n",
    "        outs = outs + F.relu(self.fc_o(outs))\n",
    "        return outs\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds):\n",
    "        super().__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mha = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.size(0)\n",
    "        query = self.S.repeat(batch_size, 1, 1)\n",
    "        return self.mha(query, X, X).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ohe(y_tr):\n",
    "    output = y_tr[:, :6]\n",
    "    y_tr_cls = y_tr[:, 6:]\n",
    "    y_tr_new = torch.zeros((y_tr.shape[0], 16))\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        for j in range(4):\n",
    "            y_tr_new[i, 4*j+int(y_tr_cls[i, j])] = 1\n",
    "    return torch.cat((output, y_tr_new), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_stack(num_layers, input_dim, hidden_dim, output_dim):\n",
    "    if num_layers == 0:\n",
    "        return nn.Identity()\n",
    "    elif num_layers == 1:\n",
    "        return nn.Linear(input_dim, output_dim)\n",
    "    else:\n",
    "        modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        modules.append(nn.Linear(hidden_dim, output_dim))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "class CrossAttEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = hid_dim\n",
    "        self.bilinear = nn.Bilinear(360, 43, 380)\n",
    "        self.mlp_v = fc_stack(enc_depth, 380, dim, dim)\n",
    "        self.mlp_qk = fc_stack(enc_depth, 370, dim, dim)\n",
    "        self.attn = MultiHeadAttention(dim, dim, dim, dim, num_heads)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_tr, y_tr, train_pred = inputs[\"tr_xyp\"][:, :-20], inputs[\"tr_xyp\"][:, -20:-10], inputs[\"tr_xyp\"][:, -10:]\n",
    "        q = self.mlp_qk(inputs[\"te_xp\"])\n",
    "        k = self.mlp_qk(inputs[\"tr_xp\"])\n",
    "        \n",
    "        y_tr = convert_y_ohe(y_tr)\n",
    "        tr_loss = inputs[\"tr_loss\"]\n",
    "        # print(y_tr.shape, tr_loss.shape, )\n",
    "        bilinear_input = torch.cat((y_tr, torch.ones((y_tr.shape[0], 1)), tr_loss, train_pred), 1)\n",
    "        bilinear_output = self.bilinear(x_tr, bilinear_input)\n",
    "        v = self.mlp_v(bilinear_output)\n",
    "        \n",
    "        out = self.attn(q, k, v)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MeanPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assert len(x.shape) == 3\n",
    "        return x.mean(0)\n",
    "\n",
    "\n",
    "class NeuralComplexity1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bs = batch_size\n",
    "        self.encoder = CrossAttEncoder()\n",
    "\n",
    "        if pool == \"pma\":\n",
    "            self.pool = PMA(dim=hid_dim, num_heads=num_heads, num_seeds=1)\n",
    "        elif pool == \"mean\":\n",
    "            self.pool = MeanPool()\n",
    "\n",
    "        self.decoder = fc_stack(dec_depth, hid_dim, hid_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # print(\"input shape:\", inputs[\"te_xp\"].shape)\n",
    "        x = self.encoder(inputs)\n",
    "        # print(\"encoded shape:\", x.shape)\n",
    "        # x = self.pool(x)\n",
    "        # print(\"pool shape:\", x.shape)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(batch_size, layers, hidden_size, activation=None, regularizer=None, task='regression', init_dim=23, num_outputs=10, seq_len=20):\n",
    "    if activation == \"relu\":\n",
    "        activation = nn.ReLU\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation = nn.Sigmoid\n",
    "    elif activation == \"tanh\":\n",
    "        activation = nn.Tanh\n",
    "    elif activation is None:\n",
    "        activation = nn.Identity\n",
    "    else:\n",
    "        raise ValueError(f\"activation={activation} not implemented!\")\n",
    "        \n",
    "    if task == 'regression':\n",
    "        return RegressionNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "            init_dim=init_dim,\n",
    "            num_outputs=num_outputs,\n",
    "        )\n",
    "    elif task == 'timeseries':\n",
    "        return TimeSeries(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            n_features=init_dim,\n",
    "            num_outputs=num_outputs,\n",
    "            seq_length=seq_len\n",
    "        )\n",
    "    elif task == 'classification':\n",
    "        raise NotImplementedError\n",
    "        return ParallelNeuralNetwork(\n",
    "            batch_size,\n",
    "            num_layers=layers,\n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation,\n",
    "            regularizer=regularizer,\n",
    "            output_activation=nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "class RegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, batch_size, num_layers, init_dim, hidden_size, activation, num_outputs, regularizer=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(init_dim, hidden_size))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(activation())\n",
    "            self.layers.append(\n",
    "                nn.Linear(hidden_size, hidden_size)\n",
    "            )\n",
    "            if regularizer == \"dropout\":\n",
    "                self.layers.append(nn.Dropout())\n",
    "\n",
    "        self.layers.append(activation())\n",
    "        self.layers.append(nn.Linear(hidden_size, num_outputs))\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # print(f\"In layer {i+1}, Shape={x.shape}\", end=\" \")\n",
    "            x = layer(x)\n",
    "            # print(f\"Ouput Shape = {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TimeSeries(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_layers=1, hidden_size=20,\n",
    "                n_features=18,\n",
    "                num_outputs=10,\n",
    "                seq_length=20):\n",
    "        super(TimeSeries, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = hidden_size\n",
    "        self.n_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.linear = torch.nn.Linear(self.n_hidden*self.seq_len, num_outputs)\n",
    "        # self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden_state = torch.zeros(self.n_layers,self.batch_size,self.n_hidden).to(device)\n",
    "        cell_state = torch.zeros(self.n_layers,self.batch_size,self.n_hidden).to(device)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        lstm_out, _ = self.lstm(x,self.hidden)\n",
    "        # print(\"last_out:\", lstm_out.shape)\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        # print(\"x\")\n",
    "        x = self.linear(x)\n",
    "        # reg = x[:, :6]\n",
    "        # cls = self.softmax(x[:, 6:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '7'\n",
    "batch_size = 32\n",
    "task_batch_size = 32\n",
    "lr = 0.0005\n",
    "time_budget = 10000000000.0\n",
    "task = 'sine'\n",
    "nc_regularize = True\n",
    "epochs = 1\n",
    "train_steps = 1\n",
    "log_steps = 1\n",
    "test_steps = 1\n",
    "learn_freq = 1\n",
    "inner_lr = 0.01\n",
    "inner_steps = 16\n",
    "nc_weight = 1.0\n",
    "learner_layers = 2\n",
    "learner_hidden = 40\n",
    "learner_act = 'relu'\n",
    "input = 'cross_att'\n",
    "enc = 'fc'\n",
    "pool = 'mean'\n",
    "dec = 'fc'\n",
    "enc_depth = 3\n",
    "dec_depth = 2\n",
    "hid_dim = 512\n",
    "num_heads = 8\n",
    "model_path = f\"results/model.ckpt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    \"\"\"\n",
    "    Memory bank class. Stores snapshots of task learners.\n",
    "    get_batch() returns a random minibatch of (snapshot, gap) for NC to train on.\n",
    "    \"\"\"\n",
    "\n",
    "    def add(self, te_xp, tr_xp, tr_xyp, gap, l_train):\n",
    "        if not hasattr(self, \"te_xp\"):\n",
    "            self.te_xp = te_xp\n",
    "            self.tr_xp = tr_xp\n",
    "            self.tr_xyp = tr_xyp\n",
    "            self.gap = gap\n",
    "            self.l_train = l_train\n",
    "        else:\n",
    "            self.te_xp = torch.cat([self.te_xp, te_xp], dim=0)\n",
    "            self.tr_xp = torch.cat([self.tr_xp, tr_xp], dim=0)\n",
    "            self.tr_xyp = torch.cat([self.tr_xyp, tr_xyp], dim=0)\n",
    "            self.gap = torch.cat([self.gap, gap], dim=0)\n",
    "            self.l_train = torch.cat([self.l_train, l_train], dim=0)\n",
    "\n",
    "            MEMORY_LIMIT = 1_000_000\n",
    "            if self.te_xp.shape[0] > MEMORY_LIMIT:\n",
    "                self.te_xp = self.te_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xp = self.tr_xp[-MEMORY_LIMIT:]\n",
    "                self.tr_xyp = self.tr_xyp[-MEMORY_LIMIT:]\n",
    "                self.gap = self.gap[-MEMORY_LIMIT:]\n",
    "                self.l_train = self.l_train[-MEMORY_LIMIT:]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        N = self.te_xp.shape[0]\n",
    "        assert N == self.tr_xp.shape[0]\n",
    "        assert N == self.tr_xyp.shape[0]\n",
    "        assert N == self.gap.shape[0]\n",
    "\n",
    "        idxs = random.sample(range(N), k=batch_size)\n",
    "        batch = {\n",
    "            \"te_xp\": self.te_xp[idxs].to(device),\n",
    "            \"tr_xp\": self.tr_xp[idxs].to(device),\n",
    "            \"tr_xyp\": self.tr_xyp[idxs].to(device),\n",
    "            \"tr_loss\": self.l_train[idxs].to(device),\n",
    "        }\n",
    "        return (batch, self.gap[idxs].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regression and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_timeseries(batch, train=True):\n",
    "    x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    h = get_learner(\n",
    "        batch_size=x_train.shape[0],\n",
    "        layers=1,\n",
    "        hidden_size=20,\n",
    "        init_dim=18,\n",
    "        num_outputs=10,\n",
    "        task='timeseries'\n",
    "    ).to(device)\n",
    "    h.init_hidden()\n",
    "    h_opt = torch.optim.SGD(h.parameters(), lr= inner_lr)\n",
    "    h_crit = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    logger.info(f\"X_train shape: {x_train.shape}, Y_train shape: {y_train.shape}, X_test shape: {x_test.shape}, Y_test shape: {y_test.shape}\")\n",
    "\n",
    "    for _ in range( inner_steps):\n",
    "        preds_train = h(x_train)\n",
    "        preds_test = h(x_test)\n",
    "\n",
    "        # logger.info(f\"Preds_train shape: {preds_train.shape}, Preds_test shape: {preds_test.shape}\")\n",
    "        te_xp = torch.cat([x_test.contiguous().view(batch_size, -1), preds_test], dim=-1)\n",
    "        tr_xp = torch.cat([x_train.contiguous().view(batch_size, -1), preds_train], dim=-1)\n",
    "        tr_xyp = torch.cat([x_train.contiguous().view(batch_size, -1), y_train, preds_train], dim=-1)\n",
    "\n",
    "        # logger.info(f\"Te_xp shape: {te_xp.shape}, Tr_xp shape: {tr_xp.shape}, Tr_xyp shape: {tr_xyp.shape}\")\n",
    "\n",
    "        h_loss = h_crit(preds_train.squeeze(), y_train.squeeze())\n",
    "        train_loss = h_loss.detach()\n",
    "        meta_batch = {\"te_xp\": te_xp, \"tr_xp\": tr_xp, \"tr_xyp\": tr_xyp, \"tr_loss\": train_loss}\n",
    "\n",
    "        h_loss = h_loss.mean(-1).sum()\n",
    "        # print(\"te_xp:\", te_xp.shape)\n",
    "\n",
    "        # if  nc_regularize and global_step >  train_steps * 2:\n",
    "        model_preds = model(meta_batch)\n",
    "        # logger.info(f\"Model_preds shape: {model_preds.shape}\")\n",
    "        # We sum NC outputs across tasks because h_loss is also summed.\n",
    "        nc_regularization = model_preds.sum()\n",
    "        h_loss += nc_regularization *  nc_weight\n",
    "\n",
    "        h_opt.zero_grad()\n",
    "        h_loss.backward()\n",
    "        h_opt.step()\n",
    "\n",
    "        l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "        l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "        # logger.info(f\"L_test shape: {l_test.shape}, L_train shape: {l_train.shape}\")\n",
    "        gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "        if train:\n",
    "            memory_bank.add(\n",
    "                te_xp=te_xp.cpu().detach(),\n",
    "                tr_xp=tr_xp.cpu().detach(),\n",
    "                tr_xyp=tr_xyp.cpu().detach(),\n",
    "                gap=gap.cpu().detach(),\n",
    "                l_train=train_loss.cpu().detach(),\n",
    "            )\n",
    "    return h, meta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralComplexity1D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "mse_criterion = nn.MSELoss(reduction=\"none\")\n",
    "huber_criterion = nn.HuberLoss(reduction='none')\n",
    "mae_criterion = nn.L1Loss()\n",
    "global_timestamp = timer()\n",
    "global_step = 0\n",
    "accum = Accumulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(type_=\"train\", metrics={}):\n",
    "    dict = tracker[type_]\n",
    "    for k, v in metrics.items():\n",
    "        if k not in dict:\n",
    "            dict[k] = []\n",
    "        dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_tasks):\n",
    "    test_accum = Accumulator()\n",
    "    for task in test_tasks:\n",
    "        for batch in task:\n",
    "            h, meta_batch = run_regression_timeseries(batch, train=False)\n",
    "\n",
    "            x_train, y_train = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "            x_test, y_test = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "            with torch.no_grad():\n",
    "                preds_train = h(x_train)\n",
    "                preds_test = h(x_test)\n",
    "\n",
    "                l_train = mse_criterion(preds_train.squeeze(), y_train.squeeze())\n",
    "                l_test = mse_criterion(preds_test.squeeze(), y_test.squeeze())\n",
    "                gap = l_test.mean(-1) - l_train.mean(-1)\n",
    "\n",
    "                model_preds = model(meta_batch)\n",
    "                loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "                mae = mae_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            test_accum.add_dict(\n",
    "                {\n",
    "                    \"l_test\": [l_test.mean(-1).detach().cpu()],\n",
    "                    \"l_train\": [l_train.mean(-1).detach().cpu()],\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_gaps = torch.cat(test_accum[\"gap\"])\n",
    "    all_preds = torch.cat(test_accum[\"pred\"])\n",
    "    R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "    mean_l_test = torch.cat(test_accum[\"l_test\"]).mean()\n",
    "    mean_l_train = torch.cat(test_accum[\"l_train\"]).mean()\n",
    "\n",
    "\n",
    "    logger.info(f\"Test epoch {epoch}\")\n",
    "    logger.info(\n",
    "        f\"mae {test_accum.mean('mae'):.2e} loss {test_accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "        f\"l_test {mean_l_test:.2e} l_train {mean_l_train:.2e} \"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": test_accum.mean(\"mae\"),\n",
    "        \"loss\": test_accum.mean(\"loss\"),\n",
    "        \"R\": R,\n",
    "        \"l_test\": mean_l_test.item(),\n",
    "        \"l_train\": mean_l_train.item(),\n",
    "    }\n",
    "    log_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    # This is the inner loop (basically this is the train_epoch function)\n",
    "    global global_step\n",
    "    for task in train_loader:\n",
    "        for batch in task:\n",
    "            global_step += 1\n",
    "            if global_step %  learn_freq == 0: # run the predictor after every 10 batches\n",
    "                run_regression_timeseries(batch)\n",
    "\n",
    "            meta_batch, gap = memory_bank.get_batch( batch_size)\n",
    "            model_preds = model(meta_batch)\n",
    "            loss = huber_criterion(model_preds.squeeze(), gap.squeeze()).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = mae_criterion(model_preds.squeeze(), gap.squeeze())\n",
    "            accum.add_dict(\n",
    "                {\n",
    "                    \"mae\": [mae.item()],\n",
    "                    \"loss\": [loss.item()],\n",
    "                    \"gap\": [gap.squeeze().detach().cpu()],\n",
    "                    \"pred\": [model_preds.squeeze().detach().cpu()],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if global_step % log_steps == 0:\n",
    "                # torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                all_gaps = torch.cat(accum[\"gap\"])\n",
    "                all_preds = torch.cat(accum[\"pred\"])\n",
    "                R = np.corrcoef(all_gaps, all_preds)[0, 1]\n",
    "                logger.info(f\"Train Step {global_step}\")\n",
    "                logger.info(\n",
    "                    f\"mae {accum.mean('mae'):.2e} loss {accum.mean('loss'):.2e} R {R:.3f} \"\n",
    "                )\n",
    "\n",
    "                metrics = {\n",
    "                    \"mae\": accum.mean(\"mae\"),\n",
    "                    \"loss\": accum.mean(\"loss\"),\n",
    "                    \"R\": R,\n",
    "                }\n",
    "                log_metrics(\"train\", metrics)\n",
    "                # print(metrics)\n",
    "\n",
    "            if timer() - global_timestamp >  time_budget:\n",
    "                logger.info(f\"Stopping at step {global_step}\")\n",
    "                quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2022-04-24 16:51:23.268 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:23.281 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:23.281 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:23.587 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:24.110 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:24.113 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:24.114 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:24.330 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:25.030 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:25.039 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:25.040 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:25.355 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:25.797 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:25.807 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:25.808 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:26.004 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:26.641 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:26.645 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:26.646 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:26.845 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:27.354 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:27.363 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:27.363 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:27.505 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:28.157 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:28.164 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:28.165 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:28.346 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:28.824 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:28.828 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:28.829 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:29.013 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:29.821 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:29.830 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:29.831 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:30.046 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:30.694 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:30.698 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:30.699 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:30.992 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:31.729 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:31.740 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:31.740 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:31.880 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:32.566 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:32.571 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:32.571 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:33.065 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:34.297 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:34.302 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:34.303 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:34.436 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:35.446 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:35.469 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:35.470 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:35.688 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:36.187 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:36.195 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:36.196 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:36.433 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:37.207 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:37.216 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:37.216 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:37.361 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:37.803 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:37.805 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:37.812 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:37.813 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:38.446 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:39.148 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:39.155 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:39.155 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:39.273 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:40.151 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:40.156 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:40.157 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:40.376 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:40.855 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:40.875 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:40.875 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:41.017 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:43.226 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:43.231 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:43.232 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:43.443 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:43.908 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:43.912 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:43.913 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:44.121 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:44.845 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:44.850 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:44.851 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:45.012 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:45.577 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:45.584 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:45.584 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:45.939 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:46.668 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:46.675 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:46.675 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:46.973 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:47.801 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:47.806 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:47.806 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:48.043 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:48.828 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:48.846 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:48.847 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:49.214 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:49.742 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:49.751 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:49.752 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:49.963 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:50.660 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:50.665 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:50.665 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:50.861 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:51.915 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:51.923 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:51.923 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:52.138 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:52.767 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:52.772 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:52.772 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:52.969 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:53.590 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:53.598 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:53.599 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:53.763 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:54.285 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:54.287 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:54.290 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:54.291 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:54.412 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:55.615 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:55.621 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:55.621 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:56.116 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:56.965 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:56.971 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:56.971 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:57.198 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:57.913 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:57.945 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:57.946 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:58.143 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:58.635 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:58.640 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:58.641 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:58.786 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:51:59.331 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:59.350 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:51:59.351 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:51:59.626 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:00.075 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:00.080 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:00.080 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:00.252 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:00.731 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:00.738 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:00.738 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:00.911 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:01.413 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:01.419 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:01.419 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:01.597 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:02.067 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:02.071 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:02.072 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:02.288 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:02.820 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:02.827 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:02.828 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:02.998 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:03.484 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:03.490 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:03.491 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:03.613 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:04.185 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:04.189 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:04.189 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:04.304 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:04.803 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:04.846 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:04.847 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:04.972 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:05.585 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:05.589 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:05.590 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:05.701 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:06.158 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:06.164 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:06.165 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:06.330 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:06.910 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:06.915 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:06.923 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:06.924 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:07.122 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:07.577 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:07.588 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:07.588 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:07.780 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:08.339 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:08.347 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:08.348 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:08.497 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:08.976 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:08.982 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:08.982 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:09.113 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:09.565 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:09.573 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:09.573 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:09.737 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:10.289 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:10.295 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:10.295 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:10.418 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:11.744 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:11.753 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:11.754 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:11.998 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:12.776 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:12.782 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:12.782 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:13.098 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:13.560 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:13.566 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:13.566 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:13.704 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:14.371 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:14.379 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:14.379 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:14.668 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:15.295 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:15.308 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:15.308 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:15.513 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:16.199 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:16.205 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:16.205 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:16.388 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000028?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, task \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(task_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000028?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(task):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000028?line=22'>23</a>\u001b[0m         run_regression_timeseries(batch)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 17'\u001b[0m in \u001b[0;36mrun_regression_timeseries\u001b[0;34m(batch, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=41'>42</a>\u001b[0m h_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nc_regularization \u001b[39m*\u001b[39m  nc_weight\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=43'>44</a>\u001b[0m h_opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=44'>45</a>\u001b[0m h_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=45'>46</a>\u001b[0m h_opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=47'>48</a>\u001b[0m l_test \u001b[39m=\u001b[39m mse_criterion(preds_test\u001b[39m.\u001b[39msqueeze(), y_test\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=245'>246</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=246'>247</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=247'>248</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=248'>249</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=252'>253</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=253'>254</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/_tensor.py?line=254'>255</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=144'>145</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=146'>147</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=147'>148</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/autograd/__init__.py?line=148'>149</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory_bank = MemoryBank()\n",
    "populate_timestamp = timer()\n",
    "\n",
    "task_count = 20\n",
    "populate_loader = []\n",
    "task_loader = []\n",
    "\n",
    "for tasks in range(task_count):\n",
    "    populate_loader = []\n",
    "    X_train, y_train, X_test, y_test = sample_task()\n",
    "    \n",
    "    for batch in zip(X_train, y_train, X_test, y_test):\n",
    "        X_tr, y_tr = batch[0].float(), batch[1].float()\n",
    "        X_te, y_te = batch[2].float(), batch[3].float()\n",
    "        if X_tr.shape[0] == X_te.shape[0]:\n",
    "            d = {\"train\": [X_tr, y_tr],\n",
    "                    \"test\": [X_te, y_te]}\n",
    "            populate_loader.append(d)\n",
    "    task_loader.append(populate_loader)\n",
    "    \n",
    "for i, task in enumerate(task_loader):\n",
    "    for j, batch in enumerate(task):\n",
    "        run_regression_timeseries(batch)\n",
    "\n",
    "# logger.info(f\"Populate time: {timer() - populate_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:52:20.531 | INFO     | __main__:<cell line: 3>:4 - Epoch 0\n",
      "2022-04-24 16:52:20.532 | INFO     | __main__:<cell line: 3>:5 - Bank size: 1888\n",
      "2022-04-24 16:52:20.533 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:20.542 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:20.542 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:20.752 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:21.222 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:21.225 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:21.226 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:21.330 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:21.962 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:21.966 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:21.966 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:22.156 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:22.670 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:22.674 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:22.675 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:22.847 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:23.523 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:23.530 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:23.531 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:23.672 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:24.503 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:24.507 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:24.507 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:24.690 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:25.302 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:25.335 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:25.335 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:25.504 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:25.916 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:25.920 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:25.921 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:26.060 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:26.669 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:26.677 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:26.677 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:26.838 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:27.313 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:27.317 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:27.318 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:27.428 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:28.028 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:28.032 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:28.033 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:28.227 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:28.677 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:28.681 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:28.682 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:28.786 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:29.435 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:29.440 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:29.440 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:29.746 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:30.186 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:30.192 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:30.192 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:30.345 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:30.859 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:30.863 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:30.864 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:31.080 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:31.542 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:31.548 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:31.549 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:31.652 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:32.117 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:32.335 | INFO     | __main__:run_regression_timeseries:17 - X_train shape: torch.Size([32, 20, 18]), Y_train shape: torch.Size([32, 10]), X_test shape: torch.Size([32, 20, 18]), Y_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:32.348 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:32.349 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:32.570 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:33.001 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:33.007 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:33.007 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:33.162 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:33.544 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:33.552 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:33.553 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:33.712 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:34.282 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:34.298 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:34.299 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:34.416 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:34.848 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:34.855 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:34.856 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:35.066 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:36.022 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:36.032 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:36.032 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:36.283 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:36.899 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:36.912 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:36.912 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:37.148 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:37.714 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:37.719 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:37.720 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n",
      "2022-04-24 16:52:37.925 | INFO     | __main__:run_regression_timeseries:39 - Model_preds shape: torch.Size([32, 1])\n",
      "2022-04-24 16:52:38.638 | INFO     | __main__:run_regression_timeseries:50 - L_test shape: torch.Size([32, 10]), L_train shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:38.643 | INFO     | __main__:run_regression_timeseries:23 - Preds_train shape: torch.Size([32, 10]), Preds_test shape: torch.Size([32, 10])\n",
      "2022-04-24 16:52:38.644 | INFO     | __main__:run_regression_timeseries:28 - Te_xp shape: torch.Size([32, 370]), Tr_xp shape: torch.Size([32, 370]), Tr_xyp shape: torch.Size([32, 380])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000030?line=4'>5</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBank size: \u001b[39m\u001b[39m{\u001b[39;00mmemory_bank\u001b[39m.\u001b[39mte_xp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000030?line=6'>7</a>\u001b[0m test_timestamp \u001b[39m=\u001b[39m timer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000030?line=7'>8</a>\u001b[0m out \u001b[39m=\u001b[39m test(epoch, task_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000030?line=8'>9</a>\u001b[0m test_elapsed \u001b[39m=\u001b[39m timer() \u001b[39m-\u001b[39m test_timestamp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000030?line=10'>11</a>\u001b[0m train_timestamp \u001b[39m=\u001b[39m timer()\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 23'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, test_tasks)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000024?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m test_tasks:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000024?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m task:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000024?line=4'>5</a>\u001b[0m         h, meta_batch \u001b[39m=\u001b[39m run_regression_timeseries(batch, train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000024?line=6'>7</a>\u001b[0m         x_train, y_train \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000024?line=7'>8</a>\u001b[0m         x_test, y_test \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 17'\u001b[0m in \u001b[0;36mrun_regression_timeseries\u001b[0;34m(batch, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=33'>34</a>\u001b[0m h_loss \u001b[39m=\u001b[39m h_loss\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=34'>35</a>\u001b[0m \u001b[39m# print(\"te_xp:\", te_xp.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=35'>36</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=36'>37</a>\u001b[0m \u001b[39m# if  nc_regularize and global_step >  train_steps * 2:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=37'>38</a>\u001b[0m model_preds \u001b[39m=\u001b[39m model(meta_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=38'>39</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel_preds shape: \u001b[39m\u001b[39m{\u001b[39;00mmodel_preds\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000018?line=39'>40</a>\u001b[0m \u001b[39m# We sum NC outputs across tasks because h_loss is also summed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 9'\u001b[0m in \u001b[0;36mNeuralComplexity1D.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=61'>62</a>\u001b[0m     \u001b[39m# print(\"input shape:\", inputs[\"te_xp\"].shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=62'>63</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=63'>64</a>\u001b[0m     \u001b[39m# print(\"encoded shape:\", x.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=64'>65</a>\u001b[0m     \u001b[39m# x = self.pool(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=65'>66</a>\u001b[0m     \u001b[39m# print(\"pool shape:\", x.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=66'>67</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rishabh/Desktop/Material/College/Meta Learning/neural-complexity/complete_notebook_ours_ds.ipynb Cell 9'\u001b[0m in \u001b[0;36mCrossAttEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=29'>30</a>\u001b[0m \u001b[39m# print(y_tr.shape, tr_loss.shape, )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=30'>31</a>\u001b[0m bilinear_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y_tr, torch\u001b[39m.\u001b[39mones((y_tr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)), tr_loss, train_pred), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=31'>32</a>\u001b[0m bilinear_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbilinear(x_tr, bilinear_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=32'>33</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_v(bilinear_output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishabh/Desktop/Material/College/Meta%20Learning/neural-complexity/complete_notebook_ours_ds.ipynb#ch0000008?line=34'>35</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(q, k, v)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py:182\u001b[0m, in \u001b[0;36mBilinear.forward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=180'>181</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input1: Tensor, input2: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=181'>182</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbilinear(input1, input2, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py:1874\u001b[0m, in \u001b[0;36mbilinear\u001b[0;34m(input1, input2, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1866'>1867</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(input1, input2, weight):\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1867'>1868</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1868'>1869</a>\u001b[0m         bilinear,\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1869'>1870</a>\u001b[0m         (input1, input2, weight),\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1870'>1871</a>\u001b[0m         input1, input2, weight,\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1871'>1872</a>\u001b[0m         bias\u001b[39m=\u001b[39mbias\n\u001b[1;32m   <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1872'>1873</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/rishabh/miniforge3/envs/mtl/lib/python3.9/site-packages/torch/nn/functional.py?line=1873'>1874</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbilinear(input1, input2, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = {\"train\": {}, \"test\":{}}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch}\")\n",
    "    logger.info(f\"Bank size: {memory_bank.te_xp.shape[0]}\")\n",
    "\n",
    "    test_timestamp = timer()\n",
    "    out = test(epoch, task_loader)\n",
    "    test_elapsed = timer() - test_timestamp\n",
    "\n",
    "    train_timestamp = timer()\n",
    "    out = train(task_loader)\n",
    "    train_elapsed = timer() - train_timestamp\n",
    "    logger.info(f\"Time: train {train_elapsed:.1f} test {test_elapsed:.1f}\")\n",
    "\n",
    "    with open(\"logs.json\", \"w\") as f:\n",
    "        json.dump(tracker, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
